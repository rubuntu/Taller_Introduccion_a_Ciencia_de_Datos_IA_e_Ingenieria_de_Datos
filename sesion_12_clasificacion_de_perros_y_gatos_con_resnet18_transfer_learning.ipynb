{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rubuntu/Taller_Introduccion_a_Ciencia_de_Datos_IA_e_Ingenieria_de_Datos/blob/main/sesion_12_clasificacion_de_perros_y_gatos_con_resnet18_transfer_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "551d7429-cd9d-4dfd-b6cd-0901ff6d6034",
      "metadata": {
        "id": "551d7429-cd9d-4dfd-b6cd-0901ff6d6034"
      },
      "source": [
        "# Clasificaci√≥n de Perros y Gatos con ResNet18 (Transfer Learning)\n",
        "\n",
        "## Objetivos\n",
        "- Aprender a cargar datasets de im√°genes desde HuggingFace (`microsoft/cats_vs_dogs`).\n",
        "- Preparar un pipeline de preprocesamiento con **transformaciones y data augmentation**.\n",
        "- Usar un modelo preentrenado (**ResNet18 con pesos de ImageNet**) y adaptarlo al problema de clasificaci√≥n binaria.\n",
        "- Practicar el concepto de **congelar capas y entrenar solo las √∫ltimas** (fine-tuning parcial).\n",
        "- Implementar **entrenamiento con early stopping** para evitar overfitting.\n",
        "- Evaluar el modelo y visualizar ejemplos de predicciones correctas e incorrectas."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a1e124a-c2d7-4f55-a7d7-feec6e2e74ae",
      "metadata": {
        "id": "2a1e124a-c2d7-4f55-a7d7-feec6e2e74ae"
      },
      "source": [
        "# Tipos de Redes Neuronales\n",
        "\n",
        "---\n",
        "\n",
        "## üåê 1. Redes Cl√°sicas (feedforward)\n",
        "\n",
        "* **Perceptr√≥n**: la m√°s simple, una sola capa de neuronas.\n",
        "* **Perceptr√≥n Multicapa (MLP / FFNN)**: varias capas densas conectadas hacia adelante, sin ciclos. Muy usadas en datos tabulares, predicciones simples y como ‚Äúbloques b√°sicos‚Äù de arquitecturas m√°s complejas.\n",
        "\n",
        "---\n",
        "\n",
        "## üì∏ 2. Redes Convolucionales (CNN)\n",
        "\n",
        "* Dise√±adas para datos con estructura espacial (im√°genes, video, audio).\n",
        "* Usan **filtros** para detectar patrones locales (bordes, texturas).\n",
        "* Variantes:\n",
        "\n",
        "  * **LeNet** (hist√≥rica, d√≠gitos MNIST).\n",
        "  * **AlexNet, VGG, ResNet, EfficientNet** (vision moderna).\n",
        "  * **Conv1D/Conv2D/Conv3D** para se√±ales, im√°genes o vol√∫menes.\n",
        "  * **U-Net / SegNet** para segmentaci√≥n (ej. en medicina).\n",
        "\n",
        "---\n",
        "\n",
        "## üï∞ 3. Redes Recurrentes (RNN)\n",
        "\n",
        "* Para datos **secuenciales** (texto, series de tiempo, audio).\n",
        "* La salida depende del estado previo.\n",
        "* Variantes:\n",
        "\n",
        "  * **RNN simple** (dif√≠ciles de entrenar).\n",
        "  * **LSTM (Long Short-Term Memory)**: maneja dependencias largas.\n",
        "  * **GRU (Gated Recurrent Unit)**: m√°s liviana que LSTM.\n",
        "\n",
        "---\n",
        "\n",
        "## üß≠ 4. Redes Basadas en Atenci√≥n y Transformers\n",
        "\n",
        "* Superaron a las RNN en NLP.\n",
        "* Mecanismo clave: **self-attention**, que permite ver relaciones globales en una secuencia.\n",
        "* Usadas en:\n",
        "\n",
        "  * **NLP** (BERT, GPT, T5).\n",
        "  * **Visi√≥n** (Vision Transformers - ViT).\n",
        "  * **Multimodalidad** (CLIP, Flamingo).\n",
        "  * **Modelos generativos** (Stable Diffusion, Llama, ChatGPT).\n",
        "\n",
        "---\n",
        "\n",
        "## üé® 5. Redes Generativas\n",
        "\n",
        "* Aprenden a **crear datos nuevos** similares a los de entrenamiento.\n",
        "* Principales tipos:\n",
        "\n",
        "  * **Autoencoders (AE, VAE)**: comprimen y reconstruyen datos.\n",
        "  * **GANs (Generative Adversarial Networks)**: generador vs discriminador.\n",
        "  * **Flow-based models**: transformaciones invertibles (Normalizing Flows).\n",
        "  * **Diffusion Models**: modelos de difusi√≥n (hoy dominan en im√°genes).\n",
        "\n",
        "---\n",
        "\n",
        "## üß© 6. Redes Especializadas\n",
        "\n",
        "* **Redes de Hopfield**: memoria asociativa.\n",
        "* **SOM (Self-Organizing Maps)**: mapas auto-organizados para clustering.\n",
        "* **Boltzmann Machines** y **Restricted Boltzmann Machines (RBM)**: usadas en preentrenamiento.\n",
        "* **Capsule Networks**: intentan superar limitaciones de las CNN.\n",
        "* **Graph Neural Networks (GNNs)**: para datos en forma de grafo (redes sociales, qu√≠mica, log√≠stica).\n",
        "\n",
        "---\n",
        "\n",
        "üëâ Como ves, el **tipo de red** depende mucho del **tipo de dato** y del **objetivo**:\n",
        "\n",
        "* Tabular ‚Üí MLP\n",
        "* Im√°genes ‚Üí CNN / ViT\n",
        "* Texto / Secuencias ‚Üí RNN / Transformers\n",
        "* Generaci√≥n ‚Üí AE / GAN / Diffusion\n",
        "* Datos relacionales ‚Üí GNN\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0af91d56-5f1c-4b40-9be6-d0377ccd7100",
      "metadata": {
        "id": "0af91d56-5f1c-4b40-9be6-d0377ccd7100"
      },
      "source": [
        "## üìä Comparativo de Tipos de Redes Neuronales\n",
        "\n",
        "| Tipo de Red                     | Ventajas                                                                       | Desventajas                                                                              | Casos de Uso                                                    |\n",
        "| ------------------------------- | ------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------- | --------------------------------------------------------------- |\n",
        "| **MLP (Perceptr√≥n Multicapa)**  | Sencilla de implementar, funciona bien en datos tabulares, sirve como baseline | No escala bien con im√°genes o secuencias, poca capacidad para captar estructura compleja | Scoring de cr√©dito, predicciones tabulares, forecasting simple  |\n",
        "| **CNN (Convolucionales)**       | Excelentes en visi√≥n, detectan patrones locales, eficientes en im√°genes        | Requieren muchos datos, sensibles a transformaciones (rotaci√≥n, escala)                  | Clasificaci√≥n de im√°genes, visi√≥n m√©dica, veh√≠culos aut√≥nomos   |\n",
        "| **RNN (Recurrentes)**           | Manejan secuencias, modelan dependencias temporales                            | Problemas de gradiente (exploding/vanishing), entrenamiento lento                        | Series de tiempo, an√°lisis de texto, speech recognition         |\n",
        "| **LSTM / GRU**                  | Capturan dependencias largas, m√°s estables que RNN cl√°sicas                    | Computacionalmente costosas, menos usadas hoy frente a Transformers                      | Traducci√≥n autom√°tica, predicci√≥n de secuencias, chatbots       |\n",
        "| **Transformers**                | Escalan muy bien, capturan relaciones globales, dominan NLP y multimodal       | Alt√≠simo costo computacional, requieren muchos datos                                     | ChatGPT, BERT, traducci√≥n, visi√≥n con ViT, modelos multimodales |\n",
        "| **Autoencoders / VAE**          | √ötiles para reducci√≥n de dimensionalidad, generaci√≥n controlada                | Reconstrucciones a veces borrosas, menos expresivos que GAN/Diffusion                    | Detecci√≥n de anomal√≠as, compresi√≥n, generaci√≥n b√°sica           |\n",
        "| **GANs**                        | Generan datos realistas (im√°genes, audio, video)                               | Entrenamiento inestable, dif√≠cil de balancear generador y discriminador                  | Deepfakes, arte digital, s√≠ntesis de im√°genes                   |\n",
        "| **Diffusion Models**            | Estado del arte en generaci√≥n de im√°genes/audio, m√°s estables que GANs         | Lentitud en inferencia (aunque existen aceleraciones)                                    | Stable Diffusion, MidJourney, generaci√≥n de m√∫sica e im√°genes   |\n",
        "| **GNN (Graph Neural Networks)** | Capturan relaciones complejas en grafos, muy √∫tiles en dominios estructurados  | Implementaci√≥n m√°s compleja, requieren conocimiento especializado                        | Redes sociales, qu√≠mica, log√≠stica, detecci√≥n de fraude         |\n",
        "\n",
        "---\n",
        "\n",
        "üëâ En resumen:\n",
        "\n",
        "* **MLP** = tabular simple.\n",
        "* **CNN** = visi√≥n.\n",
        "* **RNN/LSTM/GRU** = secuencias tradicionales.\n",
        "* **Transformers** = el rey actual (texto, imagen, multimodal).\n",
        "* **GAN/Diffusion** = generaci√≥n creativa.\n",
        "* **GNN** = datos relacionales.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dc06106-6b52-45d7-a6cd-83531eae84eb",
      "metadata": {
        "id": "1dc06106-6b52-45d7-a6cd-83531eae84eb"
      },
      "source": [
        "## üå≥ √Årbol de Decisi√≥n: ¬øQu√© red usar?\n",
        "\n",
        "```\n",
        "¬øCon qu√© tipo de datos trabajas?\n",
        "‚îÇ\n",
        "‚îú‚îÄ‚îÄ Tabulares (CSV, tablas, features num√©ricos/categ√≥ricos)\n",
        "‚îÇ     ‚îî‚îÄ‚îÄ MLP (Perceptr√≥n Multicapa)\n",
        "‚îÇ\n",
        "‚îú‚îÄ‚îÄ Im√°genes\n",
        "‚îÇ     ‚îú‚îÄ‚îÄ Clasificaci√≥n / Detecci√≥n / Segmentaci√≥n\n",
        "‚îÇ     ‚îÇ      ‚îú‚îÄ‚îÄ Pocos datos ‚Üí CNN pre-entrenada (ResNet, EfficientNet, U-Net)\n",
        "‚îÇ     ‚îÇ      ‚îî‚îÄ‚îÄ Muchos datos ‚Üí Vision Transformer (ViT)\n",
        "‚îÇ     ‚îî‚îÄ‚îÄ Generaci√≥n de im√°genes\n",
        "‚îÇ            ‚îú‚îÄ‚îÄ Realismo ‚Üí GAN\n",
        "‚îÇ            ‚îî‚îÄ‚îÄ Estado del arte ‚Üí Diffusion Models\n",
        "‚îÇ\n",
        "‚îú‚îÄ‚îÄ Texto o Secuencias (NLP, series de tiempo, audio)\n",
        "‚îÇ     ‚îú‚îÄ‚îÄ Dependencias cortas ‚Üí RNN\n",
        "‚îÇ     ‚îú‚îÄ‚îÄ Dependencias largas ‚Üí LSTM / GRU\n",
        "‚îÇ     ‚îî‚îÄ‚îÄ NLP moderno ‚Üí Transformer (BERT, GPT)\n",
        "‚îÇ\n",
        "‚îú‚îÄ‚îÄ Datos Relacionales (Grafos, redes sociales, qu√≠mica)\n",
        "‚îÇ     ‚îî‚îÄ‚îÄ Graph Neural Networks (GNNs)\n",
        "‚îÇ\n",
        "‚îî‚îÄ‚îÄ Otros casos especiales\n",
        "      ‚îú‚îÄ‚îÄ Reducci√≥n de dimensionalidad / Anomal√≠as ‚Üí Autoencoder / VAE\n",
        "      ‚îú‚îÄ‚îÄ Memoria asociativa ‚Üí Hopfield\n",
        "      ‚îî‚îÄ‚îÄ Organizaci√≥n no supervisada ‚Üí Self-Organizing Maps (SOM)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Lectura r√°pida\n",
        "\n",
        "* **Si tienes TABLAS ‚Üí MLP.**\n",
        "* **Si son IM√ÅGENES ‚Üí CNN o Transformer.**\n",
        "* **Si es TEXTO o SERIES ‚Üí Transformer (o LSTM si es m√°s cl√°sico).**\n",
        "* **Si son GRAFOS ‚Üí GNN.**\n",
        "* **Si se busca GENERAR ‚Üí GAN o Diffusion.**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Ejemplo sencillo en PyTorch** usando una **LSTM** para clasificaci√≥n de texto (positivo/negativo) con un dataset de juguete.\n",
        "\n",
        "\n",
        "### Explicaci√≥n r√°pida:\n",
        "\n",
        "* **Embedding**: convierte √≠ndices de palabras en vectores densos.\n",
        "* **LSTM**: procesa la secuencia y devuelve un hidden state final.\n",
        "* **Linear + Softmax**: convierte el hidden state en probabilidades de clases.\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "5ntG3DOW7SEV"
      },
      "id": "5ntG3DOW7SEV"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Datos de ejemplo (toy)\n",
        "# -----------------------------\n",
        "# Supongamos vocabulario reducido: {0:PAD, 1:good, 2:bad, 3:movie, 4:boring, 5:great}\n",
        "sentences = [\n",
        "    [1, 3, 5],   # \"good movie great\" (positivo)\n",
        "    [2, 3, 4],   # \"bad movie boring\" (negativo)\n",
        "    [1, 5],      # \"good great\" (positivo)\n",
        "    [2, 4]       # \"bad boring\" (negativo)\n",
        "]\n",
        "labels = [1, 0, 1, 0]  # 1: positivo, 0: negativo\n",
        "\n",
        "# Padding para igualar longitudes\n",
        "max_len = max(len(s) for s in sentences)\n",
        "X = [s + [0]*(max_len-len(s)) for s in sentences]\n",
        "X = torch.tensor(X)\n",
        "y = torch.tensor(labels)\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Definir modelo LSTM\n",
        "# -----------------------------\n",
        "class SentimentRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)              # (batch, seq_len, embed_dim)\n",
        "        output, (h_n, c_n) = self.lstm(embedded) # h_n: (1, batch, hidden_dim)\n",
        "        out = self.fc(h_n[-1])                   # Usamos el √∫ltimo hidden state\n",
        "        return out\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Entrenamiento\n",
        "# -----------------------------\n",
        "vocab_size = 6   # tokens del 0 al 5\n",
        "embed_dim = 8\n",
        "hidden_dim = 16\n",
        "output_dim = 2   # positivo / negativo\n",
        "\n",
        "model = SentimentRNN(vocab_size, embed_dim, hidden_dim, output_dim)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(20):\n",
        "    optimizer.zero_grad()\n",
        "    y_pred = model(X)\n",
        "    loss = criterion(y_pred, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if (epoch+1) % 5 == 0:\n",
        "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Prueba con nueva frase\n",
        "# -----------------------------\n",
        "test = torch.tensor([[1, 3, 5]])  # \"good movie great\"\n",
        "pred = model(test)\n",
        "print(\"Predicci√≥n:\", torch.argmax(pred, dim=1).item())  # 1 = positivo"
      ],
      "metadata": {
        "id": "V4kcsB438Nvc"
      },
      "id": "V4kcsB438Nvc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c8a6df64-99d3-4f03-87ed-a8665940489d",
      "metadata": {
        "id": "c8a6df64-99d3-4f03-87ed-a8665940489d"
      },
      "source": [
        "## üìå **Data Augmentation**\n",
        "\n",
        "* Es una t√©cnica para **aumentar artificialmente la cantidad de datos de entrenamiento** sin recolectar nuevos ejemplos.\n",
        "* Consiste en aplicar **transformaciones aleatorias** a las im√°genes originales (o datos en general).\n",
        "* Ejemplos comunes en im√°genes:\n",
        "\n",
        "  * Rotaciones, volteos (*flips*).\n",
        "  * Cambios de brillo, contraste, color.\n",
        "  * Recortes (*cropping*), escalados, zooms.\n",
        "  * Adici√≥n de ruido.\n",
        "* ‚úÖ Beneficio: evita **overfitting** y mejora la capacidad de generalizaci√≥n del modelo.\n",
        "\n",
        "---\n",
        "\n",
        "## üìå **Transfer Learning**\n",
        "\n",
        "* Estrategia en la que un modelo previamente **entrenado en una tarea grande** (ej. clasificaci√≥n en ImageNet con millones de im√°genes) se reutiliza para otra tarea similar.\n",
        "* Se puede:\n",
        "\n",
        "  1. **Congelar capas** iniciales (que extraen caracter√≠sticas generales como bordes, texturas).\n",
        "  2. **Reentrenar solo las capas finales** para la nueva tarea (ej. clasificar radiograf√≠as en lugar de gatos/perros).\n",
        "* ‚úÖ Beneficio: permite entrenar modelos con **pocos datos** y en **menos tiempo**.\n",
        "\n",
        "---\n",
        "\n",
        "## üìå **ResNet18**\n",
        "\n",
        "* Es una **Red Residual (Residual Network)** propuesta por Microsoft en 2015.\n",
        "* \"18\" significa que tiene **18 capas de profundidad** (convolucionales + fully connected).\n",
        "* Introduce el concepto de **skip connections (conexiones residuales)**:\n",
        "\n",
        "  * En lugar de pasar siempre \"capa ‚Üí capa ‚Üí capa\", se permite que la entrada se sume directamente a la salida de una capa posterior.\n",
        "  * Esto resuelve el problema del **desvanecimiento del gradiente** y permite entrenar redes **muy profundas** (cientos de capas).\n",
        "\n",
        "üìå Estructura simplificada de ResNet18:\n",
        "\n",
        "* 1 capa convolucional inicial.\n",
        "* 4 bloques residuales principales, cada uno con 2 capas convolucionales.\n",
        "* Una capa *fully connected* final.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4d71a5d-77c6-48fa-8ddc-ef5ce2ad1e7c",
      "metadata": {
        "id": "e4d71a5d-77c6-48fa-8ddc-ef5ce2ad1e7c"
      },
      "source": [
        "La **ResNet18 preentrenada** que se encuentra en librer√≠as como **PyTorch** o **Torchvision** normalmente est√° entrenada en **ImageNet** üñºÔ∏è, un dataset enorme con **m√°s de 1 mill√≥n de im√°genes en 1000 clases diferentes**.\n",
        "\n",
        "üëâ Dentro de esas 1000 clases se tienen categor√≠as de:\n",
        "\n",
        "* Animales (aves, peces, insectos, mam√≠feros, etc.).\n",
        "* Objetos cotidianos (sillas, tazas, autos, relojes, etc.).\n",
        "* Escenas naturales.\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ **Entonces:**\n",
        "\n",
        "* Gracias a **transfer learning**, se puede reutilizar y **adaptar para clasificar perros vs gatos** (binaria), ajustando solo las √∫ltimas capas.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clasificaci√≥n de Perros y Gatos"
      ],
      "metadata": {
        "id": "QdwplM9v7o27"
      },
      "id": "QdwplM9v7o27"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e050f27",
      "metadata": {
        "id": "1e050f27"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from datasets import load_dataset\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Usando dispositivo:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6563df3c-c812-4620-be8d-5bf2cb22eda9",
      "metadata": {
        "id": "6563df3c-c812-4620-be8d-5bf2cb22eda9"
      },
      "source": [
        "## 1. Cargar dataset p√∫blico (HuggingFace)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef9c4b9d",
      "metadata": {
        "id": "ef9c4b9d"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"microsoft/cats_vs_dogs\")\n",
        "\n",
        "# Dividir en 80% train / 20% test\n",
        "dataset = dataset[\"train\"].train_test_split(test_size=0.2)\n",
        "\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "380ee705-d109-4455-bf14-3e30f9c7c207",
      "metadata": {
        "id": "380ee705-d109-4455-bf14-3e30f9c7c207"
      },
      "source": [
        "## 2. Transformaciones (Data Augmentation + Normalizaci√≥n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88550b37",
      "metadata": {
        "id": "88550b37"
      },
      "outputs": [],
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
        "    transforms.Lambda(lambda img: img.convert(\"RGB\")),  # ‚ö†Ô∏è convertir a RGB\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.Lambda(lambda img: img.convert(\"RGB\")),  # ‚ö†Ô∏è convertir a RGB\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69ae6d3d-313b-4b5e-a118-a17118683924",
      "metadata": {
        "id": "69ae6d3d-313b-4b5e-a118-a17118683924"
      },
      "source": [
        "## 3. Adaptador HuggingFace ‚Üí PyTorch Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d94dda79",
      "metadata": {
        "id": "d94dda79"
      },
      "outputs": [],
      "source": [
        "class CatsDogsDataset(Dataset):\n",
        "    def __init__(self, hf_dataset, transform=None):\n",
        "        self.data = hf_dataset\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.data[idx][\"image\"]\n",
        "        label = self.data[idx][\"labels\"]   # ‚ö†Ô∏è usar 'labels' en lugar de 'file'\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "train_ds = CatsDogsDataset(dataset[\"train\"], transform=transform_train)\n",
        "test_ds  = CatsDogsDataset(dataset[\"test\"], transform=transform_test)\n",
        "\n",
        "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "test_dl  = DataLoader(test_ds, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "397a8f6c-51c5-41fa-bc6a-e176b31c532a",
      "metadata": {
        "id": "397a8f6c-51c5-41fa-bc6a-e176b31c532a"
      },
      "source": [
        "## 4. Definir modelo (ResNet18 con pesos de ImageNet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b60ae03a",
      "metadata": {
        "id": "b60ae03a"
      },
      "outputs": [],
      "source": [
        "weights = ResNet18_Weights.DEFAULT\n",
        "model = resnet18(weights=weights)\n",
        "\n",
        "# Congelar todas las capas excepto las √∫ltimas\n",
        "for name, param in model.named_parameters():\n",
        "    if \"layer4\" in name or \"fc\" in name:\n",
        "        param.requires_grad = True\n",
        "    else:\n",
        "        param.requires_grad = False\n",
        "\n",
        "# Reemplazar la √∫ltima capa para 2 clases\n",
        "model.fc = nn.Linear(model.fc.in_features, 2)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a75ff83-15db-4655-a49d-c8029b273596",
      "metadata": {
        "id": "3a75ff83-15db-4655-a49d-c8029b273596"
      },
      "source": [
        "## 5. Entrenamiento con Early Stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f424041",
      "metadata": {
        "id": "1f424041"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "opt = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)\n",
        "\n",
        "#n_epochs = 20\n",
        "n_epochs = 1\n",
        "patience = 3\n",
        "best_acc = 0\n",
        "epochs_no_improve = 0\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    # --- Entrenamiento ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for xb, yb in train_dl:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        preds = model(xb)\n",
        "        loss = loss_fn(preds, yb)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # --- Evaluaci√≥n ---\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in test_dl:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            preds = model(xb).argmax(dim=1)\n",
        "            correct += (preds == yb).sum().item()\n",
        "            total += yb.size(0)\n",
        "\n",
        "    acc = correct / total\n",
        "    print(f\"Epoch {epoch+1}, Loss={running_loss/len(train_dl):.4f}, Val Acc={acc:.4f}\")\n",
        "\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc\n",
        "        epochs_no_improve = 0\n",
        "        torch.save(model.state_dict(), \"best_catsdogs.pth\")\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(\"Early stopping activado\")\n",
        "            break\n",
        "\n",
        "print(\"Mejor accuracy alcanzado:\", best_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "976150ca-d43c-4ab3-855c-6c5a3d7f60f2",
      "metadata": {
        "id": "976150ca-d43c-4ab3-855c-6c5a3d7f60f2"
      },
      "source": [
        "## 6. Visualizaci√≥n de predicciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f111fb5b",
      "metadata": {
        "id": "f111fb5b"
      },
      "outputs": [],
      "source": [
        "\n",
        "images, labels = next(iter(test_dl))\n",
        "images, labels = images.to(device), labels.to(device)\n",
        "preds = model(images).argmax(dim=1)\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "for i in range(8):\n",
        "    plt.subplot(2,4,i+1)\n",
        "    img = images[i].cpu().permute(1,2,0).numpy()\n",
        "    img = (img * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]).clip(0,1)  # desnormalizar\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"Real: {labels[i].item()}, Pred: {preds[i].item()}\")\n",
        "    plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9601a9fa-6b6c-4041-b19a-835784b3abbf",
      "metadata": {
        "id": "9601a9fa-6b6c-4041-b19a-835784b3abbf"
      },
      "source": [
        "## Preguntas de Discusi√≥n\n",
        "\n",
        "1. ¬øQu√© ventajas ofrece usar un modelo preentrenado (transfer learning) frente a entrenar desde cero?\n",
        "2. ¬øPor qu√© es √∫til congelar capas en el inicio del entrenamiento y ajustar solo la √∫ltima capa?\n",
        "3. ¬øC√≥mo ayuda el *data augmentation* a mejorar la capacidad de generalizaci√≥n del modelo?\n",
        "4. ¬øQu√© diferencias observas en el rendimiento al descongelar m√°s capas para el fine-tuning?\n",
        "5. ¬øQu√© rol cumple la normalizaci√≥n con los valores de ImageNet en la estabilidad del entrenamiento?\n",
        "6. ¬øC√≥mo decide el early stopping cu√°ndo detener el entrenamiento y por qu√© es importante?\n",
        "7. ¬øQu√© m√©tricas adicionales (adem√°s de accuracy) podr√≠an ser √∫tiles en este problema?\n",
        "8. ¬øC√≥mo se podr√≠a mejorar a√∫n m√°s el modelo si se dispusiera de m√°s recursos computacionales?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BMrEh9c-WIpf",
      "metadata": {
        "id": "BMrEh9c-WIpf"
      },
      "source": [
        "## üí° Preguntas de Discusi√≥n (desarrolladas)\n",
        "\n",
        "1. **¬øQu√© ventajas ofrece usar un modelo preentrenado (transfer learning) frente a entrenar desde cero?**\n",
        "\n",
        "   * Entrenar desde cero requiere grandes cantidades de datos y mucho tiempo de c√≥mputo.\n",
        "   * Los modelos preentrenados en ImageNet ya han aprendido caracter√≠sticas generales (bordes, texturas, formas), que son √∫tiles para muchos problemas de visi√≥n.\n",
        "   * Con *transfer learning*, solo se adapta la parte final de la red al nuevo conjunto de clases, logrando **mejor rendimiento con menos datos y menos tiempo de entrenamiento**.\n",
        "\n",
        "---\n",
        "\n",
        "2. **¬øPor qu√© es √∫til congelar capas en el inicio del entrenamiento y ajustar solo la √∫ltima capa?**\n",
        "\n",
        "   * Las primeras capas de una CNN aprenden caracter√≠sticas muy generales (l√≠neas, bordes, patrones de color).\n",
        "   * Si se ajustan todas desde el inicio, se corre el riesgo de *desaprender* esas representaciones √∫tiles.\n",
        "   * Congelarlas permite entrenar m√°s r√°pido y reducir el riesgo de sobreajuste, enfocando el aprendizaje solo en la capa de clasificaci√≥n final.\n",
        "\n",
        "---\n",
        "\n",
        "3. **¬øC√≥mo ayuda el *data augmentation* a mejorar la capacidad de generalizaci√≥n del modelo?**\n",
        "\n",
        "   * Genera versiones modificadas de las im√°genes (rotadas, espejadas, con variaciones de color).\n",
        "   * Esto obliga al modelo a aprender **patrones invariantes** a peque√±as transformaciones, en lugar de memorizar ejemplos concretos.\n",
        "   * Mejora la robustez y reduce el riesgo de sobreajuste cuando los datasets son peque√±os.\n",
        "\n",
        "---\n",
        "\n",
        "4. **¬øQu√© diferencias observas en el rendimiento al descongelar m√°s capas para el fine-tuning?**\n",
        "\n",
        "   * Congelar casi todo ‚Üí entrenamiento r√°pido pero menos capacidad de adaptaci√≥n al nuevo dominio.\n",
        "   * Descongelar √∫ltimas capas ‚Üí mejor ajuste al dataset objetivo, a costa de m√°s tiempo de entrenamiento.\n",
        "   * Descongelar toda la red ‚Üí mayor capacidad de adaptaci√≥n, pero mayor riesgo de sobreajuste si el dataset es peque√±o.\n",
        "   * En la pr√°ctica, **descongelar gradualmente** (empezando desde las √∫ltimas capas) suele dar los mejores resultados.\n",
        "\n",
        "---\n",
        "\n",
        "5. **¬øQu√© rol cumple la normalizaci√≥n con los valores de ImageNet en la estabilidad del entrenamiento?**\n",
        "\n",
        "   * Los modelos preentrenados esperan entradas con la misma estad√≠stica que los datos de ImageNet.\n",
        "   * Normalizar con `mean=[0.485, 0.456, 0.406]` y `std=[0.229, 0.224, 0.225]` alinea la distribuci√≥n de p√≠xeles con la que el modelo fue entrenado originalmente.\n",
        "   * Esto evita desajustes que podr√≠an degradar el rendimiento o dificultar la convergencia.\n",
        "\n",
        "---\n",
        "\n",
        "6. **¬øC√≥mo decide el early stopping cu√°ndo detener el entrenamiento y por qu√© es importante?**\n",
        "\n",
        "   * Early stopping monitorea una m√©trica de validaci√≥n (ej. p√©rdida o accuracy).\n",
        "   * Si no mejora despu√©s de un n√∫mero definido de √©pocas (*patience*), se detiene el entrenamiento.\n",
        "   * Esto evita que el modelo siga ajust√°ndose al conjunto de entrenamiento mientras empeora en el de validaci√≥n (sobreajuste).\n",
        "   * Tambi√©n ahorra tiempo y recursos.\n",
        "\n",
        "---\n",
        "\n",
        "7. **¬øQu√© m√©tricas adicionales (adem√°s de accuracy) podr√≠an ser √∫tiles en este problema?**\n",
        "\n",
        "   * **Precisi√≥n (precision):** proporci√≥n de predicciones positivas correctas (√∫til si queremos pocas falsas alarmas).\n",
        "   * **Recall (sensibilidad):** proporci√≥n de verdaderos positivos detectados (√∫til si no queremos dejar escapar casos).\n",
        "   * **F1-score:** balance entre precisi√≥n y recall.\n",
        "   * **Matriz de confusi√≥n:** para ver qu√© clases se confunden m√°s.\n",
        "   * **AUC-ROC:** mide la capacidad de distinguir entre clases en distintos umbrales.\n",
        "\n",
        "---\n",
        "\n",
        "8. **¬øC√≥mo se podr√≠a mejorar a√∫n m√°s el modelo si se dispusiera de m√°s recursos computacionales?**\n",
        "\n",
        "   * Usar arquitecturas m√°s grandes y potentes (**ResNet50, EfficientNet, Vision Transformers**).\n",
        "   * Entrenar durante m√°s √©pocas con estrategias de regularizaci√≥n (dropout, weight decay).\n",
        "   * Aumentar la resoluci√≥n de entrada (224√ó224 ‚Üí 384√ó384).\n",
        "   * Usar *ensembles* de varios modelos para combinar predicciones.\n",
        "   * Aplicar *semi-supervised learning* o *self-supervised pretraining* para aprovechar datos no etiquetados.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "markdown",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}