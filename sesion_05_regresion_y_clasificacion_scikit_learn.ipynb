{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rubuntu/Taller_Introduccion_a_Ciencia_de_Datos_IA_e_Ingenieria_de_Datos/blob/main/sesion_05_regresion_y_clasificacion_scikit_learn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea4857f8",
      "metadata": {
        "id": "ea4857f8"
      },
      "source": [
        "# Sesi√≥n 05 ‚Äî Regresi√≥n y Clasificaci√≥n con scikit‚Äëlearn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a769e7a3-91d0-433e-82cd-f813a29d5a54",
      "metadata": {
        "id": "a769e7a3-91d0-433e-82cd-f813a29d5a54"
      },
      "source": [
        "# Comparativo: Regresi√≥n Lineal vs Regresi√≥n Log√≠stica"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d382c6e-354d-46c1-ae64-7f707e519088",
      "metadata": {
        "id": "9d382c6e-354d-46c1-ae64-7f707e519088"
      },
      "source": [
        "# üìò Regresi√≥n Lineal\n",
        "\n",
        "## Concepto\n",
        "\n",
        "* Modelo estad√≠stico que **predice un valor num√©rico continuo** en funci√≥n de variables independientes.\n",
        "* Supone una **relaci√≥n lineal** entre variables:\n",
        "\n",
        "$$\n",
        "Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_n X_n + \\epsilon\n",
        "$$\n",
        "\n",
        "* Donde:\n",
        "\n",
        "  * $Y$ = variable dependiente (lo que queremos predecir).\n",
        "  * $X_i$ = variables independientes (predictoras).\n",
        "  * $\\beta_i$ = coeficientes (pendientes).\n",
        "  * $\\epsilon$ = error.\n",
        "\n",
        "---\n",
        "\n",
        "## Ejemplo\n",
        "\n",
        "* Predecir el **precio de una casa** en funci√≥n de su superficie.\n",
        "\n",
        "$$\n",
        "Precio = \\beta_0 + \\beta_1 \\cdot Superficie\n",
        "$$\n",
        "\n",
        "Si $\\beta_0 = 50,000$ y $\\beta_1 = 1,000$,\n",
        "una casa de 100 m¬≤ tendr√≠a precio ‚âà 150,000.\n",
        "\n",
        "---\n",
        "\n",
        "## Animaci√≥n\n",
        "https://phet.colorado.edu/es/simulations/least-squares-regression\n",
        "\n",
        "---\n",
        "\n",
        "## Gr√°fico Intuitivo\n",
        "\n",
        "Una **l√≠nea recta** que mejor ajusta los puntos de datos.\n",
        "La idea es minimizar el error cuadr√°tico medio (**MSE**).\n",
        "\n",
        "---\n",
        "\n",
        "## Aplicaciones\n",
        "\n",
        "* Predicci√≥n de ingresos en funci√≥n de a√±os de educaci√≥n.\n",
        "* Forecast de ventas seg√∫n inversi√≥n en marketing.\n",
        "* Estimar el riesgo crediticio (como variable continua, ej. probabilidad).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59990383-19b5-4e9d-b07d-b5d17c902b86",
      "metadata": {
        "id": "59990383-19b5-4e9d-b07d-b5d17c902b86"
      },
      "outputs": [],
      "source": [
        "# Ejemplo en Python\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "X, y = make_regression(n_samples=300, n_features=1, noise=15.0, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "lin_model = LinearRegression().fit(X_train, y_train)\n",
        "y_pred = lin_model.predict(X_test)\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "plt.scatter(X_test, y_test, alpha=0.5)\n",
        "order = np.argsort(X_test[:, 0])\n",
        "plt.plot(X_test[order], y_pred[order], color=\"red\")\n",
        "plt.title(f\"Regresi√≥n Lineal: RMSE={rmse:.2f}, R¬≤={r2:.3f}\")\n",
        "plt.xlabel(\"X\"); plt.ylabel(\"y\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbfd603b-80aa-4bfe-bb40-eadb6c764e1c",
      "metadata": {
        "id": "cbfd603b-80aa-4bfe-bb40-eadb6c764e1c"
      },
      "outputs": [],
      "source": [
        "# --- Gr√°fico Predicci√≥n vs Realidad ---\n",
        "plt.scatter(y_test, y_pred, alpha=0.5)\n",
        "plt.plot([y_test.min(), y_test.max()],\n",
        "         [y_test.min(), y_test.max()],\n",
        "         'r--', lw=2)  # l√≠nea ideal\n",
        "plt.xlabel(\"Valor Real (y_test)\")\n",
        "plt.ylabel(\"Predicci√≥n (y_pred)\")\n",
        "plt.title(\"Predicci√≥n vs Realidad\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "705b7237-ab8a-4be8-a703-c07f702c6d45",
      "metadata": {
        "id": "705b7237-ab8a-4be8-a703-c07f702c6d45"
      },
      "source": [
        "# üìò Regresi√≥n Log√≠stica\n",
        "\n",
        "## Concepto\n",
        "\n",
        "* Variante que sirve para **clasificaci√≥n binaria** (S√≠/No, 0/1).\n",
        "* En lugar de predecir valores continuos, predice **probabilidades**:\n",
        "\n",
        "$$\n",
        "P(Y=1|X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1X_1 + \\dots + \\beta_nX_n)}}\n",
        "$$\n",
        "\n",
        "* La funci√≥n **sigmoide** transforma cualquier valor en un n√∫mero entre 0 y 1.\n",
        "\n",
        "---\n",
        "\n",
        "## Ejemplo\n",
        "\n",
        "* Predecir si un cliente **har√° churn** (1) o no (0).\n",
        "\n",
        "$$\n",
        "Logit(P) = \\ln\\left(\\frac{P}{1-P}\\right) = \\beta_0 + \\beta_1 X\n",
        "$$\n",
        "\n",
        "Si el modelo predice $P=0.8$, interpretamos: **80% de probabilidad de churn**.\n",
        "\n",
        "---\n",
        "\n",
        "## Gr√°fico Intuitivo\n",
        "\n",
        "* La curva de la regresi√≥n log√≠stica es en forma de **S (sigmoide)**.\n",
        "* Para valores bajos de X ‚Üí P cercano a 0.\n",
        "* Para valores altos de X ‚Üí P cercano a 1.\n",
        "* Punto de corte (threshold, usualmente 0.5) ‚Üí decide clase 0 o 1.\n",
        "\n",
        "---\n",
        "\n",
        "## Aplicaciones\n",
        "\n",
        "* Detecci√≥n de fraude (fraude = 1, no fraude = 0).\n",
        "* Diagn√≥stico m√©dico (enfermo = 1, sano = 0).\n",
        "* Predicci√≥n de abandono (churn).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83e9b1da-96e5-4a4a-b5eb-10fc4e676921",
      "metadata": {
        "id": "83e9b1da-96e5-4a4a-b5eb-10fc4e676921"
      },
      "outputs": [],
      "source": [
        "# Ejemplo en Python\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, RocCurveDisplay\n",
        "\n",
        "# Dataset binario 1D (v√°lido)\n",
        "Xc, yc = make_classification(\n",
        "    n_samples=10000,\n",
        "    n_features=1,\n",
        "    n_redundant=0,\n",
        "    n_informative=1,\n",
        "    n_classes=2,\n",
        "    n_clusters_per_class=1,\n",
        "    class_sep=0.8,      # ‚Üì separa menos las clases\n",
        "    flip_y=0.2,         # 20% de etiquetas ruidosas\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "Xc_train, Xc_test, yc_train, yc_test = train_test_split(\n",
        "    Xc, yc, test_size=0.25, stratify=yc, random_state=42\n",
        ")\n",
        "\n",
        "# Modelo\n",
        "log_model = LogisticRegression().fit(Xc_train, yc_train)\n",
        "\n",
        "y_pred = log_model.predict(Xc_test)\n",
        "y_proba = log_model.predict_proba(Xc_test)[:, 1]\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(yc_test, y_pred))\n",
        "print(\"F1:\", f1_score(yc_test, y_pred))\n",
        "print(\"AUC:\", roc_auc_score(yc_test, y_proba))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b00480da-a4c6-4dfd-8590-a2dbe1a7acf6",
      "metadata": {
        "id": "b00480da-a4c6-4dfd-8590-a2dbe1a7acf6"
      },
      "outputs": [],
      "source": [
        "# Curva sigmoide\n",
        "xs = np.linspace(Xc_test.min(), Xc_test.max(), 200).reshape(-1, 1)\n",
        "sig = log_model.predict_proba(xs)[:, 1]\n",
        "\n",
        "plt.scatter(Xc_test, yc_test, alpha=0.5)\n",
        "plt.plot(xs, sig)\n",
        "plt.title(\"Regresi√≥n Log√≠stica: Probabilidad P(Y=1|X)\")\n",
        "plt.xlabel(\"X\"); plt.ylabel(\"Probabilidad\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28cb9b02-416f-48fe-9e46-9aa7cc7fe58a",
      "metadata": {
        "id": "28cb9b02-416f-48fe-9e46-9aa7cc7fe58a"
      },
      "outputs": [],
      "source": [
        "# Curva ROC\n",
        "RocCurveDisplay.from_predictions(yc_test, y_proba)\n",
        "plt.title(\"Curva ROC\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f408e118-b9e1-4615-a29b-3ed06226ae30",
      "metadata": {
        "id": "f408e118-b9e1-4615-a29b-3ed06226ae30"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
        "    PrecisionRecallDisplay, average_precision_score\n",
        ")\n",
        "\n",
        "# --- Reporte de clasificaci√≥n ---\n",
        "print(\"\\n=== Classification Report ===\")\n",
        "print(classification_report(yc_test, y_pred, digits=3))\n",
        "\n",
        "# --- Matriz de confusi√≥n (texto + plot) ---\n",
        "cm = confusion_matrix(yc_test, y_pred)\n",
        "print(\"\\n=== Confusion Matrix ===\\n\", cm)\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.title(\"Matriz de Confusi√≥n\")\n",
        "plt.show()\n",
        "\n",
        "# --- Curva Precision-Recall + Average Precision ---\n",
        "ap = average_precision_score(yc_test, y_proba)\n",
        "PrecisionRecallDisplay.from_predictions(yc_test, y_proba)\n",
        "plt.title(f\"Curva Precision-Recall (AP = {ap:.3f})\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4110eb55-2873-43ee-b06b-76b994ddc96d",
      "metadata": {
        "id": "4110eb55-2873-43ee-b06b-76b994ddc96d"
      },
      "source": [
        "# üìä Diferencias Clave\n",
        "\n",
        "| Aspecto         | Regresi√≥n Lineal      | Regresi√≥n Log√≠stica        |\n",
        "| --------------- | --------------------- | -------------------------- |\n",
        "| Tipo de salida  | Variable continua (‚Ñù) | Probabilidad (0 a 1)       |\n",
        "| Funci√≥n         | Recta                 | Sigmoide (S)               |\n",
        "| Problema t√≠pico | Predicci√≥n de valores | Clasificaci√≥n binaria      |\n",
        "| Ejemplo         | Precio de casas       | ¬øEl cliente abandona o no? |\n",
        "| M√©trica com√∫n   | MSE, R¬≤               | AUC, F1, LogLoss           |\n",
        "\n",
        "---\n",
        "\n",
        "üëâ Una buena forma de recordarlo:\n",
        "\n",
        "* **Lineal = l√≠nea recta = predecir n√∫meros**.\n",
        "* **Log√≠stica = l√≥gica binaria = predecir s√≠/no con probabilidades**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe1ea61e",
      "metadata": {
        "id": "fe1ea61e"
      },
      "source": [
        "# Introducci√≥n al Machine Learning con Scikit-learn\n",
        "\n",
        "## Objetivos\n",
        "- Comprender qu√© es aprendizaje supervisado.\n",
        "- Diferenciar entre regresi√≥n y clasificaci√≥n.\n",
        "- Usar Scikit-learn para entrenar un primer modelo.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Aprendizaje Supervisado\n",
        "- **Regresi√≥n**: predice valores num√©ricos (ej: precio de una casa).  \n",
        "- **Clasificaci√≥n**: predice categor√≠as (ej: churn = s√≠/no).  \n",
        "\n",
        "Pipeline general:\n",
        "1. Preparar datos.  \n",
        "2. Dividir en train/test.  \n",
        "3. Entrenar modelo.  \n",
        "4. Evaluar resultados.  \n",
        "\n",
        "---\n",
        "\n",
        "## 2. Dataset de Ejemplo: Boston Housing (Regresi√≥n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca1b4661",
      "metadata": {
        "id": "ca1b4661"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "import pandas as pd\n",
        "\n",
        "data = fetch_california_housing(as_frame=True)\n",
        "df = data.frame\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83a4033f",
      "metadata": {
        "id": "83a4033f"
      },
      "source": [
        "---\n",
        "\n",
        "## 3. Dataset de Ejemplo: Telco Churn (Clasificaci√≥n)\n",
        "Este c√≥digo es un **preprocesamiento b√°sico para un modelo de machine learning** sobre el dataset de *Telco Customer Churn* (abandono de clientes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ede92ce-91d0-454f-b3fd-75a3c4d0069b",
      "metadata": {
        "id": "6ede92ce-91d0-454f-b3fd-75a3c4d0069b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/Geo-y20/Telco-Customer-Churn-/refs/heads/main/Telco%20Customer%20Churn.csv\"\n",
        "df = pd.read_csv(url, index_col=0)\n",
        "df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"], errors=\"coerce\")\n",
        "df[\"TotalCharges\"] = df[\"TotalCharges\"].fillna(df[\"MonthlyCharges\"])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa3b22ee-9c26-409b-b671-99e4b5d979e5",
      "metadata": {
        "id": "fa3b22ee-9c26-409b-b671-99e4b5d979e5"
      },
      "outputs": [],
      "source": [
        "# One-Hot Encoding con pandas\n",
        "df = pd.get_dummies(df, drop_first=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0607028c-c53f-441f-a8ca-90e692cc6726",
      "metadata": {
        "id": "0607028c-c53f-441f-a8ca-90e692cc6726"
      },
      "outputs": [],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75693bbe-c746-430d-8ae2-a78d9479e78b",
      "metadata": {
        "id": "75693bbe-c746-430d-8ae2-a78d9479e78b"
      },
      "outputs": [],
      "source": [
        "# Separar variables y target\n",
        "X = df.drop(\"Churn_Yes\", axis=1)  # porque get_dummies crea Churn_No y Churn_Yes\n",
        "y = df[\"Churn_Yes\"]\n",
        "\n",
        "# Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28b5d09c-7ad8-4ffe-8712-6e27567ff824",
      "metadata": {
        "id": "28b5d09c-7ad8-4ffe-8712-6e27567ff824"
      },
      "source": [
        "### 3.1. Importaci√≥n de librer√≠as\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "```\n",
        "\n",
        "* **pandas**: para manipular datos en forma de tablas (dataframes).\n",
        "* **train\\_test\\_split**: funci√≥n de Scikit-learn para dividir los datos en conjuntos de entrenamiento y prueba.\n",
        "\n",
        "---\n",
        "\n",
        "### 3.2. Cargar el dataset\n",
        "\n",
        "```python\n",
        "url = \"https://raw.githubusercontent.com/Geo-y20/Telco-Customer-Churn-/refs/heads/main/Telco%20Customer%20Churn.csv\"\n",
        "df = pd.read_csv(url, index_col=0)\n",
        "```\n",
        "\n",
        "* Se descarga un CSV desde GitHub.\n",
        "* `index_col=0` indica que la primera columna del CSV debe usarse como √≠ndice del dataframe.\n",
        "\n",
        "---\n",
        "\n",
        "### 3.3. One-Hot Encoding (variables categ√≥ricas a num√©ricas)\n",
        "\n",
        "```python\n",
        "df = pd.get_dummies(df, drop_first=True)\n",
        "```\n",
        "\n",
        "* `pd.get_dummies()` convierte las variables categ√≥ricas en variables num√©ricas binarias (0/1).\n",
        "* `drop_first=True` elimina una de las categor√≠as para evitar la **trampa de la multicolinealidad** (cuando una categor√≠a es redundante).\n",
        "\n",
        "Ejemplo: si hay una columna `Gender` con valores `Male` y `Female`, se crear√° solo `Gender_Male` (0 o 1), y se elimina `Gender_Female` porque es redundante.\n",
        "\n",
        "---\n",
        "\n",
        "### 3.4. Separar variables predictoras (X) y la variable objetivo (y)\n",
        "\n",
        "```python\n",
        "X = df.drop(\"Churn_Yes\", axis=1)  \n",
        "y = df[\"Churn_Yes\"]\n",
        "```\n",
        "\n",
        "* `Churn` es la variable objetivo (si el cliente se dio de baja o no).\n",
        "* Como `get_dummies` crea `Churn_No` y `Churn_Yes`, se toma `Churn_Yes` como target.\n",
        "* `X` son todas las dem√°s columnas (variables explicativas).\n",
        "\n",
        "---\n",
        "\n",
        "### 3.5. Dividir en entrenamiento y prueba\n",
        "\n",
        "```python\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "```\n",
        "\n",
        "* Divide los datos en:\n",
        "\n",
        "  * **70%** entrenamiento (`X_train`, `y_train`)\n",
        "  * **30%** prueba (`X_test`, `y_test`)\n",
        "* `random_state=42` asegura que la divisi√≥n sea **reproducible** (si corres el c√≥digo varias veces, el resultado ser√° el mismo).\n",
        "\n",
        "---\n",
        "\n",
        "### 3.6. Mostrar las dimensiones\n",
        "\n",
        "```python\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "```\n",
        "\n",
        "* Muestra el n√∫mero de **filas (observaciones)** y **columnas (features)** en cada conjunto.\n",
        "* Sirve para verificar que la divisi√≥n se hizo correctamente.\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ **Resumen**:\n",
        "Este script descarga un dataset de clientes, transforma las variables categ√≥ricas en num√©ricas, separa la variable objetivo (*churn*), divide los datos en entrenamiento y prueba, y finalmente imprime las dimensiones resultantes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72108bb2-3e3b-4ae1-a185-29a3fc7a2b53",
      "metadata": {
        "id": "72108bb2-3e3b-4ae1-a185-29a3fc7a2b53"
      },
      "source": [
        "---\n",
        "\n",
        "## 4. Ejemplo: Clasificaci√≥n (Logistic Regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf352420-cf37-433d-b2c3-d40f637a61ab",
      "metadata": {
        "id": "bf352420-cf37-433d-b2c3-d40f637a61ab"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Definir y entrenar modelo\n",
        "clf = LogisticRegression(max_iter=5000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predicciones\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Reporte de m√©tricas\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Matriz de confusi√≥n\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Matriz de confusi√≥n:\\n\", cm)\n",
        "\n",
        "# Visualizar con mapa de calor\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=clf.classes_,\n",
        "            yticklabels=clf.classes_)\n",
        "plt.xlabel(\"Predicciones\")\n",
        "plt.ylabel(\"Valores reales\")\n",
        "plt.title(\"Matriz de Confusi√≥n\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1202615b-3c8c-4403-a1cf-600ae1aaaee7",
      "metadata": {
        "id": "1202615b-3c8c-4403-a1cf-600ae1aaaee7"
      },
      "source": [
        "### üìå Recordatorio de la f√≥rmula\n",
        "\n",
        "1. **Logit (score lineal):**\n",
        "\n",
        "$$\n",
        "z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n\n",
        "$$\n",
        "\n",
        "2. **Sigmoide:**\n",
        "\n",
        "$$\n",
        "p = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31ae8860-7f3b-423e-9a17-74d39f109a43",
      "metadata": {
        "id": "31ae8860-7f3b-423e-9a17-74d39f109a43"
      },
      "outputs": [],
      "source": [
        "# --- Coeficientes, intercepto, logit y probabilidad\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# 1) Extraer par√°metros del modelo (asumiendo clf = LogisticRegression ya entrenado)\n",
        "beta  = np.asarray(clf.coef_, dtype=float).ravel()   # (n_features,)\n",
        "beta0 = float(np.asarray(clf.intercept_, dtype=float).ravel()[0])\n",
        "\n",
        "print(\"Intercepto (beta0):\", beta0)\n",
        "print(\"Coeficientes (beta):\", beta)\n",
        "\n",
        "# 2) Nombres de features si X_test es DataFrame; si no, nombres gen√©ricos\n",
        "try:\n",
        "    feature_names = list(X_test.columns)\n",
        "except AttributeError:\n",
        "    # Si es ndarray/sparse, construimos nombres gen√©ricos\n",
        "    n_features = X_test.shape[1]\n",
        "    feature_names = [f\"x{i}\" for i in range(n_features)]\n",
        "\n",
        "# 3) Asegurar que X_test sea num√©rico ndarray (maneja DataFrame/ndarray)\n",
        "if hasattr(X_test, \"values\"):   # DataFrame/Series de pandas\n",
        "    X_mat = X_test.values\n",
        "else:\n",
        "    X_mat = X_test\n",
        "\n",
        "# Si X_mat es sparse, convi√©rtelo a denso de forma segura (o usa operaciones sparse si prefieres)\n",
        "try:\n",
        "    import scipy.sparse as sp\n",
        "    if sp.issparse(X_mat):\n",
        "        X_mat = X_mat.toarray()\n",
        "except Exception:\n",
        "    # Si no hay scipy o no es sparse, seguimos\n",
        "    pass\n",
        "\n",
        "X_mat = np.asarray(X_mat, dtype=float)\n",
        "\n",
        "# 4) C√°lculo del logit (score lineal) y probabilidad con estabilidad num√©rica\n",
        "z = beta0 + X_mat @ beta\n",
        "\n",
        "# Evitar overflow en exp: recorta z a [-709, 709] (aprox. l√≠mite de np.exp para float64)\n",
        "z_clipped = np.clip(z, -709, 709)\n",
        "p = 1.0 / (1.0 + np.exp(-z_clipped))  # sigmoide\n",
        "\n",
        "# 5) Chequeo contra predict_proba de scikit-learn\n",
        "p_sklearn = clf.predict_proba(X_test)[:, 1]\n",
        "print(\"Max |p - p_sklearn|:\", float(np.max(np.abs(p - p_sklearn))))\n",
        "\n",
        "# 6) Impresi√≥n ordenada de coeficientes\n",
        "print(\"\\nCoeficientes por feature:\")\n",
        "for name, b in zip(feature_names, beta):\n",
        "    print(f\"{name:20s}: {b: .6f} \")\n",
        "    #print(f\"{name:20s}: {b: .6f}   (odds ratio: {np.exp(b):.6f})\")\n",
        "\n",
        "# 7) (Opcional) Mostrar algunos ejemplos de salida\n",
        "print(\"\\nPrimeras 5 probabilidades calculadas manualmente:\", p[:5])\n",
        "print(\"Primeras 5 probabilidades de predict_proba()   :\", p_sklearn[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "578cf754-3d8f-4e55-841b-cf643e05c2e3",
      "metadata": {
        "id": "578cf754-3d8f-4e55-841b-cf643e05c2e3"
      },
      "outputs": [],
      "source": [
        "order = np.argsort(z)\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.scatter(z, y_test, alpha=0.35, label=\"Observaciones (0/1)\")\n",
        "plt.plot(z[order], p[order], label=\"Sigmoide sobre z\")\n",
        "plt.xlabel(\"Logit (z = Œ≤0 + XŒ≤)\")\n",
        "plt.ylabel(\"P(Y=1|X)\")\n",
        "plt.title(\"Sigmoide respecto al logit (score lineal)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a71a4d13-ac39-4c5d-8560-d7863abe0a43",
      "metadata": {
        "id": "a71a4d13-ac39-4c5d-8560-d7863abe0a43"
      },
      "source": [
        "### Qu√© hace el c√≥digo\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Definir y entrenar modelo\n",
        "clf = LogisticRegression(max_iter=5000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predicciones\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Reporte de m√©tricas\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Matriz de confusi√≥n\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Matriz de confusi√≥n:\\n\", cm)\n",
        "\n",
        "# Visualizar con mapa de calor\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=clf.classes_,\n",
        "            yticklabels=clf.classes_)\n",
        "plt.xlabel(\"Predicciones\")\n",
        "plt.ylabel(\"Valores reales\")\n",
        "plt.title(\"Matriz de Confusi√≥n\")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "* **`LogisticRegression(max_iter=5000)`**: clasificador para problemas binarios/multiclase. `max_iter` alto evita warnings de no convergencia.\n",
        "* **`.fit(X_train, y_train)`**: entrena con el set de entrenamiento.\n",
        "* **`.predict(X_test)`**: predice etiquetas en el set de prueba.\n",
        "* **`classification_report`**: imprime *precision, recall, f1-score, support* por clase, m√°s promedios (macro/weighted) y *accuracy* global.\n",
        "* **`confusion_matrix`**: devuelve la tabla de aciertos/errores por clase.\n",
        "* **`sns.heatmap(...)`**: dibuja la matriz; las etiquetas de ejes usan `clf.classes_` (en tu caso, `['False','True']`).\n",
        "\n",
        "---\n",
        "\n",
        "### Reporte\n",
        "\n",
        "```\n",
        "              precision    recall  f1-score   support\n",
        "False             0.85      0.90      0.87      1539\n",
        "True              0.68      0.57      0.62       574\n",
        "accuracy                               0.81      2113\n",
        "macro avg         0.77      0.74      0.75      2113\n",
        "weighted avg      0.80      0.81      0.81      2113\n",
        "```\n",
        "\n",
        "* **Accuracy = 0.81**: el 81% de las predicciones totales fue correcto.\n",
        "* **Clase `False` (no churn)**: muy buen *recall* (0.90) ‚áí el modelo identifica bien a quienes **se quedan**.\n",
        "* **Clase `True` (churn)**: *recall* 0.57 ‚áí detecta **57%** de quienes se van; *precision* 0.68 ‚áí de los que predijo como churn, el **68%** realmente se va.\n",
        "* **Macro avg**: promedio simple entre clases (√∫til si est√°n desbalanceadas).\n",
        "* **Weighted avg**: promedio ponderado por *support* (tama√±o de cada clase).\n",
        "\n",
        "### Matriz de confusi√≥n\n",
        "\n",
        "```\n",
        "[[1386  153]\n",
        " [ 245  329]]\n",
        "```\n",
        "\n",
        "* Filas = **valores reales** (`y_test`)\n",
        "* Columnas = **predicciones** (`y_pred`)\n",
        "* Orden: `False` primero, `True` despu√©s\n",
        "\n",
        "Entonces:\n",
        "\n",
        "* **TN (True Negative) = 1386** ‚Üí Reales **False** (no churn) predichos **False**.\n",
        "  Aciertos en ‚Äúno se va‚Äù.\n",
        "* **FP (False Positive) = 153** ‚Üí Reales **False**, predichos **True**.\n",
        "  ‚ÄúFalsa alarma‚Äù: el modelo dijo churn pero no se fueron.\n",
        "  *Impacto*: podr√≠as ofrecer incentivos innecesarios.\n",
        "* **FN (False Negative) = 245** ‚Üí Reales **True**, predichos **False**.\n",
        "  ‚ÄúSe te escapan‚Äù: se fueron y no los detectaste.\n",
        "  *Impacto*: p√©rdida de clientes sin acci√≥n preventiva (suele ser el error m√°s costoso en churn).\n",
        "* **TP (True Positive) = 329** ‚Üí Reales **True**, predichos **True**.\n",
        "  Aciertos detectando quienes se van.\n",
        "\n",
        "### M√©tricas derivadas desde la matriz\n",
        "\n",
        "Con TN=1386, FP=153, FN=245, TP=329 (total=2113):\n",
        "\n",
        "* **Accuracy** = (TP+TN)/total ‚âà **0.812** (‚âà 0.81)\n",
        "* **Precision (churn=True)** = TP/(TP+FP) ‚âà **0.683**\n",
        "* **Recall (churn=True)** = TP/(TP+FN) ‚âà **0.573**\n",
        "* **F1 (churn=True)** ‚âà **0.623**\n",
        "\n",
        "### ¬øQu√© puede hacerse para mejorar la detecci√≥n de churn (reducir FN)?\n",
        "\n",
        "* **Ajustar el umbral de decisi√≥n**: en lugar de `predict` (umbral 0.5), usa `predict_proba` y mueve el umbral para priorizar *recall* en la clase `True`.\n",
        "  *M√°s recall* ‚áí menos FN (pero suben FP).\n",
        "* **`class_weight='balanced'`** en la Regresi√≥n Log√≠stica si hay desbalance.\n",
        "* **Estandarizar** las variables num√©ricas (`StandardScaler`) antes de entrenar.\n",
        "* **Curvas ROC y Precision-Recall** para elegir el umbral √≥ptimo seg√∫n tu coste FP/FN.\n",
        "* **Ingenier√≠a de variables / modelos alternativos** (√°rboles/boosting) si procede.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c858ca07",
      "metadata": {
        "id": "c858ca07"
      },
      "source": [
        "---\n",
        "\n",
        "## 5. Ejemplo: Regresi√≥n Lineal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ee0e9d4",
      "metadata": {
        "id": "0ee0e9d4"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "import pandas as pd\n",
        "\n",
        "data = fetch_california_housing(as_frame=True)\n",
        "df = data.frame\n",
        "\n",
        "# Definir modelo\n",
        "model = LinearRegression()\n",
        "model.fit(data.data, data.target)\n",
        "\n",
        "# Predicciones\n",
        "preds = model.predict(data.data)\n",
        "\n",
        "# M√©tricas\n",
        "print(\"MSE:\", mean_squared_error(data.target, preds))\n",
        "print(\"R2:\", r2_score(data.target, preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91870444-b3bb-40e8-a387-f51fcbdb5b6e",
      "metadata": {
        "id": "91870444-b3bb-40e8-a387-f51fcbdb5b6e"
      },
      "outputs": [],
      "source": [
        "# Intercepto y coeficientes\n",
        "print(\"\\nIntercepto (Œ≤0):\", model.intercept_)\n",
        "print(\"\\nCoeficientes por variable:\")\n",
        "for name, coef in zip(data.feature_names, model.coef_):\n",
        "    print(f\"{name:15s}: {coef:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "452e1ad2-9fed-4c55-9f65-2a66c8358e71",
      "metadata": {
        "id": "452e1ad2-9fed-4c55-9f65-2a66c8358e71"
      },
      "source": [
        "### Recordatorio del modelo\n",
        "\n",
        "El modelo ajustado es:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\beta_0 + \\beta_1 \\cdot x_1 + \\beta_2 \\cdot x_2 + \\dots + \\beta_p \\cdot x_p\n",
        "$$\n",
        "\n",
        "donde:\n",
        "\n",
        "* $\\hat{y}$ = valor medio de la vivienda (en **cientos de miles de d√≥lares**),\n",
        "* $x_j$ = variables predictoras,\n",
        "* $\\beta_j$ = coeficiente de la variable $j$.\n",
        "\n",
        "---\n",
        "\n",
        "### C√≥mo leer los coeficientes\n",
        "\n",
        "* **Intercepto (Œ≤‚ÇÄ)**: valor esperado de la variable objetivo cuando todas las features son 0. (No suele tener interpretaci√≥n realista en datasets como este, porque ‚Äú0 habitaciones‚Äù o ‚Äú0 latitud‚Äù no son escenarios v√°lidos).\n",
        "* **Coeficientes (Œ≤)**: representan el **cambio promedio en el valor de la vivienda (en 100k USD)** por un incremento de 1 unidad en la variable, manteniendo las dem√°s constantes.\n",
        "\n",
        "Ejemplo con las variables m√°s relevantes del dataset:\n",
        "\n",
        "| Variable                                       | Interpretaci√≥n del coeficiente (ejemplo con valores t√≠picos)                                                                            |\n",
        "| ---------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| **MedInc** (ingreso medio)                     | Si el ingreso medio en un bloque sube en 1 (es decir, 10.000 USD anuales), el precio medio de las casas aumenta en ‚âà `Œ≤` √ó 100k USD.    |\n",
        "| **HouseAge** (edad de la casa en a√±os)         | Un a√±o adicional en la antig√ºedad promedio de las casas se asocia con `Œ≤` √ó 100k USD en el precio, manteniendo todo lo dem√°s constante. |\n",
        "| **AveRooms** (habitaciones promedio por hogar) | Cada habitaci√≥n adicional promedio est√° asociada con un cambio de `Œ≤` √ó 100k USD en el precio medio.                                    |\n",
        "| **Population** (poblaci√≥n en el bloque)        | Cada incremento de 1 persona se asocia con un cambio muy peque√±o en el valor (`Œ≤` cercano a 0).                                         |\n",
        "| **Latitude / Longitude**                       | Tienen coeficientes negativos grandes porque la ubicaci√≥n (m√°s al norte/sur/oeste/este) influye fuertemente en el valor.                |\n",
        "\n",
        "---\n",
        "\n",
        "### Ejemplo con valores concretos\n",
        "\n",
        "Si el coeficiente de `MedInc` fuera ‚âà **0.437**, significa que:\n",
        "\n",
        "* Aumentar el ingreso medio en **10.000 USD** se asocia con un aumento de:\n",
        "\n",
        "  $$\n",
        "  0.437 \\times 100{,}000 = 43{,}700 \\, \\text{USD}\n",
        "  $$\n",
        "\n",
        "  en el valor medio de la vivienda.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9636dc7-18c2-476c-8023-0143dfbd5bdf",
      "metadata": {
        "id": "c9636dc7-18c2-476c-8023-0143dfbd5bdf"
      },
      "source": [
        "### üìä Interpretaci√≥n del modelo\n",
        "\n",
        "### Intercepto (Œ≤‚ÇÄ)\n",
        "\n",
        "* **-36.94** ‚Üí el valor esperado de una vivienda ser√≠a negativo si todas las variables fueran 0.\n",
        "  üëâ Esto no tiene sentido pr√°ctico (ninguna casa tiene 0 habitaciones, ingreso = 0, latitud = 0, etc.).\n",
        "  El intercepto solo asegura que el modelo ‚Äúpase‚Äù cerca de los datos reales.\n",
        "\n",
        "---\n",
        "\n",
        "### Coeficientes\n",
        "\n",
        "| Variable                                               | Coeficiente (Œ≤) | Interpretaci√≥n pr√°ctica                                                                                                                                                                                                              |\n",
        "| ------------------------------------------------------ | --------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n",
        "| **MedInc** (ingreso medio, en decenas de miles de USD) | **0.4367**      | Por cada **+10,000 USD** de ingreso medio en el distrito, el valor medio de las casas aumenta en **‚âà 43,700 USD**.                                                                                                                   |\n",
        "| **HouseAge** (edad promedio en a√±os)                   | **0.0094**      | Cada **+1 a√±o** de antig√ºedad promedio est√° asociado con **‚âà 940 USD m√°s** en el precio medio (un efecto peque√±o).                                                                                                                   |\n",
        "| **AveRooms** (habitaciones promedio por hogar)         | **-0.1073**     | Una **habitaci√≥n adicional en promedio** se asocia con **‚âà -10,732 USD** en el valor. ‚ö†Ô∏è Esto puede sonar raro, pero refleja multicolinealidad: distritos con muchas habitaciones por hogar tienden a ser m√°s baratos en California. |\n",
        "| **AveBedrms** (dormitorios promedio por hogar)         | **0.6451**      | Cada dormitorio adicional en promedio se asocia con **‚âà 64,506 USD m√°s**.                                                                                                                                                            |\n",
        "| **Population** (n√∫mero de personas en el distrito)     | **-0.000004**   | Cada persona adicional reduce el valor en apenas **‚âà -0.40 USD** (efecto casi nulo).                                                                                                                                                 |\n",
        "| **AveOccup** (personas promedio por hogar)             | **-0.0038**     | Cada persona extra por hogar est√° asociada con **‚âà -378 USD** en el valor medio.                                                                                                                                                     |\n",
        "| **Latitude** (latitud del distrito)                    | **-0.4213**     | Moverse 1 grado al norte (‚âà 111 km) reduce el valor medio en **‚âà -42,131 USD**.                                                                                                                                                      |\n",
        "| **Longitude** (longitud del distrito)                  | **-0.4345**     | Moverse 1 grado al este (‚âà 85 km en California) reduce el valor medio en **‚âà -43,451 USD**.                                                                                                                                          |\n",
        "\n",
        "---\n",
        "\n",
        "## üîë Claves de interpretaci√≥n\n",
        "\n",
        "* **MedInc** es el predictor m√°s fuerte y l√≥gico: m√°s ingreso ‚áí casas m√°s caras.\n",
        "* **Latitude / Longitude** capturan la ubicaci√≥n: en California, estar m√°s al norte o este (lejos de la costa sur/Los √Ångeles/San Francisco) reduce el valor.\n",
        "* **AveRooms** negativo y **AveBedrms** positivo reflejan correlaciones internas: distritos con muchas habitaciones suelen estar en zonas rurales/suburbanas m√°s baratas.\n",
        "* **Population** tiene un efecto pr√°cticamente nulo.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e2a7e4f-0c39-4b7d-950b-fe51108934f7",
      "metadata": {
        "id": "8e2a7e4f-0c39-4b7d-950b-fe51108934f7"
      },
      "source": [
        "### Qu√© hace el c√≥digo\n",
        "\n",
        "### 5.1. Importar librer√≠as\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "```\n",
        "\n",
        "* **LinearRegression**: el modelo de regresi√≥n lineal de scikit-learn.\n",
        "* **mean\\_squared\\_error**: funci√≥n para calcular el **error cuadr√°tico medio (MSE)**.\n",
        "* **r2\\_score**: funci√≥n para calcular el **coeficiente de determinaci√≥n (R¬≤)**.\n",
        "\n",
        "---\n",
        "\n",
        "### 5.2. Definir y entrenar el modelo\n",
        "\n",
        "```python\n",
        "model = LinearRegression()\n",
        "model.fit(data.data, data.target)\n",
        "```\n",
        "\n",
        "* Se crea un objeto `LinearRegression`.\n",
        "* `.fit(X, y)` entrena el modelo:\n",
        "\n",
        "  * `X = data.data` ‚Üí las variables predictoras (caracter√≠sticas).\n",
        "  * `y = data.target` ‚Üí la variable objetivo (lo que queremos predecir).\n",
        "\n",
        "El modelo busca la mejor l√≠nea (o hiperplano) que se ajuste a los datos.\n",
        "\n",
        "---\n",
        "\n",
        "### 5.3. Hacer predicciones\n",
        "\n",
        "```python\n",
        "preds = model.predict(data.data)\n",
        "```\n",
        "\n",
        "* Se usan las mismas variables (`data.data`) para generar predicciones.\n",
        "* `preds` es un array con los valores estimados por el modelo para cada observaci√≥n.\n",
        "\n",
        "---\n",
        "\n",
        "### 5.4. Evaluar el modelo con m√©tricas\n",
        "\n",
        "```python\n",
        "print(\"MSE:\", mean_squared_error(data.target, preds))\n",
        "print(\"R2:\", r2_score(data.target, preds))\n",
        "```\n",
        "\n",
        "* **MSE (Mean Squared Error)**:\n",
        "\n",
        "  * Calcula el promedio de los errores al cuadrado:\n",
        "\n",
        "    $$\n",
        "    MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
        "    $$\n",
        "  * Cuanto m√°s bajo, mejor es el ajuste.\n",
        "\n",
        "* **R¬≤ (Coeficiente de determinaci√≥n)**:\n",
        "\n",
        "  * Mide qu√© porcentaje de la variabilidad en `y` es explicado por el modelo.\n",
        "  * Valores:\n",
        "\n",
        "    * **1** ‚Üí ajuste perfecto.\n",
        "    * **0** ‚Üí el modelo no explica nada.\n",
        "    * **Negativo** ‚Üí el modelo es peor que predecir la media.\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ **Resumen:**\n",
        "Este c√≥digo entrena un modelo de **regresi√≥n lineal** con los datos (`data.data`, `data.target`), genera predicciones, y luego eval√∫a el modelo mostrando dos m√©tricas:\n",
        "\n",
        "* **MSE** (qu√© tan lejos est√°n las predicciones de los valores reales).\n",
        "* **R¬≤** (qu√© tan bien explica el modelo la variaci√≥n de los datos).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c8d6756",
      "metadata": {
        "id": "4c8d6756"
      },
      "source": [
        "---\n",
        "\n",
        "## 6. Preguntas de Discusi√≥n\n",
        "\n",
        "1. ¬øQu√© diferencias hay entre regresi√≥n y clasificaci√≥n?\n",
        "2. ¬øQu√© tan importante es separar train/test?\n",
        "3. ¬øPor qu√© no debemos evaluar un modelo solo con accuracy?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31d41b78-4978-4912-9212-58ecdbf6a4e8",
      "metadata": {
        "id": "31d41b78-4978-4912-9212-58ecdbf6a4e8"
      },
      "source": [
        "### 1. ¬øQu√© diferencias hay entre **regresi√≥n** y **clasificaci√≥n**?\n",
        "\n",
        "* **Regresi√≥n**:\n",
        "\n",
        "  * Predice un **valor continuo** (num√©rico).\n",
        "  * Ejemplo: predecir el precio de una casa, la temperatura, las ventas mensuales.\n",
        "  * M√©tricas comunes: MSE, RMSE, MAE, R¬≤.\n",
        "\n",
        "* **Clasificaci√≥n**:\n",
        "\n",
        "  * Predice una **categor√≠a** (discreta).\n",
        "  * Puede ser binaria (s√≠/no, churn/no churn) o multiclase (tipo de flor, enfermedad A/B/C).\n",
        "  * M√©tricas comunes: accuracy, precision, recall, F1, ROC AUC.\n",
        "\n",
        "üëâ Diferencia clave: **regresi√≥n = valores continuos**, **clasificaci√≥n = clases o categor√≠as**.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. ¬øQu√© tan importante es separar **train/test**?\n",
        "\n",
        "* **Fundamental** ‚úÖ.\n",
        "* Si entrenas y eval√∫as en los mismos datos ‚Üí el modelo se \"aprende de memoria\" (overfitting) y obtienes m√©tricas artificialmente buenas.\n",
        "* El **conjunto de entrenamiento (train)** se usa para ajustar los par√°metros del modelo.\n",
        "* El **conjunto de prueba (test)** simula datos nuevos nunca vistos, y te da una estimaci√≥n real de c√≥mo rendir√° el modelo en producci√≥n.\n",
        "* A veces tambi√©n se usa un **validation set** o **cross-validation** para afinar hiperpar√°metros antes de probar en test.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. ¬øPor qu√© no debemos evaluar un modelo solo con **accuracy**?\n",
        "\n",
        "* El accuracy mide:\n",
        "\n",
        "  $$\n",
        "  \\text{Accuracy} = \\frac{\\text{Aciertos}}{\\text{Total de ejemplos}}\n",
        "  $$\n",
        "\n",
        "  Es √∫til cuando las clases est√°n balanceadas.\n",
        "\n",
        "* Pero en problemas **desbalanceados** puede ser enga√±oso:\n",
        "\n",
        "  * Ejemplo: dataset de churn con 90% clientes que se quedan y 10% que se van.\n",
        "  * Si el modelo predice **siempre ‚Äúse queda‚Äù**, tendr√° **90% accuracy** pero **0% recall para churn** (nunca detecta a quienes se van).\n",
        "  * En este caso, m√©tricas como **precision, recall, F1-score, ROC-AUC** son m√°s informativas.\n",
        "\n",
        "üëâ En tareas cr√≠ticas (fraude, churn, c√°ncer), **recall y precision son m√°s importantes que accuracy**.\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ **Resumen**:\n",
        "\n",
        "1. Regresi√≥n predice valores continuos, clasificaci√≥n categor√≠as.\n",
        "2. Separar train/test evita sobreajuste y mide rendimiento real.\n",
        "3. Accuracy puede ser enga√±oso, especialmente con clases desbalanceadas: hay que mirar m√©tricas adicionales.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fa0e665-d99e-4e91-a669-e2e25e2bed99",
      "metadata": {
        "id": "2fa0e665-d99e-4e91-a669-e2e25e2bed99"
      },
      "source": [
        "## üìä Comparaci√≥n: Regresi√≥n vs Clasificaci√≥n\n",
        "\n",
        "| Aspecto                             | **Regresi√≥n** (valores continuos)                                                                                              | **Clasificaci√≥n** (clases/categor√≠as)                                           |\n",
        "| ----------------------------------- | ------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------- |\n",
        "| **Ejemplo**                         | Predecir el precio de una casa, la temperatura, ventas mensuales                                                               | Predecir si un cliente har√° churn, si un email es spam/no spam                  |\n",
        "| **Variable objetivo (y)**           | Num√©rica continua (ej. 250.45, 32.7)                                                                                           | Discreta (ej. {0,1} o {Rojo, Azul, Verde})                                      |\n",
        "| **Algoritmos t√≠picos**              | Regresi√≥n lineal, regresi√≥n polin√≥mica, SVR, Random Forest Regressor                                                           | Regresi√≥n log√≠stica, SVM, √Årboles de decisi√≥n, Random Forest, Redes Neuronales  |\n",
        "| **M√©tricas comunes**                | - MSE (Error Cuadr√°tico Medio)<br>- RMSE (Ra√≠z del MSE)<br>- MAE (Error Absoluto Medio)<br>- R¬≤ (Coeficiente de determinaci√≥n) | - Accuracy<br>- Precision<br>- Recall (Sensibilidad)<br>- F1-score<br>- AUC-ROC |\n",
        "| **Interpretaci√≥n**                  | Cu√°n cerca est√°n las predicciones de los valores reales (se mide error en unidades del target)                                 | Qu√© tan bien clasifica ejemplos en sus categor√≠as correctas                     |\n",
        "| **Riesgo al usar solo una m√©trica** | Usar solo R¬≤ puede ser enga√±oso si los datos no son lineales                                                                   | Usar solo Accuracy es peligroso si las clases est√°n desbalanceadas              |\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ Con esta tabla queda claro que:\n",
        "\n",
        "* En **regresi√≥n** interesa **minimizar el error num√©rico**.\n",
        "* En **clasificaci√≥n** interesa **equilibrar precisi√≥n, recall y F1**, sobre todo en datasets desbalanceados.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41d0f758-6617-4846-84d7-e76e0dc27db3",
      "metadata": {
        "id": "41d0f758-6617-4846-84d7-e76e0dc27db3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}