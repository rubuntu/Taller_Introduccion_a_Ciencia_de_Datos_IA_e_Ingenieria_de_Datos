{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rubuntu/Taller_Introduccion_a_Ciencia_de_Datos_IA_e_Ingenieria_de_Datos/blob/main/sesion_11_comparacion_entre_mlp_y_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29b97f0c",
      "metadata": {
        "id": "29b97f0c"
      },
      "source": [
        "# Clasificaci√≥n de Perros y Gatos: Comparaci√≥n entre MLP y CNN\n",
        "\n",
        "## Objetivos\n",
        "- Cargar y preparar el dataset p√∫blico `microsoft/cats_vs_dogs` desde HuggingFace.\n",
        "- Implementar un pipeline de preprocesamiento con `torchvision.transforms` para im√°genes.\n",
        "- Construir y entrenar dos arquitecturas distintas:\n",
        "  - Un **MLP** (perceptr√≥n multicapa) usando im√°genes aplanadas.\n",
        "  - Una **CNN** simple, con convoluciones y pooling.\n",
        "- Comparar el rendimiento de ambos modelos en t√©rminos de **accuracy**.\n",
        "- Reflexionar sobre las ventajas y desventajas de MLPs y CNNs en problemas de visi√≥n.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from datasets import load_dataset\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Usando dispositivo:\", device)"
      ],
      "metadata": {
        "id": "AWraGD4sZj8h"
      },
      "id": "AWraGD4sZj8h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e58ec881-2278-4a67-9318-8670f757d727",
      "metadata": {
        "id": "e58ec881-2278-4a67-9318-8670f757d727"
      },
      "source": [
        "## 1. Cargar dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "991f0b95",
      "metadata": {
        "id": "991f0b95"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"microsoft/cats_vs_dogs\")\n",
        "\n",
        "# Dividir en 80% train, 20% test\n",
        "dataset = dataset[\"train\"].train_test_split(test_size=0.2)\n",
        "\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af95ce04-f635-410b-a813-025b25bb12e5",
      "metadata": {
        "id": "af95ce04-f635-410b-a813-025b25bb12e5"
      },
      "source": [
        "## 2. Transformaciones\n",
        "- Para el **MLP**: reducimos resoluci√≥n a 64x64 y aplanamos la imagen.\n",
        "- Para la **CNN**: usamos im√°genes 128x128 en formato RGB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3cdfd39",
      "metadata": {
        "id": "d3cdfd39"
      },
      "outputs": [],
      "source": [
        "transform_mlp = transforms.Compose([\n",
        "    transforms.Resize((64,64)),\n",
        "    transforms.Lambda(lambda img: img.convert(\"RGB\")),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: x.view(-1))  # aplanar\n",
        "])\n",
        "\n",
        "transform_cnn = transforms.Compose([\n",
        "    transforms.Resize((128,128)),\n",
        "    transforms.Lambda(lambda img: img.convert(\"RGB\")),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a44ebe8b-cc62-478f-a6ce-c2f4d6790d92",
      "metadata": {
        "id": "a44ebe8b-cc62-478f-a6ce-c2f4d6790d92"
      },
      "source": [
        "## 3. Adaptador HuggingFace ‚Üí PyTorch Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67d9b78e",
      "metadata": {
        "id": "67d9b78e"
      },
      "outputs": [],
      "source": [
        "class CatsDogsDataset(Dataset):\n",
        "    def __init__(self, hf_dataset, transform=None):\n",
        "        self.data = hf_dataset\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.data[idx][\"image\"]\n",
        "        label = self.data[idx][\"labels\"]  # 0 = cat, 1 = dog\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "train_ds_mlp = CatsDogsDataset(dataset[\"train\"], transform=transform_mlp)\n",
        "test_ds_mlp  = CatsDogsDataset(dataset[\"test\"], transform=transform_mlp)\n",
        "\n",
        "train_ds_cnn = CatsDogsDataset(dataset[\"train\"], transform=transform_cnn)\n",
        "test_ds_cnn  = CatsDogsDataset(dataset[\"test\"], transform=transform_cnn)\n",
        "\n",
        "train_dl_mlp = DataLoader(train_ds_mlp, batch_size=32, shuffle=True)\n",
        "test_dl_mlp  = DataLoader(test_ds_mlp, batch_size=32)\n",
        "\n",
        "train_dl_cnn = DataLoader(train_ds_cnn, batch_size=32, shuffle=True)\n",
        "test_dl_cnn  = DataLoader(test_ds_cnn, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb710aab-0afc-47fc-b973-3e20915a852f",
      "metadata": {
        "id": "cb710aab-0afc-47fc-b973-3e20915a852f"
      },
      "source": [
        "## 4. Definir Modelos\n",
        "### 4.1 MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c7eede3",
      "metadata": {
        "id": "0c7eede3"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim=64*64*3, hidden=256):\n",
        "        super(MLP, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 2)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "mlp_model = MLP().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d896131c-5328-4331-8e84-f062f4e2311c",
      "metadata": {
        "id": "d896131c-5328-4331-8e84-f062f4e2311c"
      },
      "source": [
        "### 4.2 CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afed9edd",
      "metadata": {
        "id": "afed9edd"
      },
      "outputs": [],
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),   # 64x64\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),   # 32x32\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),   # 16x16\n",
        "        )\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128*16*16, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 2)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n",
        "\n",
        "cnn_model = SimpleCNN().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b548818b-44d0-423e-9435-362ff0b5bb88",
      "metadata": {
        "id": "b548818b-44d0-423e-9435-362ff0b5bb88"
      },
      "source": [
        "## 5. Funci√≥n de entrenamiento y evaluaci√≥n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec99beea",
      "metadata": {
        "id": "ec99beea"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_dl, test_dl, n_epochs=5, lr=1e-3):\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    opt = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for xb, yb in train_dl:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            preds = model(xb)\n",
        "            loss = loss_fn(preds, yb)\n",
        "\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Evaluaci√≥n\n",
        "        model.eval()\n",
        "        correct, total = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in test_dl:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                preds = model(xb).argmax(dim=1)\n",
        "                correct += (preds == yb).sum().item()\n",
        "                total += yb.size(0)\n",
        "\n",
        "        acc = correct / total\n",
        "        print(f\"Epoch {epoch+1}, Loss={running_loss/len(train_dl):.4f}, Val Acc={acc:.4f}\")\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9f250be-b483-49d1-91d1-7b774c44cc6b",
      "metadata": {
        "id": "f9f250be-b483-49d1-91d1-7b774c44cc6b"
      },
      "source": [
        "## 6. Entrenamiento de ambos modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2192a638",
      "metadata": {
        "id": "2192a638"
      },
      "outputs": [],
      "source": [
        "print(\"Entrenando MLP...\")\n",
        "#acc_mlp = train_model(mlp_model, train_dl_mlp, test_dl_mlp, n_epochs=5, lr=1e-3)\n",
        "acc_mlp = train_model(mlp_model, train_dl_mlp, test_dl_mlp, n_epochs=1, lr=1e-3)\n",
        "\n",
        "print(\"\\nEntrenando CNN...\")\n",
        "acc_cnn = train_model(cnn_model, train_dl_cnn, test_dl_cnn, n_epochs=1, lr=1e-3)\n",
        "\n",
        "print(\"\\nResultados finales:\")\n",
        "print(f\"Accuracy MLP: {acc_mlp:.4f}\")\n",
        "print(f\"Accuracy CNN: {acc_cnn:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d97ef61-6ca9-4536-a362-425446d83a87",
      "metadata": {
        "id": "8d97ef61-6ca9-4536-a362-425446d83a87"
      },
      "source": [
        "## 7. Visualizaci√≥n de algunas predicciones con la CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63c5e8c8",
      "metadata": {
        "id": "63c5e8c8"
      },
      "outputs": [],
      "source": [
        "images, labels = next(iter(test_dl_cnn))\n",
        "images, labels = images.to(device), labels.to(device)\n",
        "preds = cnn_model(images).argmax(dim=1)\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "for i in range(8):\n",
        "    plt.subplot(2,4,i+1)\n",
        "    img = images[i].cpu().permute(1,2,0).numpy()\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"Real: {labels[i].item()}, Pred: {preds[i].item()}\")\n",
        "    plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23ef44aa-e2de-48df-9b05-82c8a55b3698",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "23ef44aa-e2de-48df-9b05-82c8a55b3698"
      },
      "source": [
        "## Preguntas de Discusi√≥n\n",
        "\n",
        "1. ¬øQu√© diferencias arquitect√≥nicas hay entre un MLP y una CNN?\n",
        "2. ¬øPor qu√© el MLP necesita aplanar la imagen mientras que la CNN conserva la estructura espacial?\n",
        "3. ¬øEn qu√© tipo de problemas un MLP podr√≠a ser suficiente y en cu√°les una CNN es claramente superior?\n",
        "4. ¬øQu√© rol cumplen las convoluciones y el pooling en la capacidad de generalizaci√≥n de las CNN?\n",
        "5. ¬øC√≥mo esperas que var√≠e el accuracy entre el MLP y la CNN en este dataset?\n",
        "6. ¬øQu√© t√©cnicas adicionales (data augmentation, regularizaci√≥n, m√°s capas) podr√≠an mejorar todav√≠a m√°s la CNN?\n",
        "7. ¬øQu√© limitaciones pueden encontrarse al entrenar CNNs desde cero con datasets relativamente peque√±os?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "502311de",
      "metadata": {
        "id": "502311de"
      },
      "source": [
        "## üí° Preguntas de Discusi√≥n (desarrolladas)\n",
        "\n",
        "1. **¬øQu√© diferencias arquitect√≥nicas hay entre un MLP y una CNN?**\n",
        "\n",
        "   * Un **MLP (Multilayer Perceptron)** conecta cada p√≠xel de la imagen con la siguiente capa de manera densa ‚Üí todos los p√≠xeles tienen igual importancia y no se aprovecha la estructura espacial.\n",
        "   * Una **CNN (Convolutional Neural Network)** utiliza capas convolucionales que procesan regiones locales de la imagen con filtros compartidos, detectando bordes, texturas y formas de manera jer√°rquica.\n",
        "   * Resultado: la CNN es mucho m√°s eficiente para im√°genes porque reutiliza filtros y respeta la estructura espacial, mientras que un MLP escala mal con la resoluci√≥n.\n",
        "\n",
        "---\n",
        "\n",
        "2. **¬øPor qu√© el MLP necesita aplanar la imagen mientras que la CNN conserva la estructura espacial?**\n",
        "\n",
        "   * El MLP espera vectores como entrada, no tensores 2D o 3D. Por eso, la imagen debe convertirse en un vector largo (aplanado), perdiendo informaci√≥n espacial.\n",
        "   * La CNN en cambio acepta tensores de 3 dimensiones (canales, alto, ancho), lo que le permite aprender directamente patrones espaciales sin necesidad de destruir esa estructura.\n",
        "\n",
        "---\n",
        "\n",
        "3. **¬øEn qu√© tipo de problemas un MLP podr√≠a ser suficiente y en cu√°les una CNN es claramente superior?**\n",
        "\n",
        "   * **MLP suficiente:** problemas con entradas de baja dimensi√≥n (ej. tabulares, texto codificado como embeddings, im√°genes muy peque√±as y simples).\n",
        "   * **CNN superior:** tareas de visi√≥n por computadora donde la estructura espacial es cr√≠tica (clasificaci√≥n de im√°genes, detecci√≥n de objetos, segmentaci√≥n).\n",
        "   * En datasets como perros vs gatos, la CNN es claramente superior porque los rasgos distintivos (orejas, hocico, bigotes) dependen de relaciones espaciales entre p√≠xeles.\n",
        "\n",
        "---\n",
        "\n",
        "4. **¬øQu√© rol cumplen las convoluciones y el pooling en la capacidad de generalizaci√≥n de las CNN?**\n",
        "\n",
        "   * **Convoluciones:** permiten detectar patrones locales (bordes, esquinas, texturas) y reutilizar los mismos filtros en toda la imagen ‚Üí eficiencia y capacidad de reconocer patrones independientemente de su ubicaci√≥n.\n",
        "   * **Pooling (ej. max pooling):** reduce la resoluci√≥n, mantiene la informaci√≥n m√°s relevante y aporta *invarianza traslacional* (el objeto puede moverse un poco y a√∫n se reconoce).\n",
        "   * Juntos, hacen que la CNN generalice mejor y no se limite a memorizar p√≠xeles exactos.\n",
        "\n",
        "---\n",
        "\n",
        "5. **¬øC√≥mo esperas que var√≠e el accuracy entre el MLP y la CNN en este dataset?**\n",
        "\n",
        "   * El **MLP** suele alcanzar un accuracy modesto (‚âà0.55‚Äì0.65) porque ignora relaciones espaciales y necesita m√°s datos para generalizar.\n",
        "   * La **CNN** deber√≠a superar al MLP (‚âà0.70‚Äì0.80), incluso siendo peque√±a, ya que aprovecha mejor la estructura visual.\n",
        "   * La diferencia se vuelve a√∫n mayor en datasets m√°s complejos o con m√°s clases.\n",
        "\n",
        "---\n",
        "\n",
        "6. **¬øQu√© t√©cnicas adicionales (data augmentation, regularizaci√≥n, m√°s capas) podr√≠an mejorar todav√≠a m√°s la CNN?**\n",
        "\n",
        "   * **Data augmentation:** rotaciones, flips, cambios de color ‚Üí m√°s robustez.\n",
        "   * **Regularizaci√≥n:** dropout, weight decay para reducir sobreajuste.\n",
        "   * **Arquitectura m√°s profunda:** m√°s capas convolucionales y filtros.\n",
        "   * **Batch normalization:** estabiliza y acelera el entrenamiento.\n",
        "   * **Learning rate scheduling:** ajusta din√°micamente la tasa de aprendizaje para converger mejor.\n",
        "\n",
        "---\n",
        "\n",
        "7. **¬øQu√© limitaciones pueden encontrarse al entrenar CNNs desde cero con datasets relativamente peque√±os?**\n",
        "\n",
        "   * Riesgo de **sobreajuste**: la red aprende a memorizar las im√°genes en vez de generalizar.\n",
        "   * Dificultad para aprender patrones complejos sin muchos ejemplos.\n",
        "   * Mayor necesidad de regularizaci√≥n y augmentaci√≥n.\n",
        "   * Tiempo de entrenamiento m√°s alto en comparaci√≥n con MLP.\n",
        "   * En la pr√°ctica, cuando el dataset es peque√±o, se suele recurrir a **transfer learning**, usando un modelo preentrenado y adapt√°ndolo al problema.\n",
        "\n",
        "---\n"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "markdown",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}