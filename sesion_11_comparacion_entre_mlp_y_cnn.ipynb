{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rubuntu/Taller_Introduccion_a_Ciencia_de_Datos_IA_e_Ingenieria_de_Datos/blob/main/sesion_11_comparacion_entre_mlp_y_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29b97f0c",
      "metadata": {
        "id": "29b97f0c"
      },
      "source": [
        "# Clasificación de Perros y Gatos: Comparación entre MLP y CNN\n",
        "\n",
        "## Objetivos\n",
        "- Cargar y preparar el dataset público `microsoft/cats_vs_dogs` desde HuggingFace.\n",
        "- Implementar un pipeline de preprocesamiento con `torchvision.transforms` para imágenes.\n",
        "- Construir y entrenar dos arquitecturas distintas:\n",
        "  - Un **MLP** (perceptrón multicapa) usando imágenes aplanadas.\n",
        "  - Una **CNN** simple, con convoluciones y pooling.\n",
        "- Comparar el rendimiento de ambos modelos en términos de **accuracy**.\n",
        "- Reflexionar sobre las ventajas y desventajas de MLPs y CNNs en problemas de visión.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from datasets import load_dataset\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Usando dispositivo:\", device)"
      ],
      "metadata": {
        "id": "AWraGD4sZj8h"
      },
      "id": "AWraGD4sZj8h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e58ec881-2278-4a67-9318-8670f757d727",
      "metadata": {
        "id": "e58ec881-2278-4a67-9318-8670f757d727"
      },
      "source": [
        "## 1. Cargar dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "991f0b95",
      "metadata": {
        "id": "991f0b95"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"microsoft/cats_vs_dogs\")\n",
        "\n",
        "# Dividir en 80% train, 20% test\n",
        "dataset = dataset[\"train\"].train_test_split(test_size=0.2)\n",
        "\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af95ce04-f635-410b-a813-025b25bb12e5",
      "metadata": {
        "id": "af95ce04-f635-410b-a813-025b25bb12e5"
      },
      "source": [
        "## 2. Transformaciones\n",
        "- Para el **MLP**: reducimos resolución a 64x64 y aplanamos la imagen.\n",
        "- Para la **CNN**: usamos imágenes 128x128 en formato RGB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3cdfd39",
      "metadata": {
        "id": "d3cdfd39"
      },
      "outputs": [],
      "source": [
        "transform_mlp = transforms.Compose([\n",
        "    transforms.Resize((64,64)),\n",
        "    transforms.Lambda(lambda img: img.convert(\"RGB\")),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: x.view(-1))  # aplanar\n",
        "])\n",
        "\n",
        "transform_cnn = transforms.Compose([\n",
        "    transforms.Resize((128,128)),\n",
        "    transforms.Lambda(lambda img: img.convert(\"RGB\")),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a44ebe8b-cc62-478f-a6ce-c2f4d6790d92",
      "metadata": {
        "id": "a44ebe8b-cc62-478f-a6ce-c2f4d6790d92"
      },
      "source": [
        "## 3. Adaptador HuggingFace → PyTorch Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67d9b78e",
      "metadata": {
        "id": "67d9b78e"
      },
      "outputs": [],
      "source": [
        "class CatsDogsDataset(Dataset):\n",
        "    def __init__(self, hf_dataset, transform=None):\n",
        "        self.data = hf_dataset\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.data[idx][\"image\"]\n",
        "        label = self.data[idx][\"labels\"]  # 0 = cat, 1 = dog\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "train_ds_mlp = CatsDogsDataset(dataset[\"train\"], transform=transform_mlp)\n",
        "test_ds_mlp  = CatsDogsDataset(dataset[\"test\"], transform=transform_mlp)\n",
        "\n",
        "train_ds_cnn = CatsDogsDataset(dataset[\"train\"], transform=transform_cnn)\n",
        "test_ds_cnn  = CatsDogsDataset(dataset[\"test\"], transform=transform_cnn)\n",
        "\n",
        "train_dl_mlp = DataLoader(train_ds_mlp, batch_size=32, shuffle=True)\n",
        "test_dl_mlp  = DataLoader(test_ds_mlp, batch_size=32)\n",
        "\n",
        "train_dl_cnn = DataLoader(train_ds_cnn, batch_size=32, shuffle=True)\n",
        "test_dl_cnn  = DataLoader(test_ds_cnn, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb710aab-0afc-47fc-b973-3e20915a852f",
      "metadata": {
        "id": "cb710aab-0afc-47fc-b973-3e20915a852f"
      },
      "source": [
        "## 4. Definir Modelos\n",
        "### 4.1 MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c7eede3",
      "metadata": {
        "id": "0c7eede3"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim=64*64*3, hidden=256):\n",
        "        super(MLP, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 2)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "mlp_model = MLP().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d896131c-5328-4331-8e84-f062f4e2311c",
      "metadata": {
        "id": "d896131c-5328-4331-8e84-f062f4e2311c"
      },
      "source": [
        "### 4.2 CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afed9edd",
      "metadata": {
        "id": "afed9edd"
      },
      "outputs": [],
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),   # 64x64\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),   # 32x32\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),   # 16x16\n",
        "        )\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128*16*16, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 2)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n",
        "\n",
        "cnn_model = SimpleCNN().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b548818b-44d0-423e-9435-362ff0b5bb88",
      "metadata": {
        "id": "b548818b-44d0-423e-9435-362ff0b5bb88"
      },
      "source": [
        "## 5. Función de entrenamiento y evaluación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec99beea",
      "metadata": {
        "id": "ec99beea"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_dl, test_dl, n_epochs=5, lr=1e-3):\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    opt = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for xb, yb in train_dl:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            preds = model(xb)\n",
        "            loss = loss_fn(preds, yb)\n",
        "\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Evaluación\n",
        "        model.eval()\n",
        "        correct, total = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in test_dl:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                preds = model(xb).argmax(dim=1)\n",
        "                correct += (preds == yb).sum().item()\n",
        "                total += yb.size(0)\n",
        "\n",
        "        acc = correct / total\n",
        "        print(f\"Epoch {epoch+1}, Loss={running_loss/len(train_dl):.4f}, Val Acc={acc:.4f}\")\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9f250be-b483-49d1-91d1-7b774c44cc6b",
      "metadata": {
        "id": "f9f250be-b483-49d1-91d1-7b774c44cc6b"
      },
      "source": [
        "## 6. Entrenamiento de ambos modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2192a638",
      "metadata": {
        "id": "2192a638"
      },
      "outputs": [],
      "source": [
        "print(\"Entrenando MLP...\")\n",
        "#acc_mlp = train_model(mlp_model, train_dl_mlp, test_dl_mlp, n_epochs=5, lr=1e-3)\n",
        "acc_mlp = train_model(mlp_model, train_dl_mlp, test_dl_mlp, n_epochs=1, lr=1e-3)\n",
        "\n",
        "print(\"\\nEntrenando CNN...\")\n",
        "acc_cnn = train_model(cnn_model, train_dl_cnn, test_dl_cnn, n_epochs=1, lr=1e-3)\n",
        "\n",
        "print(\"\\nResultados finales:\")\n",
        "print(f\"Accuracy MLP: {acc_mlp:.4f}\")\n",
        "print(f\"Accuracy CNN: {acc_cnn:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d97ef61-6ca9-4536-a362-425446d83a87",
      "metadata": {
        "id": "8d97ef61-6ca9-4536-a362-425446d83a87"
      },
      "source": [
        "## 7. Visualización de algunas predicciones con la CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63c5e8c8",
      "metadata": {
        "id": "63c5e8c8"
      },
      "outputs": [],
      "source": [
        "images, labels = next(iter(test_dl_cnn))\n",
        "images, labels = images.to(device), labels.to(device)\n",
        "preds = cnn_model(images).argmax(dim=1)\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "for i in range(8):\n",
        "    plt.subplot(2,4,i+1)\n",
        "    img = images[i].cpu().permute(1,2,0).numpy()\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"Real: {labels[i].item()}, Pred: {preds[i].item()}\")\n",
        "    plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23ef44aa-e2de-48df-9b05-82c8a55b3698",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "23ef44aa-e2de-48df-9b05-82c8a55b3698"
      },
      "source": [
        "## Preguntas de Discusión\n",
        "\n",
        "1. ¿Qué diferencias arquitectónicas hay entre un MLP y una CNN?\n",
        "2. ¿Por qué el MLP necesita aplanar la imagen mientras que la CNN conserva la estructura espacial?\n",
        "3. ¿En qué tipo de problemas un MLP podría ser suficiente y en cuáles una CNN es claramente superior?\n",
        "4. ¿Qué rol cumplen las convoluciones y el pooling en la capacidad de generalización de las CNN?\n",
        "5. ¿Cómo esperas que varíe el accuracy entre el MLP y la CNN en este dataset?\n",
        "6. ¿Qué técnicas adicionales (data augmentation, regularización, más capas) podrían mejorar todavía más la CNN?\n",
        "7. ¿Qué limitaciones pueden encontrarse al entrenar CNNs desde cero con datasets relativamente pequeños?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "502311de",
      "metadata": {
        "id": "502311de"
      },
      "source": [
        "## 💡 Preguntas de Discusión (desarrolladas)\n",
        "\n",
        "1. **¿Qué diferencias arquitectónicas hay entre un MLP y una CNN?**\n",
        "\n",
        "   * Un **MLP (Multilayer Perceptron)** conecta cada píxel de la imagen con la siguiente capa de manera densa → todos los píxeles tienen igual importancia y no se aprovecha la estructura espacial.\n",
        "   * Una **CNN (Convolutional Neural Network)** utiliza capas convolucionales que procesan regiones locales de la imagen con filtros compartidos, detectando bordes, texturas y formas de manera jerárquica.\n",
        "   * Resultado: la CNN es mucho más eficiente para imágenes porque reutiliza filtros y respeta la estructura espacial, mientras que un MLP escala mal con la resolución.\n",
        "\n",
        "---\n",
        "\n",
        "2. **¿Por qué el MLP necesita aplanar la imagen mientras que la CNN conserva la estructura espacial?**\n",
        "\n",
        "   * El MLP espera vectores como entrada, no tensores 2D o 3D. Por eso, la imagen debe convertirse en un vector largo (aplanado), perdiendo información espacial.\n",
        "   * La CNN en cambio acepta tensores de 3 dimensiones (canales, alto, ancho), lo que le permite aprender directamente patrones espaciales sin necesidad de destruir esa estructura.\n",
        "\n",
        "---\n",
        "\n",
        "3. **¿En qué tipo de problemas un MLP podría ser suficiente y en cuáles una CNN es claramente superior?**\n",
        "\n",
        "   * **MLP suficiente:** problemas con entradas de baja dimensión (ej. tabulares, texto codificado como embeddings, imágenes muy pequeñas y simples).\n",
        "   * **CNN superior:** tareas de visión por computadora donde la estructura espacial es crítica (clasificación de imágenes, detección de objetos, segmentación).\n",
        "   * En datasets como perros vs gatos, la CNN es claramente superior porque los rasgos distintivos (orejas, hocico, bigotes) dependen de relaciones espaciales entre píxeles.\n",
        "\n",
        "---\n",
        "\n",
        "4. **¿Qué rol cumplen las convoluciones y el pooling en la capacidad de generalización de las CNN?**\n",
        "\n",
        "   * **Convoluciones:** permiten detectar patrones locales (bordes, esquinas, texturas) y reutilizar los mismos filtros en toda la imagen → eficiencia y capacidad de reconocer patrones independientemente de su ubicación.\n",
        "   * **Pooling (ej. max pooling):** reduce la resolución, mantiene la información más relevante y aporta *invarianza traslacional* (el objeto puede moverse un poco y aún se reconoce).\n",
        "   * Juntos, hacen que la CNN generalice mejor y no se limite a memorizar píxeles exactos.\n",
        "\n",
        "---\n",
        "\n",
        "5. **¿Cómo esperas que varíe el accuracy entre el MLP y la CNN en este dataset?**\n",
        "\n",
        "   * El **MLP** suele alcanzar un accuracy modesto (≈0.55–0.65) porque ignora relaciones espaciales y necesita más datos para generalizar.\n",
        "   * La **CNN** debería superar al MLP (≈0.70–0.80), incluso siendo pequeña, ya que aprovecha mejor la estructura visual.\n",
        "   * La diferencia se vuelve aún mayor en datasets más complejos o con más clases.\n",
        "\n",
        "---\n",
        "\n",
        "6. **¿Qué técnicas adicionales (data augmentation, regularización, más capas) podrían mejorar todavía más la CNN?**\n",
        "\n",
        "   * **Data augmentation:** rotaciones, flips, cambios de color → más robustez.\n",
        "   * **Regularización:** dropout, weight decay para reducir sobreajuste.\n",
        "   * **Arquitectura más profunda:** más capas convolucionales y filtros.\n",
        "   * **Batch normalization:** estabiliza y acelera el entrenamiento.\n",
        "   * **Learning rate scheduling:** ajusta dinámicamente la tasa de aprendizaje para converger mejor.\n",
        "\n",
        "---\n",
        "\n",
        "7. **¿Qué limitaciones pueden encontrarse al entrenar CNNs desde cero con datasets relativamente pequeños?**\n",
        "\n",
        "   * Riesgo de **sobreajuste**: la red aprende a memorizar las imágenes en vez de generalizar.\n",
        "   * Dificultad para aprender patrones complejos sin muchos ejemplos.\n",
        "   * Mayor necesidad de regularización y augmentación.\n",
        "   * Tiempo de entrenamiento más alto en comparación con MLP.\n",
        "   * En la práctica, cuando el dataset es pequeño, se suele recurrir a **transfer learning**, usando un modelo preentrenado y adaptándolo al problema.\n",
        "\n",
        "---\n"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "markdown",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}