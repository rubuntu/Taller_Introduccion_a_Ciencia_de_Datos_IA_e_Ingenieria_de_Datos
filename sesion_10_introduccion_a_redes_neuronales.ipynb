{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rubuntu/Taller_Introduccion_a_Ciencia_de_Datos_IA_e_Ingenieria_de_Datos/blob/main/sesion_10_introduccion_a_redes_neuronales.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "675896ac-c400-4aa9-bcaf-e53ae5679071",
      "metadata": {
        "id": "675896ac-c400-4aa9-bcaf-e53ae5679071"
      },
      "source": [
        "# üìò Sesi√≥n: Del Perceptr√≥n Cl√°sico al Multicapa (MLP) con scikit-learn, PyTorch y TensorFlow\n",
        "\n",
        "## Objetivos\n",
        "\n",
        "* Comprender el **Perceptr√≥n cl√°sico** implementado en NumPy.\n",
        "* Visualizar la **frontera de decisi√≥n** en problemas l√≥gicos simples (AND, OR, XOR).\n",
        "* Conocer las **funciones de activaci√≥n** de los MLPs.\n",
        "* Comparar modelos: **Perceptr√≥n simple vs MLP vs Regresi√≥n Log√≠stica**.\n",
        "* Resolver un caso real (Churn) con un MLP.\n",
        "* Implementar **MLP en scikit-learn, PyTorch y TensorFlow/Keras**.\n",
        "* Visualizar diferencias con un **diagrama comparativo**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "537f91da-1008-4bb7-bd32-c6874c3fbf93",
      "metadata": {
        "id": "537f91da-1008-4bb7-bd32-c6874c3fbf93"
      },
      "source": [
        "---\n",
        "\n",
        "## 1. Perceptr√≥n\n",
        "\n",
        "- Referencia: https://es.wikipedia.org/wiki/Perceptr%C3%B3n\n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b0/Perceptr%C3%B3n_5_unidades.svg/960px-Perceptr%C3%B3n_5_unidades.svg.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c06fd1f",
      "metadata": {
        "id": "1c06fd1f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, n_inputs, lr=0.1, n_epochs=10):\n",
        "        self.lr = lr\n",
        "        self.n_epochs = n_epochs\n",
        "        self.w = np.zeros(n_inputs)\n",
        "        self.b = 0.0\n",
        "\n",
        "    def activation(self, z):\n",
        "        return np.where(z >= 0, 1, 0)\n",
        "\n",
        "    def predict(self, X):\n",
        "        z = np.dot(X, self.w) + self.b\n",
        "        return self.activation(z)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for epoch in range(self.n_epochs):\n",
        "            errors = 0\n",
        "            for xi, target in zip(X, y):\n",
        "                y_hat = self.predict(xi)\n",
        "                update = self.lr * (target - y_hat)\n",
        "                self.w += update * xi\n",
        "                self.b += update\n",
        "                errors += int(update != 0.0)\n",
        "            print(f\"Epoch {epoch+1} - Errores: {errors}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El c√≥digo implementa un **Perceptr√≥n**, uno de los modelos m√°s simples de **red neuronal** para la clasificaci√≥n. Aunque es una versi√≥n b√°sica, es fundamental para entender c√≥mo funcionan los modelos m√°s complejos de *deep learning*.\n",
        "\n",
        "---\n",
        "\n",
        "### **Componentes del Perceptr√≥n**\n",
        "\n",
        "#### **a. Constructor (`__init__`)**\n",
        "Esta funci√≥n se ejecuta al crear una nueva instancia de la clase `Perceptron`.\n",
        "* `n_inputs`: Es el n√∫mero de caracter√≠sticas o *features* en tus datos de entrada. El modelo crea un vector de pesos (`self.w`) del mismo tama√±o, inicializ√°ndolo en cero.\n",
        "* `lr`: Significa **tasa de aprendizaje** (*learning rate*). Es un hiperpar√°metro crucial que determina qu√© tan grande ser√° el ajuste en los pesos del modelo con cada error. Un valor peque√±o significa que los cambios son graduales, mientras que un valor grande puede causar que el modelo \"salte\" la soluci√≥n √≥ptima.\n",
        "* `n_epochs`: Es el n√∫mero de veces que el modelo procesar√° todo el conjunto de datos de entrenamiento. Cada *√©poca* es un ciclo completo de aprendizaje.\n",
        "* `self.w`: El **vector de pesos** que el modelo aprende. Cada peso se asocia a una caracter√≠stica de entrada y representa su importancia para la predicci√≥n.\n",
        "* `self.b`: El **sesgo** (*bias*). Es un valor que se a√±ade al resultado, permitiendo que la l√≠nea de decisi√≥n se desplace, lo que ayuda a la red a clasificar datos que no son linealmente separables a trav√©s del origen.\n",
        "\n",
        "#### **b. Funci√≥n de Activaci√≥n (`activation`)**\n",
        "* Esta funci√≥n recibe la suma ponderada de las entradas m√°s el sesgo (`z`). En el caso del Perceptr√≥n, se usa una funci√≥n de activaci√≥n simple.\n",
        "* `np.where(z >= 0, 1, 0)`: Esta es una **funci√≥n escal√≥n** (*step function*). Si el resultado de `z` es mayor o igual a cero, la neurona \"se activa\" y el resultado es 1. De lo contrario, no se activa y el resultado es 0. Esto permite al modelo hacer una clasificaci√≥n binaria.\n",
        "\n",
        "#### **c. Predicci√≥n (`predict`)**\n",
        "* Esta funci√≥n toma un conjunto de datos de entrada (`X`) y calcula la predicci√≥n.\n",
        "* `z = np.dot(X, self.w) + self.b`: Aqu√≠ se realiza el c√°lculo central del Perceptr√≥n. Se toma el **producto punto** de los datos de entrada (`X`) y el vector de pesos (`self.w`), y se le suma el sesgo (`self.b`). Este c√°lculo representa la entrada neta a la neurona.\n",
        "* Luego, este valor `z` se pasa a la funci√≥n de activaci√≥n, que devuelve la predicci√≥n final (0 o 1).\n",
        "\n",
        "#### **d. Entrenamiento (`fit`)**\n",
        "Este es el coraz√≥n del algoritmo, donde el modelo aprende.\n",
        "* El bucle `for epoch in range(self.n_epochs):` permite que el modelo se entrene varias veces sobre el mismo conjunto de datos.\n",
        "* El bucle `for xi, target in zip(X, y):` itera sobre cada ejemplo de entrenamiento (`xi`) y su etiqueta correcta (`target`).\n",
        "* `y_hat = self.predict(xi)`: El modelo hace una predicci√≥n con los pesos y el sesgo actuales.\n",
        "* `update = self.lr * (target - y_hat)`: Aqu√≠ se calcula el **ajuste** necesario. El error es la diferencia entre la etiqueta real (`target`) y la predicci√≥n (`y_hat`).\n",
        "    * Si la predicci√≥n es correcta, `(target - y_hat)` es 0, y no hay actualizaci√≥n.\n",
        "    * Si la predicci√≥n es incorrecta, `(target - y_hat)` es 1 o -1, y los pesos se ajustan en la direcci√≥n correcta, multiplicados por la tasa de aprendizaje.\n",
        "* `self.w += update * xi` y `self.b += update`: Los pesos y el sesgo se **actualizan** en base al error. Este es el paso de aprendizaje.\n",
        "* `errors += int(update != 0.0)`: Se cuenta el n√∫mero de errores en cada √©poca para monitorear el progreso del aprendizaje. El objetivo del entrenamiento es que el n√∫mero de errores se reduzca a cero.\n",
        "\n",
        "El Perceptr√≥n aprende a clasificar datos de forma **lineal** ajustando sus pesos y sesgo cada vez que comete un error, hasta que todas las clasificaciones sean correctas o se complete el n√∫mero de √©pocas."
      ],
      "metadata": {
        "id": "7oXeb9Dd9G1n"
      },
      "id": "7oXeb9Dd9G1n"
    },
    {
      "cell_type": "markdown",
      "id": "cccf8518-86e4-49ee-bd70-236ebc68fe93",
      "metadata": {
        "id": "cccf8518-86e4-49ee-bd70-236ebc68fe93"
      },
      "source": [
        "### **Gr√°fico üìä de la funci√≥n de activaci√≥n del Perceptr√≥n cl√°sico**:\n",
        "\n",
        "* Es una **funci√≥n escal√≥n (step function)**.\n",
        "* Devuelve **0 si $x<0$** y **1 si $x \\geq 0$**.\n",
        "* Representa la decisi√≥n binaria m√°s simple: ‚Äúdispara o no dispara‚Äù.\n",
        "\n",
        "üëâ A diferencia de funciones suaves como sigmoide o ReLU, esta funci√≥n no tiene derivada √∫til, lo que limita al perceptr√≥n simple en problemas m√°s complejos.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17c43264-34b5-49d7-9324-1b79a979c1a8",
      "metadata": {
        "id": "17c43264-34b5-49d7-9324-1b79a979c1a8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Rango de entrada\n",
        "x = np.linspace(-3, 3, 400)\n",
        "\n",
        "# Funci√≥n escal√≥n (step), activaci√≥n del Perceptr√≥n cl√°sico\n",
        "step = np.where(x >= 0, 1, 0)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(x, step, drawstyle=\"steps-post\", color=\"purple\", linewidth=2, label=\"Step function\")\n",
        "plt.axhline(0, color=\"gray\", linestyle=\"--\")\n",
        "plt.axhline(1, color=\"gray\", linestyle=\"--\")\n",
        "plt.axvline(0, color=\"gray\", linestyle=\"--\")\n",
        "plt.title(\"Funci√≥n de Activaci√≥n del Perceptr√≥n (Step)\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"f(x)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd19d1a1",
      "metadata": {
        "id": "cd19d1a1"
      },
      "source": [
        "---\n",
        "\n",
        "## 2. Fronteras de decisi√≥n: AND y OR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07b4d5bb",
      "metadata": {
        "id": "07b4d5bb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_decision_boundary(X, y, model, title):\n",
        "    xx, yy = np.meshgrid(np.linspace(-0.5,1.5,200),\n",
        "                         np.linspace(-0.5,1.5,200))\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.Paired)\n",
        "    plt.scatter(X[:,0], X[:,1], c=y, edgecolor=\"k\", cmap=plt.cm.Paired, s=100)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "# AND\n",
        "X_and = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y_and = np.array([0,0,0,1])\n",
        "p_and = Perceptron(2, lr=0.1, n_epochs=10); p_and.fit(X_and, y_and)\n",
        "plot_decision_boundary(X_and, y_and, p_and, \"Perceptr√≥n - AND\")\n",
        "\n",
        "# OR\n",
        "X_or = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y_or = np.array([0,1,1,1])\n",
        "p_or = Perceptron(2, lr=0.1, n_epochs=10); p_or.fit(X_or, y_or)\n",
        "plot_decision_boundary(X_or, y_or, p_or, \"Perceptr√≥n - OR\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31ec03f4",
      "metadata": {
        "id": "31ec03f4"
      },
      "source": [
        "---\n",
        "\n",
        "## 3. Caso XOR (no separable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2028223c",
      "metadata": {
        "id": "2028223c"
      },
      "outputs": [],
      "source": [
        "X_xor = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y_xor = np.array([0,1,1,0])\n",
        "\n",
        "p_xor = Perceptron(2, lr=0.1, n_epochs=20)\n",
        "p_xor.fit(X_xor, y_xor)\n",
        "plot_decision_boundary(X_xor, y_xor, p_xor, \"Perceptr√≥n - XOR (fracasa)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4560671-e0a1-46eb-866f-fcf13a214d6e",
      "metadata": {
        "id": "b4560671-e0a1-46eb-866f-fcf13a214d6e"
      },
      "source": [
        "üìå El perceptr√≥n no logra resolver XOR porque el problema no es linealmente separable.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95470e96-261b-43be-8aab-6a00812d64fe",
      "metadata": {
        "id": "95470e96-261b-43be-8aab-6a00812d64fe"
      },
      "source": [
        "## 4. Perceptr√≥n multicapa - Multilayer perceptron (MLP)\n",
        "\n",
        "- Referencia: https://es.wikipedia.org/wiki/Perceptr%C3%B3n_multicapa\n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/6/64/RedNeuronalArtificial.png?20160319151219)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79606d25-f97a-4992-95f8-0c9ab971b5e4",
      "metadata": {
        "id": "79606d25-f97a-4992-95f8-0c9ab971b5e4"
      },
      "source": [
        "## 5. Funciones de activaci√≥n en MLP\n",
        "\n",
        "* **identity**: $f(x)=x$\n",
        "* **logistic**: sigmoide\n",
        "* **tanh**: tangente hiperb√≥lica\n",
        "* **relu**: rectificada\n",
        "\n",
        "Estas funciones permiten aprender **fronteras no lineales**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e265c9f-2e04-4b0c-adeb-47fa807d30d7",
      "metadata": {
        "id": "3e265c9f-2e04-4b0c-adeb-47fa807d30d7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Rango de entrada\n",
        "x = np.linspace(-5, 5, 400)\n",
        "\n",
        "# Funciones de activaci√≥n\n",
        "identity = x\n",
        "logistic = 1 / (1 + np.exp(-x))\n",
        "tanh = np.tanh(x)\n",
        "relu = np.maximum(0, x)\n",
        "\n",
        "# Graficar\n",
        "fig, axs = plt.subplots(2, 2, figsize=(10,8))\n",
        "\n",
        "axs[0,0].plot(x, identity, label=\"identity\")\n",
        "axs[0,0].set_title(\"Identity: f(x)=x\")\n",
        "axs[0,0].axhline(0, color=\"gray\", linestyle=\"--\")\n",
        "axs[0,0].axvline(0, color=\"gray\", linestyle=\"--\")\n",
        "\n",
        "axs[0,1].plot(x, logistic, label=\"sigmoid\", color=\"orange\")\n",
        "axs[0,1].set_title(\"Logistic (sigmoide)\")\n",
        "axs[0,1].axhline(0.5, color=\"gray\", linestyle=\"--\")\n",
        "\n",
        "axs[1,0].plot(x, tanh, label=\"tanh\", color=\"green\")\n",
        "axs[1,0].set_title(\"Tanh\")\n",
        "axs[1,0].axhline(0, color=\"gray\", linestyle=\"--\")\n",
        "\n",
        "axs[1,1].plot(x, relu, label=\"ReLU\", color=\"red\")\n",
        "axs[1,1].set_title(\"ReLU\")\n",
        "axs[1,1].axhline(0, color=\"gray\", linestyle=\"--\")\n",
        "axs[1,1].axvline(0, color=\"gray\", linestyle=\"--\")\n",
        "\n",
        "for ax in axs.flat:\n",
        "    ax.legend()\n",
        "    ax.grid(True)\n",
        "\n",
        "plt.suptitle(\"Funciones de Activaci√≥n en MLP\", fontsize=14, weight=\"bold\")\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f420f67-dda7-46a5-afc4-e24a7eba82c5",
      "metadata": {
        "id": "9f420f67-dda7-46a5-afc4-e24a7eba82c5"
      },
      "source": [
        "---\n",
        "\n",
        "## üîπ Ejemplo con dataset MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d303518-30c7-4c97-a5c4-22e8cf0adc67",
      "metadata": {
        "id": "4d303518-30c7-4c97-a5c4-22e8cf0adc67"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Descargar MNIST (70k im√°genes 28x28)\n",
        "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
        "X, y = mnist[\"data\"], mnist[\"target\"].astype(int)\n",
        "\n",
        "# Mostrar una imagen\n",
        "idx = 0\n",
        "plt.imshow(X[idx].reshape(28,28), cmap=\"gray\")\n",
        "plt.title(f\"Etiqueta: {y[idx]}\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8e769d4-f462-49c3-86fa-f7f449783483",
      "metadata": {
        "id": "b8e769d4-f462-49c3-86fa-f7f449783483"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Descargar MNIST (70k im√°genes 28x28)\n",
        "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
        "X, y = mnist[\"data\"], mnist[\"target\"].astype(int)\n",
        "\n",
        "\n",
        "# Normalizar los datos\n",
        "# Escalar los valores de p√≠xeles al rango [0, 1] para un mejor rendimiento del modelo\n",
        "X = X / 255.0\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "# Usamos el 80% para entrenar y el 20% para probar\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Inicializar y entrenar el clasificador MLP\n",
        "# 'hidden_layer_sizes' define la estructura de la red, en este caso 100 neuronas\n",
        "# 'max_iter' es el n√∫mero m√°ximo de √©pocas de entrenamiento\n",
        "print(\"Entrenando el clasificador MLP...\")\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=20, random_state=42)\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "# Hacer predicciones en el conjunto de prueba\n",
        "y_pred = mlp.predict(X_test)\n",
        "\n",
        "# Evaluar la precisi√≥n del modelo\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Precisi√≥n del modelo MLP: {accuracy:.4f}\")\n",
        "\n",
        "# Mostrar una imagen y la predicci√≥n del modelo\n",
        "idx_test = 0  # Puedes cambiar este √≠ndice para ver otras im√°genes\n",
        "plt.imshow(X_test[idx_test].reshape(28,28), cmap=\"gray\")\n",
        "plt.title(f\"Etiqueta Real: {y_test[idx_test]}, Predicci√≥n del modelo: {y_pred[idx_test]}\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e19f14b-5d4e-4cec-a24e-572738fe33a3",
      "metadata": {
        "id": "3e19f14b-5d4e-4cec-a24e-572738fe33a3"
      },
      "source": [
        "\n",
        "-----\n",
        "\n",
        "## üëâ **Explicaci√≥n**:\n",
        "\n",
        "### Datos\n",
        "\n",
        "* `fetch_openml`: descarga el dataset **MNIST** desde OpenML (70,000 im√°genes de d√≠gitos escritos a mano, cada imagen tiene 28x28 = 784 p√≠xeles).\n",
        "* `X`: contiene las im√°genes en formato **vector de 784 valores**.\n",
        "* `y`: contiene las etiquetas (0‚Äì9).\n",
        "\n",
        "### Preprocesamiento de Datos\n",
        "\n",
        "Antes de alimentar los datos a la red neuronal, es crucial **preprocesarlos** adecuadamente.\n",
        "\n",
        "  * **Normalizaci√≥n:** Se dividen los valores de los p√≠xeles (que van de 0 a 255) por 255. Esto escala todos los valores al rango `[0, 1]`. Esta normalizaci√≥n es esencial para que los algoritmos basados en gradiente, como el que usa el MLP, converjan m√°s r√°pido y de manera m√°s estable.\n",
        "\n",
        "### Divisi√≥n de Datos\n",
        "\n",
        "Para evaluar el rendimiento del modelo de forma realista, se dividen los datos en dos conjuntos:\n",
        "\n",
        "  * **Entrenamiento (`X_train`, `y_train`):** El modelo aprende de estas im√°genes y sus etiquetas.\n",
        "  * **Prueba (`X_test`, `y_test`):** El modelo se prueba con estas im√°genes, que nunca ha visto antes, para evaluar su capacidad de generalizaci√≥n. El par√°metro `test_size=0.2` indica que el 20% de los datos se utilizar√° para la prueba.\n",
        "\n",
        "### Clasificador MLP de scikit-learn\n",
        "\n",
        "  * **`MLPClassifier`:** Es la clase que implementa el **Perceptron Multicapa**. Es una red neuronal de tipo *feed-forward*.\n",
        "  * **`hidden_layer_sizes=(100,)`:** Define la arquitectura de la red. En este caso, se crea una capa oculta con 100 neuronas. Puedes experimentar con diferentes tama√±os o a√±adir m√°s capas (ej: `(100, 50, 20)`).\n",
        "  * **`max_iter=20`:** Establece el n√∫mero m√°ximo de √©pocas (iteraciones completas sobre los datos de entrenamiento) que el modelo realizar√°. Aumentar este valor puede mejorar el rendimiento, pero tambi√©n el tiempo de entrenamiento.\n",
        "  * **`mlp.fit(X_train, y_train)`:** Este es el paso de entrenamiento. El modelo ajusta sus pesos internos para aprender a mapear las im√°genes (`X_train`) a sus etiquetas correctas (`y_train`).\n",
        "  * **`mlp.predict(X_test)`:** Una vez entrenado, el modelo predice las etiquetas para las im√°genes en el conjunto de prueba.\n",
        "\n",
        "### Evaluaci√≥n del Modelo\n",
        "\n",
        "  * **`accuracy_score`:** Se utiliza para medir la **precisi√≥n** del modelo. Compara las predicciones (`y_pred`) con las etiquetas reales (`y_test`) y devuelve la proporci√≥n de predicciones correctas. La precisi√≥n es una m√©trica com√∫n para evaluar modelos de clasificaci√≥n.\n",
        "\n",
        "### Visualizaci√≥n\n",
        "  * **`plt.imshow`:** reacomoda una fila de 784 valores en una matriz 28x28 para visualizar como imagen en escala de grises."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c62bc4ae-d4aa-449f-9c7f-4d40fc3bd19b",
      "metadata": {
        "id": "c62bc4ae-d4aa-449f-9c7f-4d40fc3bd19b"
      },
      "source": [
        "---\n",
        "\n",
        "## 6. Caso real: Telco Churn (MLP con 1 capa oculta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ebca816",
      "metadata": {
        "id": "0ebca816"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import roc_auc_score, RocCurveDisplay\n",
        "\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
        "df = pd.read_excel(url, header=1)\n",
        "df.rename(columns={\"default payment next month\": \"default\"}, inplace=True)\n",
        "\n",
        "y = df[\"default\"]\n",
        "X = pd.get_dummies(df.drop(columns=[\"ID\",\"default\"]), drop_first=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, stratify=y, random_state=42)\n",
        "scaler = StandardScaler(with_mean=False)\n",
        "X_train = scaler.fit_transform(X_train); X_test = scaler.transform(X_test)\n",
        "\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(64,), max_iter=1000, random_state=42)\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "RocCurveDisplay.from_estimator(mlp, X_test, y_test)\n",
        "print(\"AUC:\", roc_auc_score(y_test, mlp.predict_proba(X_test)[:,1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55b0b558-9214-4a20-a6cd-8b6bf27b3586",
      "metadata": {
        "id": "55b0b558-9214-4a20-a6cd-8b6bf27b3586"
      },
      "source": [
        "## Explicaci√≥n\n",
        "\n",
        "---\n",
        "\n",
        "### üì• 6.1. Importar librer√≠as\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import roc_auc_score, RocCurveDisplay\n",
        "```\n",
        "\n",
        "* **pandas** ‚Üí manejo de datos.\n",
        "* **train\\_test\\_split** ‚Üí divide dataset en entrenamiento y test.\n",
        "* **StandardScaler** ‚Üí normaliza variables num√©ricas (importante para MLP).\n",
        "* **MLPClassifier** ‚Üí red neuronal de scikit-learn.\n",
        "* **roc\\_auc\\_score, RocCurveDisplay** ‚Üí m√©tricas y visualizaci√≥n ROC.\n",
        "\n",
        "---\n",
        "\n",
        "### üìä 6.2. Cargar dataset UCI\n",
        "\n",
        "```python\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
        "df = pd.read_excel(url, header=1)\n",
        "df.rename(columns={\"default payment next month\": \"default\"}, inplace=True)\n",
        "```\n",
        "\n",
        "* Se descarga el **dataset Default of Credit Card Clients** en formato Excel.\n",
        "* `header=1`: omite la primera fila (tiene descripci√≥n).\n",
        "* Se renombra la columna objetivo `\"default payment next month\"` a `\"default\"` para mayor claridad.\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ 6.3. Variables independientes y dependiente\n",
        "\n",
        "```python\n",
        "y = df[\"default\"]\n",
        "X = pd.get_dummies(df.drop(columns=[\"ID\",\"default\"]), drop_first=True)\n",
        "```\n",
        "\n",
        "* **y**: columna objetivo ‚Üí `default` (1 = cliente incumple, 0 = no incumple).\n",
        "* **X**: resto de variables (sociodemogr√°ficas, historial de pagos, etc.).\n",
        "\n",
        "  * Se eliminan `\"ID\"` (irrelevante) y `\"default\"` (ya es target).\n",
        "  * `get_dummies`: convierte variables categ√≥ricas en variables dummy (one-hot encoding).\n",
        "  * `drop_first=True`: evita multicolinealidad (elimina una categor√≠a redundante).\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÇÔ∏è 6.4. Divisi√≥n train/test\n",
        "\n",
        "```python\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, stratify=y, random_state=42\n",
        ")\n",
        "```\n",
        "\n",
        "* Divide en **70% entrenamiento / 30% test**.\n",
        "* `stratify=y`: mantiene la misma proporci√≥n de casos de default en ambos conjuntos.\n",
        "* `random_state=42`: asegura reproducibilidad.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öñÔ∏è 6.5. Escalado de variables\n",
        "\n",
        "```python\n",
        "scaler = StandardScaler(with_mean=False)\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "```\n",
        "\n",
        "* El **MLP es sensible a la escala de las variables** ‚Üí normalizamos para que todas tengan media 0 y desviaci√≥n 1.\n",
        "* `with_mean=False`: se usa porque con dummies (sparse matrix) no conviene centrar en 0.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† 6.6. Definir y entrenar MLP\n",
        "\n",
        "```python\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(64,), max_iter=1000, random_state=42)\n",
        "mlp.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "* `MLPClassifier`: crea un perceptr√≥n multicapa.\n",
        "* `hidden_layer_sizes=(64,)`: 1 capa oculta con 64 neuronas.\n",
        "* `max_iter=1000`: hasta 1000 iteraciones para converger.\n",
        "* `.fit`: entrena la red en los datos de entrenamiento.\n",
        "\n",
        "---\n",
        "\n",
        "### üìà 6.7. Evaluaci√≥n con curva ROC y AUC\n",
        "\n",
        "```python\n",
        "RocCurveDisplay.from_estimator(mlp, X_test, y_test)\n",
        "print(\"AUC:\", roc_auc_score(y_test, mlp.predict_proba(X_test)[:,1]))\n",
        "```\n",
        "\n",
        "* `.predict_proba(X_test)[:,1]`: obtiene la **probabilidad predicha de clase 1 (default)**.\n",
        "* `roc_auc_score`: calcula el **√Årea Bajo la Curva ROC (AUC)**.\n",
        "\n",
        "  * AUC = 0.5 ‚Üí azar.\n",
        "  * AUC cerca de 1 ‚Üí excelente modelo.\n",
        "* `RocCurveDisplay`: dibuja la **curva ROC** (trade-off entre TPR y FPR).\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Resumen del flujo\n",
        "\n",
        "1. Descargamos dataset de defaults.\n",
        "2. Preparamos variables (dummy encoding + escalado).\n",
        "3. Dividimos en train/test.\n",
        "4. Entrenamos un **MLP con 64 neuronas ocultas**.\n",
        "5. Evaluamos con **ROC y AUC** ‚Üí m√©trica est√°ndar en problemas de scoring crediticio.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d768db05",
      "metadata": {
        "id": "d768db05"
      },
      "source": [
        "---\n",
        "\n",
        "## 7. Comparaci√≥n: Perceptr√≥n vs MLP vs Regresi√≥n Log√≠stica"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08b9e782",
      "metadata": {
        "id": "08b9e782"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression, Perceptron as SkPerceptron\n",
        "\n",
        "perc = SkPerceptron(max_iter=1000, random_state=42).fit(X_train, y_train)\n",
        "logreg = LogisticRegression(max_iter=1000).fit(X_train, y_train)\n",
        "\n",
        "print(\"Acc (Perceptr√≥n):\", perc.score(X_test,y_test))\n",
        "print(\"AUC (LogReg):\", roc_auc_score(y_test, logreg.predict_proba(X_test)[:,1]))\n",
        "print(\"AUC (MLP):\", roc_auc_score(y_test, mlp.predict_proba(X_test)[:,1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5006fd1a",
      "metadata": {
        "id": "5006fd1a"
      },
      "source": [
        "---\n",
        "\n",
        "## 8. Resolver XOR con MLP en 3 librer√≠as\n",
        "\n",
        "### üîπ scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f321ab0",
      "metadata": {
        "id": "0f321ab0"
      },
      "outputs": [],
      "source": [
        "# ========== Scikit-learn ==========\n",
        "import numpy as np\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Dataset XOR\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([0,1,1,0])\n",
        "\n",
        "mlp_sklearn = MLPClassifier(hidden_layer_sizes=(4,),\n",
        "                            activation=\"tanh\",\n",
        "                            max_iter=3000,\n",
        "                            random_state=42)\n",
        "mlp_sklearn.fit(X,y)\n",
        "print(\"Predicciones XOR (sklearn):\", mlp_sklearn.predict(X))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f823acea-34e7-40d8-b22e-7c79028794d1",
      "metadata": {
        "id": "f823acea-34e7-40d8-b22e-7c79028794d1"
      },
      "source": [
        "### üìä a. Dataset XOR\n",
        "\n",
        "```python\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([0,1,1,0])\n",
        "```\n",
        "\n",
        "* `X`: son las **entradas** (todas las combinaciones posibles de dos bits).\n",
        "\n",
        "  * (0,0)\n",
        "  * (0,1)\n",
        "  * (1,0)\n",
        "  * (1,1)\n",
        "\n",
        "* `y`: es la **salida esperada (XOR l√≥gico)**:\n",
        "\n",
        "  * 0 XOR 0 ‚Üí 0\n",
        "  * 0 XOR 1 ‚Üí 1\n",
        "  * 1 XOR 0 ‚Üí 1\n",
        "  * 1 XOR 1 ‚Üí 0\n",
        "\n",
        "üìå Este dataset **no es linealmente separable**, por lo que un perceptr√≥n simple (funci√≥n escal√≥n + hiperplano) no puede resolverlo.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† b. Definir MLP\n",
        "\n",
        "```python\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(4,), activation=\"tanh\", max_iter=3000, random_state=42)\n",
        "```\n",
        "\n",
        "* `MLPClassifier`: perceptr√≥n multicapa de scikit-learn.\n",
        "* `hidden_layer_sizes=(4,)`:\n",
        "\n",
        "  * Significa **una capa oculta con 4 neuronas**.\n",
        "  * Esto da al modelo capacidad de representar **fronteras no lineales**.\n",
        "* `activation=\"tanh\"`:\n",
        "\n",
        "  * Usamos **tangente hiperb√≥lica** como activaci√≥n.\n",
        "  * Es no lineal y centrada en 0, muy √∫til para este tipo de problemas.\n",
        "* `max_iter=3000`:\n",
        "\n",
        "  * Permitimos hasta 3000 iteraciones para asegurar que el modelo converge.\n",
        "* `random_state=42`:\n",
        "\n",
        "  * Semilla fija para reproducibilidad.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö° c. Entrenar el modelo\n",
        "\n",
        "```python\n",
        "mlp.fit(X,y)\n",
        "```\n",
        "\n",
        "* El modelo recibe las entradas `X` y etiquetas `y`.\n",
        "* Internamente realiza:\n",
        "\n",
        "  1. Forward pass (propaga datos por la red).\n",
        "  2. Calcula error con **entrop√≠a cruzada**.\n",
        "  3. Backpropagation (ajusta pesos con gradiente descendente).\n",
        "  4. Itera hasta minimizar error o llegar a `max_iter`.\n",
        "\n",
        "---\n",
        "\n",
        "### üìà d. Predicciones\n",
        "\n",
        "```python\n",
        "print(\"Predicciones XOR (sklearn):\", mlp.predict(X))\n",
        "```\n",
        "\n",
        "* Se eval√∫a el MLP en las 4 combinaciones de entrada.\n",
        "* La salida esperada es `[0,1,1,0]`.\n",
        "* Si la red aprendi√≥ correctamente, imprime algo como:\n",
        "\n",
        "```\n",
        "Predicciones XOR (sklearn): [0 1 1 0]\n",
        "```\n",
        "\n",
        "‚úÖ Lo importante: el **MLP logra resolver XOR**, demostrando que al a√±adir **capa oculta + activaci√≥n no lineal**, el modelo supera las limitaciones del perceptr√≥n simple.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb0754fb",
      "metadata": {
        "id": "eb0754fb"
      },
      "source": [
        "### üîπ PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3d4721b",
      "metadata": {
        "id": "a3d4721b"
      },
      "outputs": [],
      "source": [
        "# ========== PyTorch ==========\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "\n",
        "# Dataset en tensores\n",
        "X_t = torch.tensor(X, dtype=torch.float32)\n",
        "y_t = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "# Definici√≥n del modelo\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(2,4),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(4,1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "loss_fn = nn.BCELoss()\n",
        "opt = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# Entrenamiento\n",
        "for epoch in range(2000):\n",
        "    opt.zero_grad()\n",
        "    y_pred = model(X_t)\n",
        "    loss = loss_fn(y_pred, y_t)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "print(\"Predicciones XOR (PyTorch):\", model(X_t).detach().round().view(-1).numpy())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec294b81-7f14-443b-b994-a3b0473815e2",
      "metadata": {
        "id": "ec294b81-7f14-443b-b994-a3b0473815e2"
      },
      "source": [
        "Vamos a **explicar este bloque PyTorch paso a paso**. El objetivo es mostrar c√≥mo un **MLP simple** con una capa oculta aprende a resolver el problema **XOR**, que el perceptr√≥n cl√°sico no puede.\n",
        "\n",
        "---\n",
        "\n",
        "## üì• a. Importaciones\n",
        "\n",
        "```python\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "```\n",
        "\n",
        "* **torch** ‚Üí librer√≠a principal de tensores.\n",
        "* **nn** ‚Üí m√≥dulo de redes neuronales (`Linear`, `Tanh`, `Sigmoid`, etc.).\n",
        "* **optim** ‚Üí optimizadores (`SGD`, Adam, etc.).\n",
        "\n",
        "---\n",
        "\n",
        "## üìä b. Dataset en tensores\n",
        "\n",
        "```python\n",
        "X_t = torch.tensor(X, dtype=torch.float32)\n",
        "y_t = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
        "```\n",
        "\n",
        "* `X_t`: entradas (matriz de 4 ejemplos √ó 2 features).\n",
        "* `y_t`: etiquetas, convertidas en vector columna con `.unsqueeze(1)` para que tenga forma `(4,1)`.\n",
        "\n",
        "  * Necesario porque la red devuelve una salida de 1 neurona.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† c. Definici√≥n del modelo\n",
        "\n",
        "```python\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(2,4),   # capa lineal: 2 -> 4\n",
        "    nn.Tanh(),        # activaci√≥n no lineal\n",
        "    nn.Linear(4,1),   # capa lineal: 4 -> 1\n",
        "    nn.Sigmoid()      # salida probabil√≠stica (0 a 1)\n",
        ")\n",
        "```\n",
        "\n",
        "* Es un **MLP con arquitectura 2 ‚Üí 4 ‚Üí 1**.\n",
        "* `nn.Sequential`: permite encadenar capas.\n",
        "* `nn.Linear(2,4)`: transforma los 2 inputs en 4 neuronas ocultas.\n",
        "* `nn.Tanh()`: introduce no linealidad ‚Üí clave para aprender XOR.\n",
        "* `nn.Linear(4,1)`: capa de salida (1 neurona).\n",
        "* `nn.Sigmoid()`: convierte el valor a probabilidad en $[0,1]$.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öñÔ∏è d. Funci√≥n de p√©rdida y optimizador\n",
        "\n",
        "```python\n",
        "loss_fn = nn.BCELoss()\n",
        "opt = optim.SGD(model.parameters(), lr=0.1)\n",
        "```\n",
        "\n",
        "* `nn.BCELoss()`: **Binary Cross-Entropy**, mide diferencia entre probabilidades predichas y etiquetas (0/1).\n",
        "* `optim.SGD`: descenso de gradiente estoc√°stico.\n",
        "* `model.parameters()`: lista de pesos y bias que ser√°n ajustados.\n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ e. Loop de entrenamiento\n",
        "\n",
        "```python\n",
        "for epoch in range(2000):\n",
        "    opt.zero_grad()             # 1. Resetear gradientes previos\n",
        "    y_pred = model(X_t)         # 2. Forward pass\n",
        "    loss = loss_fn(y_pred, y_t) # 3. Calcular p√©rdida\n",
        "    loss.backward()             # 4. Backpropagation (gradientes)\n",
        "    opt.step()                  # 5. Actualizar pesos\n",
        "```\n",
        "\n",
        "* Este loop se repite **2000 √©pocas**.\n",
        "* En cada iteraci√≥n:\n",
        "\n",
        "  1. Limpia gradientes acumulados.\n",
        "  2. Pasa entradas por el modelo.\n",
        "  3. Calcula la p√©rdida.\n",
        "  4. Calcula gradientes de la p√©rdida respecto a los pesos.\n",
        "  5. Ajusta los pesos con SGD.\n",
        "\n",
        "---\n",
        "\n",
        "## üìà f. Predicciones finales\n",
        "\n",
        "```python\n",
        "print(\"Predicciones XOR (PyTorch):\", model(X_t).detach().round().view(-1).numpy())\n",
        "```\n",
        "\n",
        "* `model(X_t)`: calcula probabilidades finales.\n",
        "* `.detach()`: saca el tensor del grafo de gradientes (solo para evaluaci√≥n).\n",
        "* `.round()`: redondea probabilidades ‚Üí convierte en 0 o 1.\n",
        "* `.view(-1)`: convierte a vector 1D.\n",
        "* `.numpy()`: pasa a array de NumPy.\n",
        "\n",
        "üëâ El resultado esperado es:\n",
        "\n",
        "```\n",
        "Predicciones XOR (PyTorch): [0 1 1 0]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Conclusi√≥n\n",
        "\n",
        "* El perceptr√≥n simple **no resuelve XOR** porque es lineal.\n",
        "* Con un **MLP (capa oculta + activaci√≥n no lineal)**, PyTorch aprende la separaci√≥n correctamente.\n",
        "* Este ejemplo muestra expl√≠citamente los pasos de entrenamiento que scikit-learn oculta en `.fit()`.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0fbcdeb",
      "metadata": {
        "id": "a0fbcdeb"
      },
      "source": [
        "### üîπ TensorFlow / Keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48e022d8",
      "metadata": {
        "id": "48e022d8"
      },
      "outputs": [],
      "source": [
        "# ========== Keras / TensorFlow ==========\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import warnings\n",
        "\n",
        "# Definici√≥n del modelo\n",
        "model = Sequential([\n",
        "    Dense(4, input_dim=2, activation=\"tanh\"),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "\n",
        "    model.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
        "    model.fit(X, y, epochs=2000, verbose=0)\n",
        "\n",
        "    print(\"Predicciones XOR (Keras):\", (model.predict(X) > 0.5).astype(int).ravel())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c57010b1-f677-4796-87d9-503ba38dc00d",
      "metadata": {
        "id": "c57010b1-f677-4796-87d9-503ba38dc00d"
      },
      "source": [
        "Vamos a **explicar este bloque paso a paso**. Es el equivalente en **Keras/TensorFlow** al que ya vimos en scikit-learn y PyTorch, para resolver el problema del **XOR**.\n",
        "\n",
        "---\n",
        "\n",
        "### üì• a. Importaciones\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import warnings\n",
        "```\n",
        "\n",
        "* **tensorflow** ‚Üí librer√≠a de deep learning de Google.\n",
        "* **Sequential** ‚Üí modelo secuencial (capas apiladas una detr√°s de otra).\n",
        "* **Dense** ‚Üí capa totalmente conectada (fully connected).\n",
        "* **warnings** ‚Üí solo se usa aqu√≠ para silenciar avisos innecesarios al entrenar.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† b. Definici√≥n del modelo\n",
        "\n",
        "```python\n",
        "model = Sequential([\n",
        "    Dense(4, input_dim=2, activation=\"tanh\"),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "```\n",
        "\n",
        "* **Sequential(\\[...])**: el modelo tendr√° capas en orden.\n",
        "* `Dense(4, input_dim=2, activation=\"tanh\")`:\n",
        "\n",
        "  * Capa oculta con **4 neuronas**.\n",
        "  * `input_dim=2` ‚Üí dos variables de entrada (los bits del XOR).\n",
        "  * `activation=\"tanh\"` ‚Üí funci√≥n de activaci√≥n no lineal (clave para resolver XOR).\n",
        "* `Dense(1, activation=\"sigmoid\")`:\n",
        "\n",
        "  * Capa de salida con **1 neurona**.\n",
        "  * `sigmoid` devuelve probabilidad entre 0 y 1.\n",
        "\n",
        "üëâ Arquitectura: **2 ‚Üí 4 ‚Üí 1** (igual que en PyTorch y scikit-learn).\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öñÔ∏è c. Compilaci√≥n del modelo\n",
        "\n",
        "```python\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
        "```\n",
        "\n",
        "* `loss=\"binary_crossentropy\"` ‚Üí funci√≥n de p√©rdida para clasificaci√≥n binaria.\n",
        "* `optimizer=\"sgd\"` ‚Üí descenso de gradiente estoc√°stico.\n",
        "* `metrics=[\"accuracy\"]` ‚Üí medir√° la accuracy durante entrenamiento.\n",
        "\n",
        "---\n",
        "\n",
        "### üîÅ d. Entrenamiento\n",
        "\n",
        "```python\n",
        "model.fit(X, y, epochs=2000, verbose=0)\n",
        "```\n",
        "\n",
        "* `X` y `y` ‚Üí dataset XOR.\n",
        "* `epochs=2000` ‚Üí n√∫mero de iteraciones completas sobre los datos.\n",
        "* `verbose=0` ‚Üí entrena en silencio (sin logs).\n",
        "\n",
        "üëâ Internamente, Keras hace: forward ‚Üí p√©rdida ‚Üí backpropagation ‚Üí update de pesos (como en PyTorch, pero escondido).\n",
        "\n",
        "---\n",
        "\n",
        "### üìà e. Predicciones\n",
        "\n",
        "```python\n",
        "print(\"Predicciones XOR (Keras):\", (model.predict(X) > 0.5).astype(int).ravel())\n",
        "```\n",
        "\n",
        "* `model.predict(X)` ‚Üí devuelve probabilidades de clase (entre 0 y 1).\n",
        "* `> 0.5` ‚Üí umbral: si prob > 0.5 ‚Üí clase 1, si no ‚Üí clase 0.\n",
        "* `.astype(int)` ‚Üí convierte a enteros (0 o 1).\n",
        "* `.ravel()` ‚Üí aplana a vector 1D.\n",
        "\n",
        "üëâ Salida esperada:\n",
        "\n",
        "```\n",
        "Predicciones XOR (Keras): [0 1 1 0]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Conclusi√≥n\n",
        "\n",
        "* El **Perceptr√≥n simple** falla en XOR.\n",
        "* En **Keras**, bastan 2 capas (`Dense(4,tanh)` y `Dense(1,sigmoid)`) para resolverlo.\n",
        "* Es equivalente a lo que hicimos en scikit-learn y PyTorch, pero con una **API de alto nivel** que facilita el entrenamiento en GPU/TPU y la integraci√≥n en pipelines de producci√≥n.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97e6edbf-767a-45a1-b668-af9187f963b5",
      "metadata": {
        "id": "97e6edbf-767a-45a1-b668-af9187f963b5"
      },
      "source": [
        "## 9. üìäComparativo visual: scikit-learn vs PyTorch vs Keras (MLP 2‚Üí4‚Üí1)  \n",
        "\n",
        "```text\n",
        "                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "Input (2 bits)  ‚îÇ              ‚îÇ\n",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Capa oculta ‚îÇ  4 neuronas + tanh\n",
        "                ‚îÇ   (4,tanh)   ‚îÇ\n",
        "                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                       ‚îÇ\n",
        "                       ‚ñº\n",
        "               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "               ‚îÇ   Capa salida ‚îÇ  1 neurona + sigmoide\n",
        "               ‚îÇ   (1,sigmoid) ‚îÇ\n",
        "               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                      ‚îÇ\n",
        "                      ‚ñº\n",
        "                  Predicci√≥n (0/1)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ scikit-learn\n",
        "\n",
        "```python\n",
        "MLPClassifier(hidden_layer_sizes=(4,),\n",
        "              activation=\"tanh\",\n",
        "              max_iter=3000)\n",
        "```\n",
        "\n",
        "* Arquitectura definida con un solo par√°metro (`hidden_layer_sizes`).\n",
        "* El `.fit(X,y)` es **caja negra** ‚Üí hace forward, backprop y optimizaci√≥n autom√°ticamente.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ PyTorch\n",
        "\n",
        "```python\n",
        "nn.Sequential(\n",
        "    nn.Linear(2,4),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(4,1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "```\n",
        "\n",
        "* Arquitectura expl√≠cita (capas listadas).\n",
        "* El entrenamiento requiere **loop manual** con forward ‚Üí loss ‚Üí backward ‚Üí step.\n",
        "* Mucho control, √∫til para investigaci√≥n y personalizaci√≥n.\n",
        "\n",
        "---\n",
        "\n",
        "###üîπ Keras / TensorFlow\n",
        "\n",
        "```python\n",
        "Sequential([\n",
        "    Dense(4, input_dim=2, activation=\"tanh\"),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "```\n",
        "\n",
        "* Definici√≥n de capas parecida a PyTorch.\n",
        "* `model.fit(X,y,epochs=2000)` entrena autom√°ticamente (alto nivel).\n",
        "* Compatible con GPU/TPU y despliegue en producci√≥n.\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a3bbcfb-87b1-423f-85b4-9429d41b2d44",
      "metadata": {
        "id": "2a3bbcfb-87b1-423f-85b4-9429d41b2d44"
      },
      "source": [
        "## üîë Diferencias Clave\n",
        "\n",
        "| Aspecto                 | Scikit-learn (`MLPClassifier`) | PyTorch (`nn.Module`)                  | TensorFlow / Keras (`Sequential` / `Functional`)   |\n",
        "| ----------------------- | ------------------------------ | -------------------------------------- | -------------------------------------------------- |\n",
        "| Facilidad de uso        | Muy alto (API simple)          | Medio (hay que definir loop)           | Muy alto (API de alto nivel con `fit`)             |\n",
        "| Flexibilidad            | Limitada (solo MLPs)           | Total (CNNs, RNNs, Transformers, etc.) | Total (CNNs, RNNs, Transformers, etc.)             |\n",
        "| Entrenamiento en GPU    | ‚ùå No                           | ‚úÖ S√≠                                   | ‚úÖ S√≠ (nativo en GPU/TPU)                           |\n",
        "| Rapidez para prototipos | ‚úÖ Muy r√°pido                   | ‚ùå M√°s detallado                        | ‚úÖ Muy r√°pido (con `Sequential` o `fit`)            |\n",
        "| Nivel de control        | Bajo (caja negra)              | Muy alto (bucle manual posible)        | Intermedio (APIs de alto y bajo nivel disponibles) |\n",
        "| Comunidad/uso en DL     | Acad√©mico, ML cl√°sico          | Investigaci√≥n y prototipos avanzados   | Producci√≥n y despliegue a gran escala              |\n",
        "\n",
        "---\n",
        "\n",
        "üëâ As√≠, se ve claro que:\n",
        "\n",
        "* **Scikit-learn** ‚Üí ideal para prototipar MLPs en problemas tabulares.\n",
        "* **PyTorch** ‚Üí usado en investigaci√≥n y prototipado avanzado.\n",
        "* **TensorFlow/Keras** ‚Üí usado masivamente en producci√≥n y despliegue (GPU/TPU, cloud).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "974945e4",
      "metadata": {
        "id": "974945e4"
      },
      "source": [
        "---\n",
        "\n",
        "## 10. Preguntas de discusi√≥n\n",
        "\n",
        "1. ¬øQu√© muestra el fracaso del perceptr√≥n en XOR sobre la necesidad de MLPs?\n",
        "2. ¬øCu√°ndo elegir√≠as scikit-learn vs PyTorch vs TensorFlow?\n",
        "3. ¬øQu√© funci√≥n de activaci√≥n usar√≠as para un problema real de churn?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "007a54a1-9575-4eca-bcc2-ae17b48b32af",
      "metadata": {
        "id": "007a54a1-9575-4eca-bcc2-ae17b48b32af"
      },
      "source": [
        "### 10.1. El fracaso del perceptr√≥n en XOR\n",
        "\n",
        "El fracaso del perceptr√≥n en el problema XOR demuestra que un modelo lineal simple no puede resolver problemas que no son linealmente separables. El perceptr√≥n, que es un clasificador lineal, solo puede encontrar un hiperplano (una l√≠nea en 2D) que divide los datos en dos clases. El problema XOR no puede ser resuelto con una sola l√≠nea recta, ya que los puntos de datos de la misma clase (los `(0,0)` y `(1,1)`) no se encuentran en un lado del plano y los de la otra clase (`(0,1)` y `(1,0)`) en el otro. Para resolverlo, se necesita una frontera de decisi√≥n no lineal.\n",
        "\n",
        "Aqu√≠ es donde entran los **MLPs (Multilayer Perceptrons)**. Al usar m√∫ltiples capas de neuronas (capas ocultas), los MLPs pueden combinar las salidas de las neuronas de la capa anterior de manera no lineal, creando as√≠ fronteras de decisi√≥n mucho m√°s complejas. La capacidad de crear una representaci√≥n no lineal de los datos es la raz√≥n por la que los MLPs pueden resolver el problema XOR.\n",
        "\n",
        "### 10.2. Elecci√≥n de scikit-learn vs PyTorch vs TensorFlow\n",
        "\n",
        "La elecci√≥n entre estas bibliotecas depende del objetivo y el nivel de control que necesites.\n",
        "\n",
        "  * **Scikit-learn** es la mejor opci√≥n para la mayor√≠a de los problemas de **machine learning tradicional**. Es una biblioteca de alto nivel, f√°cil de usar y muy eficiente para tareas como regresi√≥n, clasificaci√≥n, clustering y reducci√≥n de dimensionalidad. Es ideal para prototipos r√°pidos y para modelos que no requieren el poder de las redes neuronales profundas. √ösala cuando necesites un modelo de `Random Forest`, `SVM`, `Regresi√≥n Lineal`, etc.\n",
        "\n",
        "  * **PyTorch** y **TensorFlow** son frameworks de **deep learning**. Se eligen cuando se necesita construir y entrenar redes neuronales complejas. La principal diferencia entre ellos radica en su filosof√≠a de dise√±o:\n",
        "\n",
        "      * **PyTorch** es conocido por su **naturaleza m√°s `Pythonic` y su curva de aprendizaje m√°s suave**. Utiliza grafos de computaci√≥n din√°micos, lo que lo hace m√°s flexible y f√°cil de depurar. Es muy popular en el √°mbito de la investigaci√≥n y en proyectos donde la experimentaci√≥n r√°pida es clave.\n",
        "      * **TensorFlow** (especialmente con su API `Keras`) es un framework robusto y de alto rendimiento. Es conocido por su **fuerte soporte para la producci√≥n y el despliegue a gran escala**. Ofrece una amplia gama de herramientas para monitoreo (`TensorBoard`), optimizaci√≥n y despliegue en diferentes plataformas (`TensorFlow Lite`, `TensorFlow.js`). Es una excelente opci√≥n para proyectos empresariales que requieren escalabilidad.\n",
        "\n",
        "\n",
        "\n",
        "-----\n",
        "\n",
        "\n",
        "\n",
        "### 10.3. Funci√≥n de activaci√≥n para un problema de churn\n",
        "\n",
        "Para un problema de **churn (abandono de clientes)**, que es un problema de clasificaci√≥n binaria (el cliente abandona o no abandona), la funci√≥n de activaci√≥n ideal para la **capa de salida** es la funci√≥n **Sigmoide**.\n",
        "\n",
        "La funci√≥n Sigmoide comprime cualquier valor real en un rango entre 0 y 1. Esto es perfecto para la clasificaci√≥n binaria, ya que la salida de la neurona se puede interpretar directamente como la **probabilidad de que el cliente abandone**. Por ejemplo, una salida de `0.85` podr√≠a significar que hay un 85% de probabilidad de que el cliente abandone.\n",
        "\n",
        "Para las **capas ocultas**, las funciones de activaci√≥n m√°s comunes y efectivas son **ReLU (Rectified Linear Unit)** o sus variantes (`Leaky ReLU`). Estas funciones ayudan a mitigar el problema del **gradiente desvaneciente** (vanishing gradient problem), permitiendo que la red aprenda de manera m√°s eficiente en capas profundas. Por lo tanto, una arquitectura com√∫n ser√≠a usar **ReLU** en las capas ocultas y **Sigmoide** en la capa de salida."
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}