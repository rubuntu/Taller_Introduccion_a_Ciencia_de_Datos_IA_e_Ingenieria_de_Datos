{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rubuntu/Taller_Introduccion_a_Ciencia_de_Datos_IA_e_Ingenieria_de_Datos/blob/main/sesion_10_introduccion_a_redes_neuronales.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "675896ac-c400-4aa9-bcaf-e53ae5679071",
      "metadata": {
        "id": "675896ac-c400-4aa9-bcaf-e53ae5679071"
      },
      "source": [
        "# ğŸ“˜ SesiÃ³n: Del PerceptrÃ³n ClÃ¡sico al Multicapa (MLP) con scikit-learn, PyTorch y TensorFlow\n",
        "\n",
        "## Objetivos\n",
        "\n",
        "* Comprender el **PerceptrÃ³n clÃ¡sico** implementado en NumPy.\n",
        "* Visualizar la **frontera de decisiÃ³n** en problemas lÃ³gicos simples (AND, OR, XOR).\n",
        "* Conocer las **funciones de activaciÃ³n** de los MLPs.\n",
        "* Comparar modelos: **PerceptrÃ³n simple vs MLP vs RegresiÃ³n LogÃ­stica**.\n",
        "* Resolver un caso real (Churn) con un MLP.\n",
        "* Implementar **MLP en scikit-learn, PyTorch y TensorFlow/Keras**.\n",
        "* Visualizar diferencias con un **diagrama comparativo**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "537f91da-1008-4bb7-bd32-c6874c3fbf93",
      "metadata": {
        "id": "537f91da-1008-4bb7-bd32-c6874c3fbf93"
      },
      "source": [
        "---\n",
        "\n",
        "## 1. PerceptrÃ³n\n",
        "\n",
        "- Referencia: https://es.wikipedia.org/wiki/Perceptr%C3%B3n\n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b0/Perceptr%C3%B3n_5_unidades.svg/960px-Perceptr%C3%B3n_5_unidades.svg.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c06fd1f",
      "metadata": {
        "id": "1c06fd1f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, n_inputs, lr=0.1, n_epochs=10):\n",
        "        self.lr = lr\n",
        "        self.n_epochs = n_epochs\n",
        "        self.w = np.zeros(n_inputs)\n",
        "        self.b = 0.0\n",
        "\n",
        "    def activation(self, z):\n",
        "        return np.where(z >= 0, 1, 0)\n",
        "\n",
        "    def predict(self, X):\n",
        "        z = np.dot(X, self.w) + self.b\n",
        "        return self.activation(z)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for epoch in range(self.n_epochs):\n",
        "            errors = 0\n",
        "            for xi, target in zip(X, y):\n",
        "                y_hat = self.predict(xi)\n",
        "                update = self.lr * (target - y_hat)\n",
        "                self.w += update * xi\n",
        "                self.b += update\n",
        "                errors += int(update != 0.0)\n",
        "            print(f\"Epoch {epoch+1} - Errores: {errors}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El cÃ³digo implementa un **PerceptrÃ³n**, uno de los modelos mÃ¡s simples de **red neuronal** para la clasificaciÃ³n. Aunque es una versiÃ³n bÃ¡sica, es fundamental para entender cÃ³mo funcionan los modelos mÃ¡s complejos de *deep learning*.\n",
        "\n",
        "---\n",
        "\n",
        "### **Componentes del PerceptrÃ³n**\n",
        "\n",
        "#### **a. Constructor (`__init__`)**\n",
        "Esta funciÃ³n se ejecuta al crear una nueva instancia de la clase `Perceptron`.\n",
        "* `n_inputs`: Es el nÃºmero de caracterÃ­sticas o *features* en tus datos de entrada. El modelo crea un vector de pesos (`self.w`) del mismo tamaÃ±o, inicializÃ¡ndolo en cero.\n",
        "* `lr`: Significa **tasa de aprendizaje** (*learning rate*). Es un hiperparÃ¡metro crucial que determina quÃ© tan grande serÃ¡ el ajuste en los pesos del modelo con cada error. Un valor pequeÃ±o significa que los cambios son graduales, mientras que un valor grande puede causar que el modelo \"salte\" la soluciÃ³n Ã³ptima.\n",
        "* `n_epochs`: Es el nÃºmero de veces que el modelo procesarÃ¡ todo el conjunto de datos de entrenamiento. Cada *Ã©poca* es un ciclo completo de aprendizaje.\n",
        "* `self.w`: El **vector de pesos** que el modelo aprende. Cada peso se asocia a una caracterÃ­stica de entrada y representa su importancia para la predicciÃ³n.\n",
        "* `self.b`: El **sesgo** (*bias*). Es un valor que se aÃ±ade al resultado, permitiendo que la lÃ­nea de decisiÃ³n se desplace, lo que ayuda a la red a clasificar datos que no son linealmente separables a travÃ©s del origen.\n",
        "\n",
        "#### **b. FunciÃ³n de ActivaciÃ³n (`activation`)**\n",
        "* Esta funciÃ³n recibe la suma ponderada de las entradas mÃ¡s el sesgo (`z`). En el caso del PerceptrÃ³n, se usa una funciÃ³n de activaciÃ³n simple.\n",
        "* `np.where(z >= 0, 1, 0)`: Esta es una **funciÃ³n escalÃ³n** (*step function*). Si el resultado de `z` es mayor o igual a cero, la neurona \"se activa\" y el resultado es 1. De lo contrario, no se activa y el resultado es 0. Esto permite al modelo hacer una clasificaciÃ³n binaria.\n",
        "\n",
        "#### **c. PredicciÃ³n (`predict`)**\n",
        "* Esta funciÃ³n toma un conjunto de datos de entrada (`X`) y calcula la predicciÃ³n.\n",
        "* `z = np.dot(X, self.w) + self.b`: AquÃ­ se realiza el cÃ¡lculo central del PerceptrÃ³n. Se toma el **producto punto** de los datos de entrada (`X`) y el vector de pesos (`self.w`), y se le suma el sesgo (`self.b`). Este cÃ¡lculo representa la entrada neta a la neurona.\n",
        "* Luego, este valor `z` se pasa a la funciÃ³n de activaciÃ³n, que devuelve la predicciÃ³n final (0 o 1).\n",
        "\n",
        "#### **d. Entrenamiento (`fit`)**\n",
        "Este es el corazÃ³n del algoritmo, donde el modelo aprende.\n",
        "* El bucle `for epoch in range(self.n_epochs):` permite que el modelo se entrene varias veces sobre el mismo conjunto de datos.\n",
        "* El bucle `for xi, target in zip(X, y):` itera sobre cada ejemplo de entrenamiento (`xi`) y su etiqueta correcta (`target`).\n",
        "* `y_hat = self.predict(xi)`: El modelo hace una predicciÃ³n con los pesos y el sesgo actuales.\n",
        "* `update = self.lr * (target - y_hat)`: AquÃ­ se calcula el **ajuste** necesario. El error es la diferencia entre la etiqueta real (`target`) y la predicciÃ³n (`y_hat`).\n",
        "    * Si la predicciÃ³n es correcta, `(target - y_hat)` es 0, y no hay actualizaciÃ³n.\n",
        "    * Si la predicciÃ³n es incorrecta, `(target - y_hat)` es 1 o -1, y los pesos se ajustan en la direcciÃ³n correcta, multiplicados por la tasa de aprendizaje.\n",
        "* `self.w += update * xi` y `self.b += update`: Los pesos y el sesgo se **actualizan** en base al error. Este es el paso de aprendizaje.\n",
        "* `errors += int(update != 0.0)`: Se cuenta el nÃºmero de errores en cada Ã©poca para monitorear el progreso del aprendizaje. El objetivo del entrenamiento es que el nÃºmero de errores se reduzca a cero.\n",
        "\n",
        "El PerceptrÃ³n aprende a clasificar datos de forma **lineal** ajustando sus pesos y sesgo cada vez que comete un error, hasta que todas las clasificaciones sean correctas o se complete el nÃºmero de Ã©pocas."
      ],
      "metadata": {
        "id": "7oXeb9Dd9G1n"
      },
      "id": "7oXeb9Dd9G1n"
    },
    {
      "cell_type": "markdown",
      "id": "cccf8518-86e4-49ee-bd70-236ebc68fe93",
      "metadata": {
        "id": "cccf8518-86e4-49ee-bd70-236ebc68fe93"
      },
      "source": [
        "### **GrÃ¡fico ğŸ“Š de la funciÃ³n de activaciÃ³n del PerceptrÃ³n clÃ¡sico**:\n",
        "\n",
        "* Es una **funciÃ³n escalÃ³n (step function)**.\n",
        "* Devuelve **0 si $x<0$** y **1 si $x \\geq 0$**.\n",
        "* Representa la decisiÃ³n binaria mÃ¡s simple: â€œdispara o no disparaâ€.\n",
        "\n",
        "ğŸ‘‰ A diferencia de funciones suaves como sigmoide o ReLU, esta funciÃ³n no tiene derivada Ãºtil, lo que limita al perceptrÃ³n simple en problemas mÃ¡s complejos.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17c43264-34b5-49d7-9324-1b79a979c1a8",
      "metadata": {
        "id": "17c43264-34b5-49d7-9324-1b79a979c1a8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Rango de entrada\n",
        "x = np.linspace(-3, 3, 400)\n",
        "\n",
        "# FunciÃ³n escalÃ³n (step), activaciÃ³n del PerceptrÃ³n clÃ¡sico\n",
        "step = np.where(x >= 0, 1, 0)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(x, step, drawstyle=\"steps-post\", color=\"purple\", linewidth=2, label=\"Step function\")\n",
        "plt.axhline(0, color=\"gray\", linestyle=\"--\")\n",
        "plt.axhline(1, color=\"gray\", linestyle=\"--\")\n",
        "plt.axvline(0, color=\"gray\", linestyle=\"--\")\n",
        "plt.title(\"FunciÃ³n de ActivaciÃ³n del PerceptrÃ³n (Step)\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"f(x)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd19d1a1",
      "metadata": {
        "id": "cd19d1a1"
      },
      "source": [
        "---\n",
        "\n",
        "## 2. Fronteras de decisiÃ³n: AND y OR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07b4d5bb",
      "metadata": {
        "id": "07b4d5bb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_decision_boundary(X, y, model, title):\n",
        "    xx, yy = np.meshgrid(np.linspace(-0.5,1.5,200),\n",
        "                         np.linspace(-0.5,1.5,200))\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.Paired)\n",
        "    plt.scatter(X[:,0], X[:,1], c=y, edgecolor=\"k\", cmap=plt.cm.Paired, s=100)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "# AND\n",
        "X_and = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y_and = np.array([0,0,0,1])\n",
        "p_and = Perceptron(2, lr=0.1, n_epochs=10); p_and.fit(X_and, y_and)\n",
        "plot_decision_boundary(X_and, y_and, p_and, \"PerceptrÃ³n - AND\")\n",
        "\n",
        "# OR\n",
        "X_or = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y_or = np.array([0,1,1,1])\n",
        "p_or = Perceptron(2, lr=0.1, n_epochs=10); p_or.fit(X_or, y_or)\n",
        "plot_decision_boundary(X_or, y_or, p_or, \"PerceptrÃ³n - OR\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31ec03f4",
      "metadata": {
        "id": "31ec03f4"
      },
      "source": [
        "---\n",
        "\n",
        "## 3. Caso XOR (no separable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2028223c",
      "metadata": {
        "id": "2028223c"
      },
      "outputs": [],
      "source": [
        "X_xor = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y_xor = np.array([0,1,1,0])\n",
        "\n",
        "p_xor = Perceptron(2, lr=0.1, n_epochs=20)\n",
        "p_xor.fit(X_xor, y_xor)\n",
        "plot_decision_boundary(X_xor, y_xor, p_xor, \"PerceptrÃ³n - XOR (fracasa)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4560671-e0a1-46eb-866f-fcf13a214d6e",
      "metadata": {
        "id": "b4560671-e0a1-46eb-866f-fcf13a214d6e"
      },
      "source": [
        "ğŸ“Œ El perceptrÃ³n no logra resolver XOR porque el problema no es linealmente separable.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95470e96-261b-43be-8aab-6a00812d64fe",
      "metadata": {
        "id": "95470e96-261b-43be-8aab-6a00812d64fe"
      },
      "source": [
        "## 4. PerceptrÃ³n multicapa - Multilayer perceptron (MLP)\n",
        "\n",
        "- Referencia: https://es.wikipedia.org/wiki/Perceptr%C3%B3n_multicapa\n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/6/64/RedNeuronalArtificial.png?20160319151219)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79606d25-f97a-4992-95f8-0c9ab971b5e4",
      "metadata": {
        "id": "79606d25-f97a-4992-95f8-0c9ab971b5e4"
      },
      "source": [
        "## 5. Funciones de activaciÃ³n en MLP\n",
        "\n",
        "* **identity**: $f(x)=x$\n",
        "* **logistic**: sigmoide\n",
        "* **tanh**: tangente hiperbÃ³lica\n",
        "* **relu**: rectificada\n",
        "\n",
        "Estas funciones permiten aprender **fronteras no lineales**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e265c9f-2e04-4b0c-adeb-47fa807d30d7",
      "metadata": {
        "id": "3e265c9f-2e04-4b0c-adeb-47fa807d30d7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Rango de entrada\n",
        "x = np.linspace(-5, 5, 400)\n",
        "\n",
        "# Funciones de activaciÃ³n\n",
        "identity = x\n",
        "logistic = 1 / (1 + np.exp(-x))\n",
        "tanh = np.tanh(x)\n",
        "relu = np.maximum(0, x)\n",
        "\n",
        "# Graficar\n",
        "fig, axs = plt.subplots(2, 2, figsize=(10,8))\n",
        "\n",
        "axs[0,0].plot(x, identity, label=\"identity\")\n",
        "axs[0,0].set_title(\"Identity: f(x)=x\")\n",
        "axs[0,0].axhline(0, color=\"gray\", linestyle=\"--\")\n",
        "axs[0,0].axvline(0, color=\"gray\", linestyle=\"--\")\n",
        "\n",
        "axs[0,1].plot(x, logistic, label=\"sigmoid\", color=\"orange\")\n",
        "axs[0,1].set_title(\"Logistic (sigmoide)\")\n",
        "axs[0,1].axhline(0.5, color=\"gray\", linestyle=\"--\")\n",
        "\n",
        "axs[1,0].plot(x, tanh, label=\"tanh\", color=\"green\")\n",
        "axs[1,0].set_title(\"Tanh\")\n",
        "axs[1,0].axhline(0, color=\"gray\", linestyle=\"--\")\n",
        "\n",
        "axs[1,1].plot(x, relu, label=\"ReLU\", color=\"red\")\n",
        "axs[1,1].set_title(\"ReLU\")\n",
        "axs[1,1].axhline(0, color=\"gray\", linestyle=\"--\")\n",
        "axs[1,1].axvline(0, color=\"gray\", linestyle=\"--\")\n",
        "\n",
        "for ax in axs.flat:\n",
        "    ax.legend()\n",
        "    ax.grid(True)\n",
        "\n",
        "plt.suptitle(\"Funciones de ActivaciÃ³n en MLP\", fontsize=14, weight=\"bold\")\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f420f67-dda7-46a5-afc4-e24a7eba82c5",
      "metadata": {
        "id": "9f420f67-dda7-46a5-afc4-e24a7eba82c5"
      },
      "source": [
        "---\n",
        "\n",
        "## ğŸ”¹ Ejemplo con dataset MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d303518-30c7-4c97-a5c4-22e8cf0adc67",
      "metadata": {
        "id": "4d303518-30c7-4c97-a5c4-22e8cf0adc67"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Descargar MNIST (70k imÃ¡genes 28x28)\n",
        "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
        "X, y = mnist[\"data\"], mnist[\"target\"].astype(int)\n",
        "\n",
        "# Mostrar una imagen\n",
        "idx = 0\n",
        "plt.imshow(X[idx].reshape(28,28), cmap=\"gray\")\n",
        "plt.title(f\"Etiqueta: {y[idx]}\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8e769d4-f462-49c3-86fa-f7f449783483",
      "metadata": {
        "id": "b8e769d4-f462-49c3-86fa-f7f449783483"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Descargar MNIST (70k imÃ¡genes 28x28)\n",
        "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
        "X, y = mnist[\"data\"], mnist[\"target\"].astype(int)\n",
        "\n",
        "\n",
        "# Normalizar los datos\n",
        "# Escalar los valores de pÃ­xeles al rango [0, 1] para un mejor rendimiento del modelo\n",
        "X = X / 255.0\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "# Usamos el 80% para entrenar y el 20% para probar\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Inicializar y entrenar el clasificador MLP\n",
        "# 'hidden_layer_sizes' define la estructura de la red, en este caso 100 neuronas\n",
        "# 'max_iter' es el nÃºmero mÃ¡ximo de Ã©pocas de entrenamiento\n",
        "print(\"Entrenando el clasificador MLP...\")\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=20, random_state=42)\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "# Hacer predicciones en el conjunto de prueba\n",
        "y_pred = mlp.predict(X_test)\n",
        "\n",
        "# Evaluar la precisiÃ³n del modelo\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"PrecisiÃ³n del modelo MLP: {accuracy:.4f}\")\n",
        "\n",
        "# Mostrar una imagen y la predicciÃ³n del modelo\n",
        "idx_test = 0  # Puedes cambiar este Ã­ndice para ver otras imÃ¡genes\n",
        "plt.imshow(X_test[idx_test].reshape(28,28), cmap=\"gray\")\n",
        "plt.title(f\"Etiqueta Real: {y_test[idx_test]}, PredicciÃ³n del modelo: {y_pred[idx_test]}\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e19f14b-5d4e-4cec-a24e-572738fe33a3",
      "metadata": {
        "id": "3e19f14b-5d4e-4cec-a24e-572738fe33a3"
      },
      "source": [
        "\n",
        "-----\n",
        "\n",
        "## ğŸ‘‰ **ExplicaciÃ³n**:\n",
        "\n",
        "### Datos\n",
        "\n",
        "* `fetch_openml`: descarga el dataset **MNIST** desde OpenML (70,000 imÃ¡genes de dÃ­gitos escritos a mano, cada imagen tiene 28x28 = 784 pÃ­xeles).\n",
        "* `X`: contiene las imÃ¡genes en formato **vector de 784 valores**.\n",
        "* `y`: contiene las etiquetas (0â€“9).\n",
        "\n",
        "### Preprocesamiento de Datos\n",
        "\n",
        "Antes de alimentar los datos a la red neuronal, es crucial **preprocesarlos** adecuadamente.\n",
        "\n",
        "  * **NormalizaciÃ³n:** Se dividen los valores de los pÃ­xeles (que van de 0 a 255) por 255. Esto escala todos los valores al rango `[0, 1]`. Esta normalizaciÃ³n es esencial para que los algoritmos basados en gradiente, como el que usa el MLP, converjan mÃ¡s rÃ¡pido y de manera mÃ¡s estable.\n",
        "\n",
        "### DivisiÃ³n de Datos\n",
        "\n",
        "Para evaluar el rendimiento del modelo de forma realista, se dividen los datos en dos conjuntos:\n",
        "\n",
        "  * **Entrenamiento (`X_train`, `y_train`):** El modelo aprende de estas imÃ¡genes y sus etiquetas.\n",
        "  * **Prueba (`X_test`, `y_test`):** El modelo se prueba con estas imÃ¡genes, que nunca ha visto antes, para evaluar su capacidad de generalizaciÃ³n. El parÃ¡metro `test_size=0.2` indica que el 20% de los datos se utilizarÃ¡ para la prueba.\n",
        "\n",
        "### Clasificador MLP de scikit-learn\n",
        "\n",
        "  * **`MLPClassifier`:** Es la clase que implementa el **Perceptron Multicapa**. Es una red neuronal de tipo *feed-forward*.\n",
        "  * **`hidden_layer_sizes=(100,)`:** Define la arquitectura de la red. En este caso, se crea una capa oculta con 100 neuronas. Puedes experimentar con diferentes tamaÃ±os o aÃ±adir mÃ¡s capas (ej: `(100, 50, 20)`).\n",
        "  * **`max_iter=20`:** Establece el nÃºmero mÃ¡ximo de Ã©pocas (iteraciones completas sobre los datos de entrenamiento) que el modelo realizarÃ¡. Aumentar este valor puede mejorar el rendimiento, pero tambiÃ©n el tiempo de entrenamiento.\n",
        "  * **`mlp.fit(X_train, y_train)`:** Este es el paso de entrenamiento. El modelo ajusta sus pesos internos para aprender a mapear las imÃ¡genes (`X_train`) a sus etiquetas correctas (`y_train`).\n",
        "  * **`mlp.predict(X_test)`:** Una vez entrenado, el modelo predice las etiquetas para las imÃ¡genes en el conjunto de prueba.\n",
        "\n",
        "### EvaluaciÃ³n del Modelo\n",
        "\n",
        "  * **`accuracy_score`:** Se utiliza para medir la **precisiÃ³n** del modelo. Compara las predicciones (`y_pred`) con las etiquetas reales (`y_test`) y devuelve la proporciÃ³n de predicciones correctas. La precisiÃ³n es una mÃ©trica comÃºn para evaluar modelos de clasificaciÃ³n.\n",
        "\n",
        "### VisualizaciÃ³n\n",
        "  * **`plt.imshow`:** reacomoda una fila de 784 valores en una matriz 28x28 para visualizar como imagen en escala de grises."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c62bc4ae-d4aa-449f-9c7f-4d40fc3bd19b",
      "metadata": {
        "id": "c62bc4ae-d4aa-449f-9c7f-4d40fc3bd19b"
      },
      "source": [
        "---\n",
        "\n",
        "## 6. Caso real: Telco Churn (MLP con 1 capa oculta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ebca816",
      "metadata": {
        "id": "0ebca816"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import roc_auc_score, RocCurveDisplay\n",
        "\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
        "df = pd.read_excel(url, header=1)\n",
        "df.rename(columns={\"default payment next month\": \"default\"}, inplace=True)\n",
        "\n",
        "y = df[\"default\"]\n",
        "X = pd.get_dummies(df.drop(columns=[\"ID\",\"default\"]), drop_first=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, stratify=y, random_state=42)\n",
        "scaler = StandardScaler(with_mean=False)\n",
        "X_train = scaler.fit_transform(X_train); X_test = scaler.transform(X_test)\n",
        "\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(64,), max_iter=1000, random_state=42)\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "RocCurveDisplay.from_estimator(mlp, X_test, y_test)\n",
        "print(\"AUC:\", roc_auc_score(y_test, mlp.predict_proba(X_test)[:,1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55b0b558-9214-4a20-a6cd-8b6bf27b3586",
      "metadata": {
        "id": "55b0b558-9214-4a20-a6cd-8b6bf27b3586"
      },
      "source": [
        "## ExplicaciÃ³n\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ“¥ 6.1. Importar librerÃ­as\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import roc_auc_score, RocCurveDisplay\n",
        "```\n",
        "\n",
        "* **pandas** â†’ manejo de datos.\n",
        "* **train\\_test\\_split** â†’ divide dataset en entrenamiento y test.\n",
        "* **StandardScaler** â†’ normaliza variables numÃ©ricas (importante para MLP).\n",
        "* **MLPClassifier** â†’ red neuronal de scikit-learn.\n",
        "* **roc\\_auc\\_score, RocCurveDisplay** â†’ mÃ©tricas y visualizaciÃ³n ROC.\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ“Š 6.2. Cargar dataset UCI\n",
        "\n",
        "```python\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
        "df = pd.read_excel(url, header=1)\n",
        "df.rename(columns={\"default payment next month\": \"default\"}, inplace=True)\n",
        "```\n",
        "\n",
        "* Se descarga el **dataset Default of Credit Card Clients** en formato Excel.\n",
        "* `header=1`: omite la primera fila (tiene descripciÃ³n).\n",
        "* Se renombra la columna objetivo `\"default payment next month\"` a `\"default\"` para mayor claridad.\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ¯ 6.3. Variables independientes y dependiente\n",
        "\n",
        "```python\n",
        "y = df[\"default\"]\n",
        "X = pd.get_dummies(df.drop(columns=[\"ID\",\"default\"]), drop_first=True)\n",
        "```\n",
        "\n",
        "* **y**: columna objetivo â†’ `default` (1 = cliente incumple, 0 = no incumple).\n",
        "* **X**: resto de variables (sociodemogrÃ¡ficas, historial de pagos, etc.).\n",
        "\n",
        "  * Se eliminan `\"ID\"` (irrelevante) y `\"default\"` (ya es target).\n",
        "  * `get_dummies`: convierte variables categÃ³ricas en variables dummy (one-hot encoding).\n",
        "  * `drop_first=True`: evita multicolinealidad (elimina una categorÃ­a redundante).\n",
        "\n",
        "---\n",
        "\n",
        "### âœ‚ï¸ 6.4. DivisiÃ³n train/test\n",
        "\n",
        "```python\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, stratify=y, random_state=42\n",
        ")\n",
        "```\n",
        "\n",
        "* Divide en **70% entrenamiento / 30% test**.\n",
        "* `stratify=y`: mantiene la misma proporciÃ³n de casos de default en ambos conjuntos.\n",
        "* `random_state=42`: asegura reproducibilidad.\n",
        "\n",
        "---\n",
        "\n",
        "### âš–ï¸ 6.5. Escalado de variables\n",
        "\n",
        "```python\n",
        "scaler = StandardScaler(with_mean=False)\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "```\n",
        "\n",
        "* El **MLP es sensible a la escala de las variables** â†’ normalizamos para que todas tengan media 0 y desviaciÃ³n 1.\n",
        "* `with_mean=False`: se usa porque con dummies (sparse matrix) no conviene centrar en 0.\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ§  6.6. Definir y entrenar MLP\n",
        "\n",
        "```python\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(64,), max_iter=1000, random_state=42)\n",
        "mlp.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "* `MLPClassifier`: crea un perceptrÃ³n multicapa.\n",
        "* `hidden_layer_sizes=(64,)`: 1 capa oculta con 64 neuronas.\n",
        "* `max_iter=1000`: hasta 1000 iteraciones para converger.\n",
        "* `.fit`: entrena la red en los datos de entrenamiento.\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ“ˆ 6.7. EvaluaciÃ³n con curva ROC y AUC\n",
        "\n",
        "```python\n",
        "RocCurveDisplay.from_estimator(mlp, X_test, y_test)\n",
        "print(\"AUC:\", roc_auc_score(y_test, mlp.predict_proba(X_test)[:,1]))\n",
        "```\n",
        "\n",
        "* `.predict_proba(X_test)[:,1]`: obtiene la **probabilidad predicha de clase 1 (default)**.\n",
        "* `roc_auc_score`: calcula el **Ãrea Bajo la Curva ROC (AUC)**.\n",
        "\n",
        "  * AUC = 0.5 â†’ azar.\n",
        "  * AUC cerca de 1 â†’ excelente modelo.\n",
        "* `RocCurveDisplay`: dibuja la **curva ROC** (trade-off entre TPR y FPR).\n",
        "\n",
        "---\n",
        "\n",
        "### âœ… Resumen del flujo\n",
        "\n",
        "1. Descargamos dataset de defaults.\n",
        "2. Preparamos variables (dummy encoding + escalado).\n",
        "3. Dividimos en train/test.\n",
        "4. Entrenamos un **MLP con 64 neuronas ocultas**.\n",
        "5. Evaluamos con **ROC y AUC** â†’ mÃ©trica estÃ¡ndar en problemas de scoring crediticio.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d768db05",
      "metadata": {
        "id": "d768db05"
      },
      "source": [
        "---\n",
        "\n",
        "## 7. ComparaciÃ³n: PerceptrÃ³n vs MLP vs RegresiÃ³n LogÃ­stica"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08b9e782",
      "metadata": {
        "id": "08b9e782"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression, Perceptron as SkPerceptron\n",
        "\n",
        "perc = SkPerceptron(max_iter=1000, random_state=42).fit(X_train, y_train)\n",
        "logreg = LogisticRegression(max_iter=1000).fit(X_train, y_train)\n",
        "\n",
        "print(\"Acc (PerceptrÃ³n):\", perc.score(X_test,y_test))\n",
        "print(\"AUC (LogReg):\", roc_auc_score(y_test, logreg.predict_proba(X_test)[:,1]))\n",
        "print(\"AUC (MLP):\", roc_auc_score(y_test, mlp.predict_proba(X_test)[:,1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5006fd1a",
      "metadata": {
        "id": "5006fd1a"
      },
      "source": [
        "---\n",
        "\n",
        "## 8. Resolver XOR con MLP en 3 librerÃ­as\n",
        "\n",
        "### ğŸ”¹ scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f321ab0",
      "metadata": {
        "id": "0f321ab0"
      },
      "outputs": [],
      "source": [
        "# ========== Scikit-learn ==========\n",
        "import numpy as np\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Dataset XOR\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([0,1,1,0])\n",
        "\n",
        "mlp_sklearn = MLPClassifier(hidden_layer_sizes=(4,),\n",
        "                            activation=\"tanh\",\n",
        "                            max_iter=3000,\n",
        "                            random_state=42)\n",
        "mlp_sklearn.fit(X,y)\n",
        "print(\"Predicciones XOR (sklearn):\", mlp_sklearn.predict(X))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f823acea-34e7-40d8-b22e-7c79028794d1",
      "metadata": {
        "id": "f823acea-34e7-40d8-b22e-7c79028794d1"
      },
      "source": [
        "### ğŸ“Š a. Dataset XOR\n",
        "\n",
        "```python\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([0,1,1,0])\n",
        "```\n",
        "\n",
        "* `X`: son las **entradas** (todas las combinaciones posibles de dos bits).\n",
        "\n",
        "  * (0,0)\n",
        "  * (0,1)\n",
        "  * (1,0)\n",
        "  * (1,1)\n",
        "\n",
        "* `y`: es la **salida esperada (XOR lÃ³gico)**:\n",
        "\n",
        "  * 0 XOR 0 â†’ 0\n",
        "  * 0 XOR 1 â†’ 1\n",
        "  * 1 XOR 0 â†’ 1\n",
        "  * 1 XOR 1 â†’ 0\n",
        "\n",
        "ğŸ“Œ Este dataset **no es linealmente separable**, por lo que un perceptrÃ³n simple (funciÃ³n escalÃ³n + hiperplano) no puede resolverlo.\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ§  b. Definir MLP\n",
        "\n",
        "```python\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(4,), activation=\"tanh\", max_iter=3000, random_state=42)\n",
        "```\n",
        "\n",
        "* `MLPClassifier`: perceptrÃ³n multicapa de scikit-learn.\n",
        "* `hidden_layer_sizes=(4,)`:\n",
        "\n",
        "  * Significa **una capa oculta con 4 neuronas**.\n",
        "  * Esto da al modelo capacidad de representar **fronteras no lineales**.\n",
        "* `activation=\"tanh\"`:\n",
        "\n",
        "  * Usamos **tangente hiperbÃ³lica** como activaciÃ³n.\n",
        "  * Es no lineal y centrada en 0, muy Ãºtil para este tipo de problemas.\n",
        "* `max_iter=3000`:\n",
        "\n",
        "  * Permitimos hasta 3000 iteraciones para asegurar que el modelo converge.\n",
        "* `random_state=42`:\n",
        "\n",
        "  * Semilla fija para reproducibilidad.\n",
        "\n",
        "---\n",
        "\n",
        "### âš¡ c. Entrenar el modelo\n",
        "\n",
        "```python\n",
        "mlp.fit(X,y)\n",
        "```\n",
        "\n",
        "* El modelo recibe las entradas `X` y etiquetas `y`.\n",
        "* Internamente realiza:\n",
        "\n",
        "  1. Forward pass (propaga datos por la red).\n",
        "  2. Calcula error con **entropÃ­a cruzada**.\n",
        "  3. Backpropagation (ajusta pesos con gradiente descendente).\n",
        "  4. Itera hasta minimizar error o llegar a `max_iter`.\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ“ˆ d. Predicciones\n",
        "\n",
        "```python\n",
        "print(\"Predicciones XOR (sklearn):\", mlp.predict(X))\n",
        "```\n",
        "\n",
        "* Se evalÃºa el MLP en las 4 combinaciones de entrada.\n",
        "* La salida esperada es `[0,1,1,0]`.\n",
        "* Si la red aprendiÃ³ correctamente, imprime algo como:\n",
        "\n",
        "```\n",
        "Predicciones XOR (sklearn): [0 1 1 0]\n",
        "```\n",
        "\n",
        "âœ… Lo importante: el **MLP logra resolver XOR**, demostrando que al aÃ±adir **capa oculta + activaciÃ³n no lineal**, el modelo supera las limitaciones del perceptrÃ³n simple.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb0754fb",
      "metadata": {
        "id": "eb0754fb"
      },
      "source": [
        "### ğŸ”¹ PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3d4721b",
      "metadata": {
        "id": "a3d4721b"
      },
      "outputs": [],
      "source": [
        "# ========== PyTorch ==========\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "\n",
        "# Dataset en tensores\n",
        "X_t = torch.tensor(X, dtype=torch.float32)\n",
        "y_t = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "# DefiniciÃ³n del modelo\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(2,4),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(4,1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "loss_fn = nn.BCELoss()\n",
        "opt = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# Entrenamiento\n",
        "for epoch in range(2000):\n",
        "    opt.zero_grad()\n",
        "    y_pred = model(X_t)\n",
        "    loss = loss_fn(y_pred, y_t)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "print(\"Predicciones XOR (PyTorch):\", model(X_t).detach().round().view(-1).numpy())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec294b81-7f14-443b-b994-a3b0473815e2",
      "metadata": {
        "id": "ec294b81-7f14-443b-b994-a3b0473815e2"
      },
      "source": [
        "Vamos a **explicar este bloque PyTorch paso a paso**. El objetivo es mostrar cÃ³mo un **MLP simple** con una capa oculta aprende a resolver el problema **XOR**, que el perceptrÃ³n clÃ¡sico no puede.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“¥ a. Importaciones\n",
        "\n",
        "```python\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "```\n",
        "\n",
        "* **torch** â†’ librerÃ­a principal de tensores.\n",
        "* **nn** â†’ mÃ³dulo de redes neuronales (`Linear`, `Tanh`, `Sigmoid`, etc.).\n",
        "* **optim** â†’ optimizadores (`SGD`, Adam, etc.).\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“Š b. Dataset en tensores\n",
        "\n",
        "```python\n",
        "X_t = torch.tensor(X, dtype=torch.float32)\n",
        "y_t = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
        "```\n",
        "\n",
        "* `X_t`: entradas (matriz de 4 ejemplos Ã— 2 features).\n",
        "* `y_t`: etiquetas, convertidas en vector columna con `.unsqueeze(1)` para que tenga forma `(4,1)`.\n",
        "\n",
        "  * Necesario porque la red devuelve una salida de 1 neurona.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ§  c. DefiniciÃ³n del modelo\n",
        "\n",
        "```python\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(2,4),   # capa lineal: 2 -> 4\n",
        "    nn.Tanh(),        # activaciÃ³n no lineal\n",
        "    nn.Linear(4,1),   # capa lineal: 4 -> 1\n",
        "    nn.Sigmoid()      # salida probabilÃ­stica (0 a 1)\n",
        ")\n",
        "```\n",
        "\n",
        "* Es un **MLP con arquitectura 2 â†’ 4 â†’ 1**.\n",
        "* `nn.Sequential`: permite encadenar capas.\n",
        "* `nn.Linear(2,4)`: transforma los 2 inputs en 4 neuronas ocultas.\n",
        "* `nn.Tanh()`: introduce no linealidad â†’ clave para aprender XOR.\n",
        "* `nn.Linear(4,1)`: capa de salida (1 neurona).\n",
        "* `nn.Sigmoid()`: convierte el valor a probabilidad en $[0,1]$.\n",
        "\n",
        "---\n",
        "\n",
        "## âš–ï¸ d. FunciÃ³n de pÃ©rdida y optimizador\n",
        "\n",
        "```python\n",
        "loss_fn = nn.BCELoss()\n",
        "opt = optim.SGD(model.parameters(), lr=0.1)\n",
        "```\n",
        "\n",
        "* `nn.BCELoss()`: **Binary Cross-Entropy**, mide diferencia entre probabilidades predichas y etiquetas (0/1).\n",
        "* `optim.SGD`: descenso de gradiente estocÃ¡stico.\n",
        "* `model.parameters()`: lista de pesos y bias que serÃ¡n ajustados.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ” e. Loop de entrenamiento\n",
        "\n",
        "```python\n",
        "for epoch in range(2000):\n",
        "    opt.zero_grad()             # 1. Resetear gradientes previos\n",
        "    y_pred = model(X_t)         # 2. Forward pass\n",
        "    loss = loss_fn(y_pred, y_t) # 3. Calcular pÃ©rdida\n",
        "    loss.backward()             # 4. Backpropagation (gradientes)\n",
        "    opt.step()                  # 5. Actualizar pesos\n",
        "```\n",
        "\n",
        "* Este loop se repite **2000 Ã©pocas**.\n",
        "* En cada iteraciÃ³n:\n",
        "\n",
        "  1. Limpia gradientes acumulados.\n",
        "  2. Pasa entradas por el modelo.\n",
        "  3. Calcula la pÃ©rdida.\n",
        "  4. Calcula gradientes de la pÃ©rdida respecto a los pesos.\n",
        "  5. Ajusta los pesos con SGD.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“ˆ f. Predicciones finales\n",
        "\n",
        "```python\n",
        "print(\"Predicciones XOR (PyTorch):\", model(X_t).detach().round().view(-1).numpy())\n",
        "```\n",
        "\n",
        "* `model(X_t)`: calcula probabilidades finales.\n",
        "* `.detach()`: saca el tensor del grafo de gradientes (solo para evaluaciÃ³n).\n",
        "* `.round()`: redondea probabilidades â†’ convierte en 0 o 1.\n",
        "* `.view(-1)`: convierte a vector 1D.\n",
        "* `.numpy()`: pasa a array de NumPy.\n",
        "\n",
        "ğŸ‘‰ El resultado esperado es:\n",
        "\n",
        "```\n",
        "Predicciones XOR (PyTorch): [0 1 1 0]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## âœ… ConclusiÃ³n\n",
        "\n",
        "* El perceptrÃ³n simple **no resuelve XOR** porque es lineal.\n",
        "* Con un **MLP (capa oculta + activaciÃ³n no lineal)**, PyTorch aprende la separaciÃ³n correctamente.\n",
        "* Este ejemplo muestra explÃ­citamente los pasos de entrenamiento que scikit-learn oculta en `.fit()`.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0fbcdeb",
      "metadata": {
        "id": "a0fbcdeb"
      },
      "source": [
        "### ğŸ”¹ TensorFlow / Keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48e022d8",
      "metadata": {
        "id": "48e022d8"
      },
      "outputs": [],
      "source": [
        "# ========== Keras / TensorFlow ==========\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import warnings\n",
        "\n",
        "# DefiniciÃ³n del modelo\n",
        "model = Sequential([\n",
        "    Dense(4, input_dim=2, activation=\"tanh\"),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "\n",
        "    model.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
        "    model.fit(X, y, epochs=2000, verbose=0)\n",
        "\n",
        "    print(\"Predicciones XOR (Keras):\", (model.predict(X) > 0.5).astype(int).ravel())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c57010b1-f677-4796-87d9-503ba38dc00d",
      "metadata": {
        "id": "c57010b1-f677-4796-87d9-503ba38dc00d"
      },
      "source": [
        "Vamos a **explicar este bloque paso a paso**. Es el equivalente en **Keras/TensorFlow** al que ya vimos en scikit-learn y PyTorch, para resolver el problema del **XOR**.\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ“¥ a. Importaciones\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import warnings\n",
        "```\n",
        "\n",
        "* **tensorflow** â†’ librerÃ­a de deep learning de Google.\n",
        "* **Sequential** â†’ modelo secuencial (capas apiladas una detrÃ¡s de otra).\n",
        "* **Dense** â†’ capa totalmente conectada (fully connected).\n",
        "* **warnings** â†’ solo se usa aquÃ­ para silenciar avisos innecesarios al entrenar.\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ§  b. DefiniciÃ³n del modelo\n",
        "\n",
        "```python\n",
        "model = Sequential([\n",
        "    Dense(4, input_dim=2, activation=\"tanh\"),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "```\n",
        "\n",
        "* **Sequential(\\[...])**: el modelo tendrÃ¡ capas en orden.\n",
        "* `Dense(4, input_dim=2, activation=\"tanh\")`:\n",
        "\n",
        "  * Capa oculta con **4 neuronas**.\n",
        "  * `input_dim=2` â†’ dos variables de entrada (los bits del XOR).\n",
        "  * `activation=\"tanh\"` â†’ funciÃ³n de activaciÃ³n no lineal (clave para resolver XOR).\n",
        "* `Dense(1, activation=\"sigmoid\")`:\n",
        "\n",
        "  * Capa de salida con **1 neurona**.\n",
        "  * `sigmoid` devuelve probabilidad entre 0 y 1.\n",
        "\n",
        "ğŸ‘‰ Arquitectura: **2 â†’ 4 â†’ 1** (igual que en PyTorch y scikit-learn).\n",
        "\n",
        "---\n",
        "\n",
        "### âš–ï¸ c. CompilaciÃ³n del modelo\n",
        "\n",
        "```python\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
        "```\n",
        "\n",
        "* `loss=\"binary_crossentropy\"` â†’ funciÃ³n de pÃ©rdida para clasificaciÃ³n binaria.\n",
        "* `optimizer=\"sgd\"` â†’ descenso de gradiente estocÃ¡stico.\n",
        "* `metrics=[\"accuracy\"]` â†’ medirÃ¡ la accuracy durante entrenamiento.\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ” d. Entrenamiento\n",
        "\n",
        "```python\n",
        "model.fit(X, y, epochs=2000, verbose=0)\n",
        "```\n",
        "\n",
        "* `X` y `y` â†’ dataset XOR.\n",
        "* `epochs=2000` â†’ nÃºmero de iteraciones completas sobre los datos.\n",
        "* `verbose=0` â†’ entrena en silencio (sin logs).\n",
        "\n",
        "ğŸ‘‰ Internamente, Keras hace: forward â†’ pÃ©rdida â†’ backpropagation â†’ update de pesos (como en PyTorch, pero escondido).\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ“ˆ e. Predicciones\n",
        "\n",
        "```python\n",
        "print(\"Predicciones XOR (Keras):\", (model.predict(X) > 0.5).astype(int).ravel())\n",
        "```\n",
        "\n",
        "* `model.predict(X)` â†’ devuelve probabilidades de clase (entre 0 y 1).\n",
        "* `> 0.5` â†’ umbral: si prob > 0.5 â†’ clase 1, si no â†’ clase 0.\n",
        "* `.astype(int)` â†’ convierte a enteros (0 o 1).\n",
        "* `.ravel()` â†’ aplana a vector 1D.\n",
        "\n",
        "ğŸ‘‰ Salida esperada:\n",
        "\n",
        "```\n",
        "Predicciones XOR (Keras): [0 1 1 0]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## âœ… ConclusiÃ³n\n",
        "\n",
        "* El **PerceptrÃ³n simple** falla en XOR.\n",
        "* En **Keras**, bastan 2 capas (`Dense(4,tanh)` y `Dense(1,sigmoid)`) para resolverlo.\n",
        "* Es equivalente a lo que hicimos en scikit-learn y PyTorch, pero con una **API de alto nivel** que facilita el entrenamiento en GPU/TPU y la integraciÃ³n en pipelines de producciÃ³n.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97e6edbf-767a-45a1-b668-af9187f963b5",
      "metadata": {
        "id": "97e6edbf-767a-45a1-b668-af9187f963b5"
      },
      "source": [
        "## 9. ğŸ“ŠComparativo visual: scikit-learn vs PyTorch vs Keras (MLP 2â†’4â†’1)  \n",
        "\n",
        "```text\n",
        "                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "Input (2 bits)  â”‚              â”‚\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Capa oculta â”‚  4 neuronas + tanh\n",
        "                â”‚   (4,tanh)   â”‚\n",
        "                â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                       â”‚\n",
        "                       â–¼\n",
        "               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "               â”‚   Capa salida â”‚  1 neurona + sigmoide\n",
        "               â”‚   (1,sigmoid) â”‚\n",
        "               â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                      â”‚\n",
        "                      â–¼\n",
        "                  PredicciÃ³n (0/1)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ”¹ scikit-learn\n",
        "\n",
        "```python\n",
        "MLPClassifier(hidden_layer_sizes=(4,),\n",
        "              activation=\"tanh\",\n",
        "              max_iter=3000)\n",
        "```\n",
        "\n",
        "* Arquitectura definida con un solo parÃ¡metro (`hidden_layer_sizes`).\n",
        "* El `.fit(X,y)` es **caja negra** â†’ hace forward, backprop y optimizaciÃ³n automÃ¡ticamente.\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ”¹ PyTorch\n",
        "\n",
        "```python\n",
        "nn.Sequential(\n",
        "    nn.Linear(2,4),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(4,1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "```\n",
        "\n",
        "* Arquitectura explÃ­cita (capas listadas).\n",
        "* El entrenamiento requiere **loop manual** con forward â†’ loss â†’ backward â†’ step.\n",
        "* Mucho control, Ãºtil para investigaciÃ³n y personalizaciÃ³n.\n",
        "\n",
        "---\n",
        "\n",
        "###ğŸ”¹ Keras / TensorFlow\n",
        "\n",
        "```python\n",
        "Sequential([\n",
        "    Dense(4, input_dim=2, activation=\"tanh\"),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "```\n",
        "\n",
        "* DefiniciÃ³n de capas parecida a PyTorch.\n",
        "* `model.fit(X,y,epochs=2000)` entrena automÃ¡ticamente (alto nivel).\n",
        "* Compatible con GPU/TPU y despliegue en producciÃ³n.\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a3bbcfb-87b1-423f-85b4-9429d41b2d44",
      "metadata": {
        "id": "2a3bbcfb-87b1-423f-85b4-9429d41b2d44"
      },
      "source": [
        "## ğŸ”‘ Diferencias Clave\n",
        "\n",
        "| Aspecto                 | Scikit-learn (`MLPClassifier`) | PyTorch (`nn.Module`)                  | TensorFlow / Keras (`Sequential` / `Functional`)   |\n",
        "| ----------------------- | ------------------------------ | -------------------------------------- | -------------------------------------------------- |\n",
        "| Facilidad de uso        | Muy alto (API simple)          | Medio (hay que definir loop)           | Muy alto (API de alto nivel con `fit`)             |\n",
        "| Flexibilidad            | Limitada (solo MLPs)           | Total (CNNs, RNNs, Transformers, etc.) | Total (CNNs, RNNs, Transformers, etc.)             |\n",
        "| Entrenamiento en GPU    | âŒ No                           | âœ… SÃ­                                   | âœ… SÃ­ (nativo en GPU/TPU)                           |\n",
        "| Rapidez para prototipos | âœ… Muy rÃ¡pido                   | âŒ MÃ¡s detallado                        | âœ… Muy rÃ¡pido (con `Sequential` o `fit`)            |\n",
        "| Nivel de control        | Bajo (caja negra)              | Muy alto (bucle manual posible)        | Intermedio (APIs de alto y bajo nivel disponibles) |\n",
        "| Comunidad/uso en DL     | AcadÃ©mico, ML clÃ¡sico          | InvestigaciÃ³n y prototipos avanzados   | ProducciÃ³n y despliegue a gran escala              |\n",
        "\n",
        "---\n",
        "\n",
        "ğŸ‘‰ AsÃ­, se ve claro que:\n",
        "\n",
        "* **Scikit-learn** â†’ ideal para prototipar MLPs en problemas tabulares.\n",
        "* **PyTorch** â†’ usado en investigaciÃ³n y prototipado avanzado.\n",
        "* **TensorFlow/Keras** â†’ usado masivamente en producciÃ³n y despliegue (GPU/TPU, cloud).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "974945e4",
      "metadata": {
        "id": "974945e4"
      },
      "source": [
        "---\n",
        "\n",
        "## 10. Preguntas de discusiÃ³n\n",
        "\n",
        "1. Â¿QuÃ© muestra el fracaso del perceptrÃ³n en XOR sobre la necesidad de MLPs?\n",
        "2. Â¿CuÃ¡ndo elegirÃ­as scikit-learn vs PyTorch vs TensorFlow?\n",
        "3. Â¿QuÃ© funciÃ³n de activaciÃ³n usarÃ­as para un problema real de churn?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "007a54a1-9575-4eca-bcc2-ae17b48b32af",
      "metadata": {
        "id": "007a54a1-9575-4eca-bcc2-ae17b48b32af"
      },
      "source": [
        "### 10.1. El fracaso del perceptrÃ³n en XOR\n",
        "\n",
        "El fracaso del perceptrÃ³n en el problema XOR demuestra que un modelo lineal simple no puede resolver problemas que no son linealmente separables. El perceptrÃ³n, que es un clasificador lineal, solo puede encontrar un hiperplano (una lÃ­nea en 2D) que divide los datos en dos clases. El problema XOR no puede ser resuelto con una sola lÃ­nea recta, ya que los puntos de datos de la misma clase (los `(0,0)` y `(1,1)`) no se encuentran en un lado del plano y los de la otra clase (`(0,1)` y `(1,0)`) en el otro. Para resolverlo, se necesita una frontera de decisiÃ³n no lineal.\n",
        "\n",
        "AquÃ­ es donde entran los **MLPs (Multilayer Perceptrons)**. Al usar mÃºltiples capas de neuronas (capas ocultas), los MLPs pueden combinar las salidas de las neuronas de la capa anterior de manera no lineal, creando asÃ­ fronteras de decisiÃ³n mucho mÃ¡s complejas. La capacidad de crear una representaciÃ³n no lineal de los datos es la razÃ³n por la que los MLPs pueden resolver el problema XOR.\n",
        "\n",
        "### 10.2. ElecciÃ³n de scikit-learn vs PyTorch vs TensorFlow\n",
        "\n",
        "La elecciÃ³n entre estas bibliotecas depende del objetivo y el nivel de control que necesites.\n",
        "\n",
        "  * **Scikit-learn** es la mejor opciÃ³n para la mayorÃ­a de los problemas de **machine learning tradicional**. Es una biblioteca de alto nivel, fÃ¡cil de usar y muy eficiente para tareas como regresiÃ³n, clasificaciÃ³n, clustering y reducciÃ³n de dimensionalidad. Es ideal para prototipos rÃ¡pidos y para modelos que no requieren el poder de las redes neuronales profundas. Ãšsala cuando necesites un modelo de `Random Forest`, `SVM`, `RegresiÃ³n Lineal`, etc.\n",
        "\n",
        "  * **PyTorch** y **TensorFlow** son frameworks de **deep learning**. Se eligen cuando se necesita construir y entrenar redes neuronales complejas. La principal diferencia entre ellos radica en su filosofÃ­a de diseÃ±o:\n",
        "\n",
        "      * **PyTorch** es conocido por su **naturaleza mÃ¡s `Pythonic` y su curva de aprendizaje mÃ¡s suave**. Utiliza grafos de computaciÃ³n dinÃ¡micos, lo que lo hace mÃ¡s flexible y fÃ¡cil de depurar. Es muy popular en el Ã¡mbito de la investigaciÃ³n y en proyectos donde la experimentaciÃ³n rÃ¡pida es clave.\n",
        "      * **TensorFlow** (especialmente con su API `Keras`) es un framework robusto y de alto rendimiento. Es conocido por su **fuerte soporte para la producciÃ³n y el despliegue a gran escala**. Ofrece una amplia gama de herramientas para monitoreo (`TensorBoard`), optimizaciÃ³n y despliegue en diferentes plataformas (`TensorFlow Lite`, `TensorFlow.js`). Es una excelente opciÃ³n para proyectos empresariales que requieren escalabilidad.\n",
        "\n",
        "\n",
        "\n",
        "-----\n",
        "\n",
        "\n",
        "\n",
        "### 10.3. FunciÃ³n de activaciÃ³n para un problema de churn\n",
        "\n",
        "Para un problema de **churn (abandono de clientes)**, que es un problema de clasificaciÃ³n binaria (el cliente abandona o no abandona), la funciÃ³n de activaciÃ³n ideal para la **capa de salida** es la funciÃ³n **Sigmoide**.\n",
        "\n",
        "La funciÃ³n Sigmoide comprime cualquier valor real en un rango entre 0 y 1. Esto es perfecto para la clasificaciÃ³n binaria, ya que la salida de la neurona se puede interpretar directamente como la **probabilidad de que el cliente abandone**. Por ejemplo, una salida de `0.85` podrÃ­a significar que hay un 85% de probabilidad de que el cliente abandone.\n",
        "\n",
        "Para las **capas ocultas**, las funciones de activaciÃ³n mÃ¡s comunes y efectivas son **ReLU (Rectified Linear Unit)** o sus variantes (`Leaky ReLU`). Estas funciones ayudan a mitigar el problema del **gradiente desvaneciente** (vanishing gradient problem), permitiendo que la red aprenda de manera mÃ¡s eficiente en capas profundas. Por lo tanto, una arquitectura comÃºn serÃ­a usar **ReLU** en las capas ocultas y **Sigmoide** en la capa de salida."
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}