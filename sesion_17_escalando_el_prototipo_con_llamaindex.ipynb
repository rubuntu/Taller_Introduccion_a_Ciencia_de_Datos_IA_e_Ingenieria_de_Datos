{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rubuntu/Taller_Introduccion_a_Ciencia_de_Datos_IA_e_Ingenieria_de_Datos/blob/main/sesion_17_escalando_el_prototipo_con_llamaindex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98090d85",
      "metadata": {
        "id": "98090d85"
      },
      "source": [
        "# üìò Sesi√≥n 17 - Escalando el prototipo con LlamaIndex + Modelos Abiertos Cuantizados\n",
        "\n",
        "## üéØ Objetivos\n",
        "\n",
        "- Introducir **LlamaIndex** como framework para construir RAGs m√°s robustos.  \n",
        "- Aprender c√≥mo indexar datasets completos y ejecutar consultas.  \n",
        "- Explorar el uso de **modelos de pesos abiertos cuantizados**.\n",
        "- Evaluar la recuperaci√≥n con m√©tricas b√°sicas y probar calidad de respuestas.  \n",
        "\n",
        "---\n",
        "\n",
        "## 1. üöÄ Instalaci√≥n de dependencias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70bf4b27",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "70bf4b27"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Instalaci√≥n de librer√≠as necesarias en Colab\n",
        "!pip install transformers accelerate bitsandbytes\n",
        "!pip install llama-index-core llama-index-embeddings-huggingface llama-index-llms-huggingface\n",
        "!pip install wikipedia-api\n",
        "!pip install chromadb==0.4.24\n",
        "!pip install llama-index-vector-stores-chroma\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51795a62",
      "metadata": {
        "id": "51795a62"
      },
      "source": [
        "Antes de empezar, necesitamos instalar las librer√≠as clave:\n",
        "\n",
        "* `transformers`: para cargar y usar el modelo Mistral.\n",
        "* `bitsandbytes`: nos permite usar cuantizaci√≥n en 4 bits y ahorrar memoria en GPU.\n",
        "* `accelerate`: maneja la asignaci√≥n autom√°tica de dispositivos (CPU/GPU).\n",
        "* `llama-index`: framework para crear RAGs (Retrieval-Augmented Generation).\n",
        "* `chromadb`: motor vectorial que servir√° como base de datos sem√°ntica.\n",
        "* `wikipedia-api`: descargar√° textos de Wikipedia en espa√±ol.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. üì• Carga del modelo Mistral"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "628991da",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "628991da"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "fourbit_models = [\n",
        "    \"unsloth/mistral-7b-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
        "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
        "    \"unsloth/llama-2-13b-bnb-4bit\",\n",
        "    \"unsloth/codellama-34b-bnb-4bit\",\n",
        "    \"unsloth/tinyllama-bnb-4bit\",\n",
        "    \"unsloth/gemma-7b-bnb-4bit\",\n",
        "    \"unsloth/gemma-2b-bnb-4bit\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "# üì• Descargamos el modelo Mistral desde Hugging Face\n",
        "model_id = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\"\n",
        "\n",
        "# Tokenizador\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Cargar modelo en GPU\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02723fa9",
      "metadata": {
        "id": "02723fa9"
      },
      "source": [
        "Aqu√≠ descargamos el modelo **Mistral-7B-Instruct-v0.3** de Hugging Face y lo cargamos en memoria.\n",
        "\n",
        "* `AutoTokenizer` prepara el texto para que el modelo lo entienda.\n",
        "* `device_map=\"auto\"` hace que se cargue en GPU autom√°ticamente si est√° disponible (ej. T4 en Colab).\n",
        "\n",
        "---\n",
        "\n",
        "## 3. üîå Conexi√≥n con LlamaIndex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ec4284a",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "0ec4284a"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.core import Settings\n",
        "\n",
        "# Crear LLM\n",
        "llm = HuggingFaceLLM(model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Configurar como LLM por defecto\n",
        "Settings.llm = llm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe819e00",
      "metadata": {
        "id": "fe819e00"
      },
      "source": [
        "* `HuggingFaceLLM` envuelve el modelo de Hugging Face para que LlamaIndex pueda usarlo.\n",
        "* `Settings.llm` = llm indica que todas las operaciones de LlamaIndex usar√°n Mistral como motor LLM.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. ‚úÖ Prueba r√°pida del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "693ed7ee",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "693ed7ee"
      },
      "outputs": [],
      "source": [
        "response = llm.complete(\"Cuentame de Paraguay.\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8d68624",
      "metadata": {
        "id": "e8d68624"
      },
      "source": [
        "Aqu√≠ hacemos una prueba inicial para verificar que Mistral est√° funcionando y que responde en **espa√±ol**.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. üìö Descarga de documentos de Wikipedia (Paraguay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c803295a",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "c803295a"
      },
      "outputs": [],
      "source": [
        "import wikipediaapi\n",
        "\n",
        "# Configuraci√≥n de Wikipedia API\n",
        "wiki_wiki = wikipediaapi.Wikipedia(\n",
        "    language='es',\n",
        "    user_agent='MiProyectoRAG/1.0 (https://github.com/rubuntu)'\n",
        ")\n",
        "\n",
        "# Lista de p√°ginas a descargar\n",
        "paginas = [\n",
        "    \"Paraguay\",\n",
        "    \"Geograf√≠a de Paraguay\",\n",
        "    \"Econom√≠a de Paraguay\",\n",
        "    \"Cultura de Paraguay\",\n",
        "    \"Historia de Paraguay\",\n",
        "    \"Asunci√≥n\",\n",
        "    \"Guerra de la Triple Alianza\",\n",
        "    \"Guerra del Chaco\",\n",
        "]\n",
        "\n",
        "# Descargar y almacenar textos\n",
        "docs_texts = []\n",
        "for titulo in paginas:\n",
        "    page = wiki_wiki.page(titulo)\n",
        "    if page.exists():\n",
        "        docs_texts.append(page.text)\n",
        "        print(f\"Cargado: {titulo}\")\n",
        "print(f\"Total documentos cargados: {len(docs_texts)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e592e7c",
      "metadata": {
        "id": "2e592e7c"
      },
      "source": [
        "Aqu√≠ usamos la API de Wikipedia para descargar m√∫ltiples art√≠culos relacionados con **Paraguay**.\n",
        "\n",
        "* Se definen las p√°ginas de inter√©s (historia, geograf√≠a, guerras, cultura, etc.).\n",
        "* Cada art√≠culo se descarga en espa√±ol y se guarda en `docs_texts`.\n",
        "* As√≠ construimos nuestro **corpus de conocimiento**.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. üìÇ Indexaci√≥n de documentos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da853645",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "da853645"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Document, VectorStoreIndex, Settings\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "import chromadb\n",
        "\n",
        "# 1. Configurar LLM\n",
        "llm = HuggingFaceLLM(model=model, tokenizer=tokenizer)\n",
        "Settings.llm = llm\n",
        "\n",
        "# 2. Configurar embeddings\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"intfloat/multilingual-e5-base\")\n",
        "Settings.embed_model = embed_model\n",
        "\n",
        "# 3. Documentos\n",
        "documents = [Document(text=doc) for doc in docs_texts]\n",
        "\n",
        "# 4. Dividir en nodos\n",
        "splitter = SentenceSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "nodes = splitter.get_nodes_from_documents(documents)\n",
        "\n",
        "# 5. Cliente Chroma\n",
        "chroma_client = chromadb.EphemeralClient()\n",
        "# OR: chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "\n",
        "# 6. Create collection and vector store\n",
        "chroma_collection = chroma_client.get_or_create_collection(name=\"paraguay\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "\n",
        "# 7. Crear √≠ndice\n",
        "index = VectorStoreIndex(nodes, vector_store=vector_store)\n",
        "\n",
        "print(\"‚úÖ √çndice creado con colecci√≥n 'paraguay'\")\n",
        "\n",
        "# 8. Consulta de prueba\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"¬øCu√°l es la capital de Paraguay?\")\n",
        "print(\"üîé Respuesta:\", response)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "859f8c94",
      "metadata": {
        "id": "859f8c94"
      },
      "source": [
        "* Settings.llm = llm ‚Üí define que todas las consultas usen Mistral como generador.\n",
        "* Settings.embed_model = embed_model ‚Üí define que los documentos se indexen con embeddings de Hugging Face en lugar de OpenAI.\n",
        "* Elegimos intfloat/multilingual-e5-base, que funciona muy bien en espa√±ol.\n",
        "* Transformamos cada texto en un `Document` de LlamaIndex.\n",
        "* Inicializamos Chroma como nuestra base vectorial.\n",
        "* Construimos un **√≠ndice sem√°ntico** para consultas r√°pidas y contextuales.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. üîé Consultas al √≠ndice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99d8a812",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "99d8a812"
      },
      "outputs": [],
      "source": [
        "system_prompt = \"\"\"Eres un asistente experto en historia, geograf√≠a y cultura de Paraguay.\n",
        "Debes responder SIEMPRE en espa√±ol, de forma clara y concisa.\n",
        "Si no sabes la respuesta, admite que no lo sabes.\n",
        "\"\"\"\n",
        "\n",
        "query_engine = index.as_query_engine(\n",
        "    system_prompt=system_prompt\n",
        ")\n",
        "\n",
        "# Ejemplo de consultas\n",
        "preguntas = [\n",
        "    \"¬øCu√°les son los principales r√≠os de Paraguay?\",\n",
        "    \"¬øQu√© importancia tiene la ciudad de Asunci√≥n?\",\n",
        "    \"¬øQu√© pa√≠ses participaron en la Guerra de la Triple Alianza?\",\n",
        "    \"Resume la econom√≠a de Paraguay en pocas frases\"\n",
        "]\n",
        "\n",
        "for q in preguntas:\n",
        "    response = query_engine.query(q)\n",
        "    print(\"Pregunta:\", q)\n",
        "    print(\"Respuesta:\", response, \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8248d947",
      "metadata": {
        "id": "8248d947"
      },
      "source": [
        "* `system_prompt` define las instrucciones iniciales para el modelo. Aqu√≠ le decimos expl√≠citamente: ‚ÄúResponde siempre en espa√±ol‚Äù. Esto asegura consistencia incluso si alguna pregunta llega o se genera una respuesta en ingl√©s\n",
        "* `index.as_query_engine()` crea un motor de consulta listo para preguntas en lenguaje natural.\n",
        "* Probamos con preguntas t√≠picas sobre geograf√≠a, historia y econom√≠a de Paraguay.\n",
        "* El modelo recupera fragmentos relevantes y responde en espa√±ol.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. üß™ Evaluaciones b√°sicas\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preparar_queries_multi_chunk(index, queries_texto):\n",
        "    queries_final = []\n",
        "    for q in queries_texto:\n",
        "        expected_ids = []\n",
        "        for ref in q[\"reference\"]:\n",
        "            for node in index.docstore.docs.values():\n",
        "                if ref.lower() in node.text.lower():\n",
        "                    expected_ids.append(node.node_id)\n",
        "        expected_ids = list(set(expected_ids))  # quitar duplicados\n",
        "        queries_final.append({\n",
        "            \"query\": q[\"query\"],\n",
        "            \"expected_ids\": expected_ids,\n",
        "            \"reference\": q[\"reference\"]\n",
        "        })\n",
        "    return queries_final\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Dataset en texto\n",
        "# ------------------------------\n",
        "queries_texto = [\n",
        "    {\"query\": \"¬øCu√°l es la capital de Paraguay?\", \"reference\": [\"Asunci√≥n\"]},\n",
        "    {\"query\": \"¬øQu√© guerra enfrent√≥ Paraguay contra Bolivia?\", \"reference\": [\"Guerra del Chaco\"]},\n",
        "]\n",
        "\n",
        "queries = preparar_queries_multi_chunk(index, queries_texto)\n",
        "print(\"Queries preparadas (multi-chunk):\")\n",
        "for q in queries:\n",
        "    print(q)\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Retriever con top_k alto\n",
        "# ------------------------------\n",
        "from llama_index.core.evaluation import RetrieverEvaluator\n",
        "\n",
        "retriever = index.as_retriever(similarity_top_k=200)\n",
        "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
        "    [\"hit_rate\", \"mrr\"], retriever=retriever\n",
        ")\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Evaluaci√≥n sin reranker\n",
        "# ------------------------------\n",
        "results = []\n",
        "for q in queries:\n",
        "    res = retriever_evaluator.evaluate(\n",
        "        query=q[\"query\"],\n",
        "        expected_ids=q[\"expected_ids\"]\n",
        "    )\n",
        "    results.append(res)\n",
        "\n",
        "print(\"\\n=== Resultados sin reranker ===\")\n",
        "for r in results:\n",
        "    print(\"Query:\", r.query)\n",
        "    print(\"Expected IDs:\", len(r.expected_ids))\n",
        "    print(\"Retrieved IDs (top 5):\", r.retrieved_ids[:5])\n",
        "    print(\"Metrics:\", {k: v.score for k, v in r.metric_dict.items()})\n",
        "    print(\"‚Äî\")\n",
        "\n",
        "# M√©tricas agregadas\n",
        "hit_rate_avg = sum(r.metric_dict[\"hit_rate\"].score for r in results) / len(results)\n",
        "mrr_avg = sum(r.metric_dict[\"mrr\"].score for r in results) / len(results)\n",
        "\n",
        "print(f\"\\nHit Rate promedio: {hit_rate_avg:.2f}\")\n",
        "print(f\"MRR promedio: {mrr_avg:.2f}\")\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Recall@k\n",
        "# ------------------------------\n",
        "def compute_recall_at_k(results, k=3):\n",
        "    recalls = []\n",
        "    for r in results:\n",
        "        expected = set(r.expected_ids)\n",
        "        retrieved_topk = set(r.retrieved_ids[:k])\n",
        "        intersect = expected.intersection(retrieved_topk)\n",
        "        recall = len(intersect) / len(expected) if expected else 0.0\n",
        "        recalls.append(recall)\n",
        "    return sum(recalls) / len(recalls)\n",
        "\n",
        "for k in [3, 5, 10, 20]:\n",
        "    recall_at_k = compute_recall_at_k(results, k=k)\n",
        "    print(f\"Recall@{k} promedio: {recall_at_k:.2f}\")\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# A√±adir Re-ranker\n",
        "# ------------------------------\n",
        "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
        "\n",
        "reranker = SentenceTransformerRerank(\n",
        "    model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\",  # r√°pido y liviano\n",
        "    top_n=10\n",
        ")\n",
        "\n",
        "from llama_index.core import QueryBundle\n",
        "\n",
        "print(\"\\n=== Resultados con reranker ===\")\n",
        "for q in queries:\n",
        "    # Recuperar candidatos\n",
        "    retrieved_nodes = retriever.retrieve(q[\"query\"])\n",
        "    # Re-rankear (ahora con QueryBundle)\n",
        "    query_bundle = QueryBundle(q[\"query\"])\n",
        "    reranked_nodes = reranker.postprocess_nodes(retrieved_nodes, query_bundle)\n",
        "    reranked_ids = [n.node_id for n in reranked_nodes]\n",
        "\n",
        "    expected = set(q[\"expected_ids\"])\n",
        "    retrieved_topk = set(reranked_ids[:10])\n",
        "    intersect = expected.intersection(retrieved_topk)\n",
        "    recall = len(intersect) / len(expected) if expected else 0.0\n",
        "\n",
        "    print(\"Query:\", q[\"query\"])\n",
        "    print(\"Top 5 IDs re-rankeados:\", reranked_ids[:5])\n",
        "    print(\"Recall@10 (con reranker):\", round(recall, 2))\n",
        "    print(\"‚Äî\")\n",
        "\n"
      ],
      "metadata": {
        "id": "HRYZvm--41gZ"
      },
      "id": "HRYZvm--41gZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6fbfcde4",
      "metadata": {
        "id": "6fbfcde4"
      },
      "source": [
        "* Definimos un `retriever` que devuelve los 3 documentos m√°s similares.\n",
        "* Usamos `RetrieverEvaluator` para obtener metricas.\n",
        "* Creamos un dataset de prueba con preguntas y respuestas esperadas.\n",
        "* As√≠ validamos si el sistema recupera informaci√≥n relevante.\n",
        "\n",
        "---\n",
        "\n",
        "### Calidad de respuesta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d99351ea",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "d99351ea"
      },
      "outputs": [],
      "source": [
        "query = \"Resume brevemente la historia de Paraguay.\"\n",
        "response = query_engine.query(query)\n",
        "print(\"Respuesta generada:\", response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "371324dc",
      "metadata": {
        "id": "371324dc"
      },
      "source": [
        "Aqu√≠ evaluamos la **calidad de la respuesta generada**.\n",
        "El modelo debe devolver un resumen coherente y en espa√±ol sobre la historia de Paraguay.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e215230b",
      "metadata": {
        "id": "e215230b"
      },
      "source": [
        "## 8. ‚ùì Preguntas de discusi√≥n\n",
        "\n",
        "### 1. ¬øQu√© ventajas aporta LlamaIndex frente a implementar embeddings + vector DB manualmente?\n",
        "\n",
        "- **Abstracci√≥n de complejidad**: en lugar de escribir decenas de l√≠neas de c√≥digo para manejar embeddings, base vectorial, consulta y recuperaci√≥n, LlamaIndex lo encapsula en APIs de alto nivel como `VectorStoreIndex` y `QueryEngine`.  \n",
        "- **Integraci√≥n con m√∫ltiples backends**: soporta distintos motores de embeddings, bases vectoriales (Chroma, Weaviate, Pinecone, FAISS, etc.) y LLMs, sin tener que reescribir l√≥gica de integraci√≥n.  \n",
        "- **Extensibilidad**: provee componentes modulares (retrievers, evaluadores, response synthesizers) que se pueden personalizar.  \n",
        "- **Productividad**: permite centrarse en la l√≥gica de negocio y evaluaci√≥n de resultados, en lugar de la infraestructura.  \n",
        "- **Evaluaci√≥n integrada**: ya incorpora utilidades para medir \"hit_rate\", \"mrr\".  \n",
        "\n",
        "En resumen: **menos c√≥digo, m√°s rapidez y flexibilidad**.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. ¬øC√≥mo se podr√≠a evaluar la calidad de las respuestas en un RAG real?\n",
        "\n",
        "La evaluaci√≥n en un sistema RAG (Retrieval-Augmented Generation) debe considerar dos niveles:\n",
        "\n",
        "1. **Calidad de recuperaci√≥n**:  \n",
        "   - M√©tricas cl√°sicas de IR (Information Retrieval):  \n",
        "     - *Recall@k* ‚Üí ¬øcu√°ntas de las respuestas correctas se encuentran en el top-k recuperado?  \n",
        "     - *Precision@k* ‚Üí ¬øqu√© proporci√≥n de los documentos recuperados son relevantes?  \n",
        "     - *MRR* (Mean Reciprocal Rank) ‚Üí mide la posici√≥n del primer documento relevante.  \n",
        "   - Estas m√©tricas permiten cuantificar si el sistema realmente trae el contexto adecuado.  \n",
        "\n",
        "2. **Calidad de la respuesta generada por el LLM**:  \n",
        "   - **Autom√°tica**: comparar con respuestas de referencia (*exact match*, *BLEU*, *ROUGE*, *BERTScore*).  \n",
        "   - **LLM-as-a-judge**: usar otro LLM para evaluar criterios como coherencia, cobertura y exactitud.  \n",
        "   - **Humana**: encuestas de usuarios, validaci√≥n por expertos del dominio.  \n",
        "   - **Evaluaci√≥n factual**: detectar alucinaciones y verificar citas contra fuentes.  \n",
        "\n",
        "En la pr√°ctica, se combinan m√©tricas autom√°ticas + validaci√≥n humana para un panorama m√°s realista.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. ¬øEn qu√© casos NO usar√≠as RAG?\n",
        "\n",
        "Aunque poderoso, **RAG no siempre es la mejor soluci√≥n**. Ejemplos de casos donde no conviene:\n",
        "\n",
        "- **Datos muy estructurados y peque√±os**: si la informaci√≥n ya est√° en una base de datos relacional bien dise√±ada, una simple consulta SQL es m√°s eficiente y precisa.  \n",
        "- **Consultas determin√≠sticas**: cuando las respuestas deben ser exactas, como en sistemas bancarios o m√©dicos cr√≠ticos, donde no se puede tolerar ‚Äúalucinaciones‚Äù del modelo.  \n",
        "- **Dominio cerrado con vocabulario limitado**: si el conocimiento se puede representar en reglas o tablas, un sistema simb√≥lico puede ser m√°s adecuado.  \n",
        "- **Restricciones de costo o latencia**: RAG introduce pasos adicionales (generaci√≥n de embeddings, b√∫squeda en vector DB, inferencia de LLM), lo que puede aumentar el tiempo de respuesta y consumo de recursos.  \n",
        "- **Confidencialidad extrema**: cuando no se puede permitir almacenar o indexar informaci√≥n sensible, ni siquiera en bases vectoriales privadas.  \n",
        "\n",
        "üëâ En resumen: **RAG es ideal cuando tenemos grandes corpus no estructurados y queremos respuestas en lenguaje natural**, pero no siempre es la herramienta m√°s eficiente o segura para todo escenario.\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}