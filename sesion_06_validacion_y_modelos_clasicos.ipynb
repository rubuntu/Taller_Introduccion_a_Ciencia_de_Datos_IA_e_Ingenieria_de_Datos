{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rubuntu/Taller_Introduccion_a_Ciencia_de_Datos_IA_e_Ingenieria_de_Datos/blob/main/sesion_06_validacion_y_modelos_clasicos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5d6654b",
      "metadata": {
        "id": "c5d6654b"
      },
      "source": [
        "# Sesion 06 ‚Äî Validaci√≥n y Modelos Cl√°sicos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71182087",
      "metadata": {
        "id": "71182087"
      },
      "source": [
        "# Parte A - Validaci√≥n cruzada y m√©tricas de performance\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48635d6f-74ec-41ef-9dca-4ade9e466f30",
      "metadata": {
        "id": "48635d6f-74ec-41ef-9dca-4ade9e466f30"
      },
      "source": [
        "## Objetivos\n",
        "- Entender la importancia de la validaci√≥n cruzada.\n",
        "- Conocer m√©tricas comunes: Accuracy, Precision, Recall, F1, ROC-AUC.\n",
        "- Evaluar un modelo de churn de forma rigurosa.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Validaci√≥n Cruzada\n",
        "Divide los datos en **k-folds**.  \n",
        "Cada fold se usa una vez para test y el resto para train.  \n",
        "Ventaja: evita depender de una sola partici√≥n.\n",
        "\n",
        "*Referencia:* https://www.datacamp.com/es/tutorial/k-fold-cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c756668-622e-442f-98fd-49dcbf7f9cfd",
      "metadata": {
        "id": "8c756668-622e-442f-98fd-49dcbf7f9cfd"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "import numpy as np\n",
        "\n",
        "# Par√°metros\n",
        "n_samples = 25\n",
        "n_splits = 5\n",
        "indices = np.arange(n_samples)\n",
        "\n",
        "# Colores para las 5 carpetas\n",
        "colors = plt.cm.tab10(np.linspace(0, 1, n_splits))\n",
        "\n",
        "# Crear figura\n",
        "fig, ax = plt.subplots(figsize=(10, 2))\n",
        "bars = ax.bar(indices, np.ones_like(indices), color=\"lightgray\", edgecolor=\"black\")\n",
        "\n",
        "ax.set_xticks(indices)\n",
        "ax.set_yticks([])\n",
        "ax.set_title(\"Validaci√≥n Cruzada de 5 Carpetas\", fontsize=14)\n",
        "ax.set_xlabel(\"Observaciones\")\n",
        "\n",
        "def update(frame):\n",
        "    for bar in bars:\n",
        "        bar.set_color(\"lightgray\")\n",
        "    test_idx = np.array_split(indices, n_splits)[frame]\n",
        "    for i in range(len(indices)):\n",
        "        if i in test_idx:\n",
        "            bars[i].set_color(colors[frame])  # Test set\n",
        "        else:\n",
        "            bars[i].set_color(\"lightblue\")   # Train set\n",
        "    ax.set_title(f\"Validaci√≥n Cruzada - Fold {frame+1}\", fontsize=14)\n",
        "    return bars\n",
        "\n",
        "ani = animation.FuncAnimation(fig, update, frames=n_splits, blit=False, repeat=True)\n",
        "\n",
        "# Guardar como GIF sin mostrar\n",
        "gif_path = \"cross_validation_5fold.gif\"\n",
        "ani.save(gif_path, writer=\"pillow\", fps=1)\n",
        "plt.close(fig)  # Cierra la figura para que no se renderice en Jupyter\n",
        "\n",
        "#print(f\"Animaci√≥n guardada en: {gif_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b670656f-4a94-43e8-ad5c-82f88baa9053",
      "metadata": {
        "id": "b670656f-4a94-43e8-ad5c-82f88baa9053"
      },
      "source": [
        "![Animaci√≥n de Validaci√≥n Cruzada](cross_validation_5fold.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "111f4368-d179-43a6-bc29-bdb10b343f38",
      "metadata": {
        "id": "111f4368-d179-43a6-bc29-bdb10b343f38"
      },
      "source": [
        "## 2. Ejemplo con Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01c78219",
      "metadata": {
        "id": "01c78219"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# 1. Cargar dataset\n",
        "url = \"https://raw.githubusercontent.com/Geo-y20/Telco-Customer-Churn-/refs/heads/main/Telco%20Customer%20Churn.csv\"\n",
        "df = pd.read_csv(url)\n",
        "df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"], errors=\"coerce\")\n",
        "df[\"TotalCharges\"] = df[\"TotalCharges\"].fillna(df[\"MonthlyCharges\"])\n",
        "display(df.head())\n",
        "\n",
        "# 2. Eliminar columnas irrelevantes\n",
        "df = df.drop(\"customerID\", axis=1)\n",
        "\n",
        "# 3. One-Hot Encoding con pandas para variables categ√≥ricas\n",
        "df = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# 4. Separar variables y target (Definir X e y)\n",
        "X = df.drop(\"Churn_Yes\", axis=1)  # porque get_dummies crea Churn_No y Churn_Yes\n",
        "y = df[\"Churn_Yes\"]\n",
        "\n",
        "# 5. Escalar datos\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# 6. Definir modelo\n",
        "clf = LogisticRegression(max_iter=5000)\n",
        "\n",
        "# 7. Validaci√≥n cruzada\n",
        "scores = cross_val_score(clf, X, y, cv=5, scoring=\"accuracy\")\n",
        "print(\"Accuracy promedio:\", np.mean(scores))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f73d5182",
      "metadata": {
        "id": "f73d5182"
      },
      "source": [
        "---\n",
        "\n",
        "## 3. M√©tricas de Clasificaci√≥n\n",
        "\n",
        "* **Accuracy**: % de predicciones correctas.\n",
        "* **Precision**: proporci√≥n de positivos predichos que son correctos.\n",
        "* **Recall**: proporci√≥n de verdaderos positivos detectados.\n",
        "* **F1**: balance entre precision y recall.\n",
        "* **ROC-AUC**: mide discriminaci√≥n entre clases.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01d622c7",
      "metadata": {
        "id": "01d622c7"
      },
      "source": [
        "---\n",
        "\n",
        "## 4. Curva ROC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e6696d9",
      "metadata": {
        "id": "1e6696d9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "RocCurveDisplay.from_estimator(clf, X_test, y_test)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5d380a2-40f4-4a8f-bd2f-1bbc7b876624",
      "metadata": {
        "id": "d5d380a2-40f4-4a8f-bd2f-1bbc7b876624"
      },
      "source": [
        "---\n",
        "\n",
        "## 5. Ejercicio Guiado\n",
        "\n",
        "1. Aplicar validaci√≥n cruzada al modelo de churn.\n",
        "2. Calcular precision, recall y f1-score.\n",
        "3. Graficar la curva ROC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "970e8fae-6a8b-4668-8bc9-9c86213b6e01",
      "metadata": {
        "id": "970e8fae-6a8b-4668-8bc9-9c86213b6e01"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, cross_validate, cross_val_predict\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
        ")\n",
        "\n",
        "# 1) Cargar dataset\n",
        "url = \"https://raw.githubusercontent.com/Geo-y20/Telco-Customer-Churn-/refs/heads/main/Telco%20Customer%20Churn.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Limpieza m√≠nima recomendada por este dataset\n",
        "df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"], errors=\"coerce\")\n",
        "df[\"TotalCharges\"] = df[\"TotalCharges\"].fillna(df[\"MonthlyCharges\"])\n",
        "\n",
        "# 2) Eliminar columnas irrelevantes\n",
        "df = df.drop(columns=[\"customerID\"])\n",
        "\n",
        "# 3) One-Hot Encoding\n",
        "df = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# 4) X, y\n",
        "X = df.drop(columns=[\"Churn_Yes\"])\n",
        "y = df[\"Churn_Yes\"]\n",
        "\n",
        "# 5) Modelo en Pipeline para evitar fuga de informaci√≥n del escalado\n",
        "pipe = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"clf\", LogisticRegression(max_iter=1000))\n",
        "])\n",
        "\n",
        "# 6) Definir validaci√≥n cruzada y m√©tricas\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scoring = {\n",
        "    \"accuracy\": \"accuracy\",\n",
        "    \"precision\": \"precision\",\n",
        "    \"recall\": \"recall\",\n",
        "    \"f1\": \"f1\",\n",
        "    \"roc_auc\": \"roc_auc\"\n",
        "}\n",
        "\n",
        "# 7) cross_validate para obtener m√©tricas por fold y promedio\n",
        "cv_results = cross_validate(pipe, X, y, cv=cv, scoring=scoring, return_train_score=False)\n",
        "\n",
        "print(\"=== M√©tricas promedio (5-fold CV) ===\")\n",
        "for m in scoring.keys():\n",
        "    vals = cv_results[f\"test_{m}\"]\n",
        "    print(f\"{m.capitalize():<9}: {vals.mean():.4f}  (¬± {vals.std():.4f})\")\n",
        "\n",
        "# 8) Predicciones out-of-fold para reportes agregados (confusion matrix, ROC, etc.)\n",
        "#    cross_val_predict entrena en cada fold y predice sobre el holdout de ese fold.\n",
        "y_pred_oof = cross_val_predict(pipe, X, y, cv=cv, method=\"predict\")\n",
        "y_prob_oof = cross_val_predict(pipe, X, y, cv=cv, method=\"predict_proba\")[:, 1]\n",
        "\n",
        "# 9) Reportes agregados con OOF\n",
        "print(\"\\n=== Reporte de clasificaci√≥n (OOF) ===\")\n",
        "print(classification_report(y, y_pred_oof, digits=4))\n",
        "\n",
        "cm = confusion_matrix(y, y_pred_oof)\n",
        "print(\"=== Matriz de confusi√≥n (OOF) ===\")\n",
        "print(cm)\n",
        "\n",
        "acc  = accuracy_score(y, y_pred_oof)\n",
        "prec = precision_score(y, y_pred_oof)\n",
        "rec  = recall_score(y, y_pred_oof)\n",
        "f1   = f1_score(y, y_pred_oof)\n",
        "rocA = roc_auc_score(y, y_prob_oof)\n",
        "\n",
        "print(\"\\n=== M√©tricas agregadas (OOF) ===\")\n",
        "print(f\"Accuracy : {acc:.4f}\")\n",
        "print(f\"Precision: {prec:.4f}\")\n",
        "print(f\"Recall   : {rec:.4f}\")\n",
        "print(f\"F1       : {f1:.4f}\")\n",
        "print(f\"ROC-AUC  : {rocA:.4f}\")\n",
        "\n",
        "# 10) Curva ROC (OOF)\n",
        "fpr, tpr, _ = roc_curve(y, y_prob_oof)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {rocA:.3f})\")\n",
        "plt.plot([0, 1], [0, 1], linestyle=\"--\", label=\"Azar\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"Curva ROC (validaci√≥n cruzada OOF)\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7131df26-adec-43c7-99e6-248433a92613",
      "metadata": {
        "id": "7131df26-adec-43c7-99e6-248433a92613"
      },
      "source": [
        "---\n",
        "\n",
        "## 6. Preguntas de Discusi√≥n\n",
        "\n",
        "### 6.1. ¬øPor qu√© conviene usar validaci√≥n cruzada?\n",
        "\n",
        "* **Idea central:** La validaci√≥n cruzada permite evaluar el desempe√±o de un modelo no en un √∫nico ‚Äúcorte‚Äù de train/test, sino en m√∫ltiples particiones.\n",
        "* **Beneficio:**\n",
        "\n",
        "  * Reduce la **varianza** en la estimaci√≥n del desempe√±o.\n",
        "  * Asegura que todos los datos participan alguna vez como entrenamiento y como validaci√≥n.\n",
        "  * Evita que nuestra evaluaci√≥n dependa de un ‚Äúgolpe de suerte‚Äù en c√≥mo partimos los datos.\n",
        "* **Analog√≠a:** Es como probar un coche no solo en una pista lisa, sino tambi√©n en caminos distintos para asegurarnos de que rinde bien en general.\n",
        "\n",
        "---\n",
        "\n",
        "### 6.2. ¬øQu√© m√©trica elegir si me importa m√°s detectar todos los clientes que van a fugarse (churn)?\n",
        "\n",
        "* Aqu√≠ hablamos de **sensibilidad/recall**.\n",
        "* **Recall** = de todos los que realmente se fueron, ¬øcu√°ntos detect√©?\n",
        "* Si tu objetivo es **no perder a ning√∫n cliente que se fuga**, necesitas maximizar el recall, incluso si eso significa aceptar m√°s falsos positivos.\n",
        "* **Ejemplo:** Es como un detector de incendios: mejor que suene muchas veces aunque sea por vapor de la ducha, a que no suene cuando hay fuego real.\n",
        "\n",
        "---\n",
        "\n",
        "### 6.3. ¬øY si el costo de un falso positivo es muy alto?\n",
        "\n",
        "* Entonces priorizamos la **precisi√≥n**.\n",
        "* **Precisi√≥n** = de todos los que marqu√© como fuga, ¬øcu√°ntos realmente se fueron?\n",
        "* Si contactar o dar beneficios a un cliente mal clasificado cuesta mucho, entonces conviene sacrificar recall y ganar en precisi√≥n.\n",
        "* **Ejemplo:** Campa√±as de retenci√≥n costosas: no quieres gastar en clientes que igual no se iban a ir.\n",
        "\n",
        "---\n",
        "\n",
        "### 6.4. ¬øPor qu√© ROC-AUC es preferible a Accuracy en problemas desbalanceados?\n",
        "\n",
        "* En datasets desbalanceados, la **accuracy** puede ser enga√±osa:\n",
        "\n",
        "  * Si el 95% de clientes no se fuga, un modelo que siempre prediga ‚Äúno fuga‚Äù tendr√° 95% de accuracy.\n",
        "  * Pero ese modelo no sirve para detectar churn.\n",
        "* **ROC-AUC** mide la capacidad del modelo de rankear correctamente entre positivos y negativos.\n",
        "\n",
        "  * Es independiente del umbral de decisi√≥n.\n",
        "  * Permite comparar modelos incluso en situaciones de gran desbalance.\n",
        "* **Ejemplo:** En un aeropuerto, casi todos los equipajes son normales; accuracy te dir√≠a ‚Äúest√°s perfecto‚Äù si nunca detectas nada sospechoso. Pero lo que interesa es tu habilidad de distinguir maletas peligrosas de las normales.\n",
        "\n",
        "### 6.5. ¬øQu√© trade-off aceptar√≠as en un banco? ¬øY en un hospital?\n",
        "\n",
        "| Contexto                   | Costo m√°s grave                                | M√©trica a priorizar           | Ejemplo de decisi√≥n                                                                 |\n",
        "| -------------------------- | ---------------------------------------------- | ----------------------------- | ----------------------------------------------------------------------------------- |\n",
        "| **Banco ‚Äì Churn**          | Falso negativo (perder un cliente que se fuga) | Recall (Sensibilidad)         | Mejor contactar de m√°s que perder clientes valiosos                                 |\n",
        "| **Banco ‚Äì Fraude**         | Falso positivo (bloquear cliente inocente)     | Precisi√≥n                     | Minimizar molestias y costos reputacionales                                         |\n",
        "| **Banco ‚Äì Scoring (tradicional)** | Falso positivo (dar cr√©dito a quien no paga)   | Precisi√≥n en clase ‚Äúaprobado‚Äù | Mejor rechazar a algunos buenos que aprobar demasiados malos                        |\n",
        "| **Banco ‚Äì Scoring (inclusi√≥n/fintech)** | Falso negativo (rechazar a un cliente confiable) | Recall (Sensibilidad)         | Mejor aceptar a casi todos los buenos clientes aunque se filtren algunos malos      |\n",
        "| **Hospital ‚Äì Diagn√≥stico** | Falso negativo (no detectar enfermo)           | Recall (Sensibilidad)         | Mejor hacer pruebas extras que dejar un enfermo sin diagn√≥stico                     |\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5334fa80",
      "metadata": {
        "id": "5334fa80"
      },
      "source": [
        "# Parte B: Modelos cl√°sicos (Logistic Regression y √Årboles)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "734272e5",
      "metadata": {
        "id": "734272e5"
      },
      "source": [
        "## Objetivos\n",
        "- Aplicar Logistic Regression a un caso real de churn.\n",
        "- Comparar con un √Årbol de Decisi√≥n.\n",
        "- Analizar ventajas/desventajas de cada modelo.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Dataset Churn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb64cecf",
      "metadata": {
        "id": "eb64cecf"
      },
      "source": [
        "---\n",
        "\n",
        "## 2. Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d27d7b46",
      "metadata": {
        "id": "d27d7b46"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "lr = LogisticRegression(max_iter=5000)\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "y_proba_lr = lr.predict_proba(X_test)[:,1]\n",
        "\n",
        "print(\"Logistic Regression\")\n",
        "print(classification_report(y_test, y_pred_lr))\n",
        "print(\"AUC:\", roc_auc_score(y_test, y_proba_lr))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8bd7234",
      "metadata": {
        "id": "b8bd7234"
      },
      "source": [
        "---\n",
        "\n",
        "## 3. √Årbol de Decisi√≥n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa380a93",
      "metadata": {
        "id": "aa380a93"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "tree = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "y_pred_tree = tree.predict(X_test)\n",
        "y_proba_tree = tree.predict_proba(X_test)[:,1]\n",
        "\n",
        "print(\"Decision Tree\")\n",
        "print(classification_report(y_test, y_pred_tree))\n",
        "print(\"AUC:\", roc_auc_score(y_test, y_proba_tree))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f95e836-25d3-429e-97be-26c3027488f3",
      "metadata": {
        "id": "5f95e836-25d3-429e-97be-26c3027488f3"
      },
      "source": [
        "---\n",
        "\n",
        "## 4. Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0879cf51-d9eb-4f32-adfd-ad48ccaf5051",
      "metadata": {
        "id": "0879cf51-d9eb-4f32-adfd-ad48ccaf5051"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "# Definir modelo Random Forest\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=200,   # n√∫mero de √°rboles\n",
        "    max_depth=6,        # profundidad m√°xima de cada √°rbol\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Entrenar\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predicciones\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "y_proba_rf = rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# M√©tricas\n",
        "print(\"Random Forest\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "print(\"AUC:\", roc_auc_score(y_test, y_proba_rf))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b24bdae-49cb-4f2a-bfb8-6d0d108e8e1f",
      "metadata": {
        "id": "3b24bdae-49cb-4f2a-bfb8-6d0d108e8e1f"
      },
      "source": [
        "---\n",
        "\n",
        "üîé **Diferencias clave RandomForestClassifier vs DecisionTreeClassifier**:\n",
        "\n",
        "* `RandomForestClassifier` combina muchos √°rboles (bagging) ‚Üí m√°s robusto y menos sobreajuste.\n",
        "* Hiperpar√°metros importantes:\n",
        "\n",
        "  * `n_estimators`: n¬∫ de √°rboles (100‚Äì500 es com√∫n).\n",
        "  * `max_depth`: controla complejidad (igual que en √°rboles individuales).\n",
        "  * `max_features`: n¬∫ de variables usadas en cada split (default = sqrt para clasificaci√≥n).\n",
        "* Se obtiene `predict` y `predict_proba` igual que en el √°rbol simple.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46e324cb",
      "metadata": {
        "id": "46e324cb"
      },
      "source": [
        "---\n",
        "\n",
        "## 4. Comparaci√≥n Gr√°fica"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe4bce4a",
      "metadata": {
        "id": "fe4bce4a"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import RocCurveDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "RocCurveDisplay.from_estimator(lr, X_test, y_test, name=\"Logistic Regression\")\n",
        "RocCurveDisplay.from_estimator(tree, X_test, y_test, name=\"Decision Tree\")\n",
        "RocCurveDisplay.from_estimator(rf, X_test, y_test, name=\"Random Forest\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8999a06f",
      "metadata": {
        "id": "8999a06f"
      },
      "source": [
        "---\n",
        "\n",
        "## 5. Discusi√≥n\n",
        "\n",
        "* **Logistic Regression**:\n",
        "\n",
        "  * Modelo lineal, interpretable.\n",
        "  * √ötil como *baseline* porque establece un punto de referencia claro.\n",
        "  * Bueno cuando las relaciones entre variables y el objetivo son aproximadamente lineales.\n",
        "  * F√°cil de explicar a negocio (‚Äúcada variable aumenta/disminuye la probabilidad de churn en tanto %‚Äù).\n",
        "\n",
        "* **Decision Tree**:\n",
        "\n",
        "  * Captura relaciones **no lineales** y **interacciones** entre variables sin necesidad de transformaciones.\n",
        "  * F√°cil de explicar visualmente (se puede graficar el √°rbol y mostrar las reglas).\n",
        "  * Puede sobreajustar si no se controla la profundidad u otros par√°metros.\n",
        "  * Bueno para comunicaci√≥n, pero menos estable (peque√±os cambios en los datos ‚Üí √°rbol diferente).\n",
        "\n",
        "* **Random Forest**:\n",
        "\n",
        "  * Conjunto de muchos √°rboles ‚Üí m√°s robusto y con mejor desempe√±o promedio que un solo √°rbol.\n",
        "  * Reduce el riesgo de sobreajuste combinando resultados (bagging).\n",
        "  * Captura no linealidades y complejas interacciones de forma m√°s estable.\n",
        "  * Menos interpretable que un √°rbol √∫nico o regresi√≥n log√≠stica (aunque se pueden usar **importancias de variables** o **SHAP** para interpretarlo).\n",
        "  * Normalmente alcanza un **mejor equilibrio entre bias y varianza**.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Preguntas de Discusi√≥n\n",
        "\n",
        "1. ¬øQu√© modelo recomendar√≠an para presentar a un equipo de negocio?\n",
        "2. ¬øQu√© riesgos hay de sobreajuste en un √°rbol?\n",
        "3. ¬øC√≥mo se podr√≠an combinar ambos enfoques?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7802ab3-aa06-44bb-a73e-664296d43039",
      "metadata": {
        "id": "d7802ab3-aa06-44bb-a73e-664296d43039"
      },
      "source": [
        "## 6. Preguntas de Discusi√≥n\n",
        "\n",
        "**1. ¬øQu√© modelo recomendar√≠an para presentar a un equipo de negocio?**\n",
        "\n",
        "* **Regresi√≥n Log√≠stica** es normalmente la opci√≥n m√°s adecuada para presentaciones a negocio porque:\n",
        "\n",
        "  * Es **f√°cil de interpretar**: cada variable tiene un coeficiente que indica su impacto en la probabilidad de churn.\n",
        "  * Permite explicar ‚Äúqu√© factores incrementan la probabilidad de fuga‚Äù.\n",
        "  * Ideal como **baseline** antes de pasar a modelos m√°s complejos.\n",
        "* **√Årbol de Decisi√≥n** tambi√©n es una buena alternativa si se quiere mostrar reglas claras y visuales (‚Äúsi el cliente tiene contrato mensual y bajo tenure ‚Üí alta probabilidad de churn‚Äù).\n",
        "\n",
        "---\n",
        "\n",
        "**2. ¬øQu√© riesgos hay de sobreajuste en un √°rbol?**\n",
        "\n",
        "* Un **√°rbol profundo** puede aprender demasiado los datos de entrenamiento, detectando patrones espurios.\n",
        "* Esto se traduce en **alta accuracy en train pero bajo desempe√±o en test**.\n",
        "* Factores de riesgo:\n",
        "\n",
        "  * No limitar la profundidad (`max_depth`).\n",
        "  * No restringir el n√∫mero m√≠nimo de muestras por hoja (`min_samples_leaf`).\n",
        "  * Dataset con ruido o muchas variables categ√≥ricas.\n",
        "\n",
        "---\n",
        "\n",
        "**3. ¬øC√≥mo se podr√≠an combinar ambos enfoques?**\n",
        "\n",
        "* **Random Forest** es justamente la combinaci√≥n de muchos √°rboles (bagging), reduciendo la varianza y mejorando la generalizaci√≥n.\n",
        "* Otra alternativa es **Ensemble H√≠brido**:\n",
        "\n",
        "  * Usar **Regresi√≥n Log√≠stica** como baseline por su interpretabilidad.\n",
        "  * Usar **Random Forest / Gradient Boosting** para mejorar predicciones.\n",
        "  * Comparar ambos y reportar al negocio insights del modelo interpretable + performance del modelo robusto.\n",
        "* Incluso se pueden usar **modelos en cascada**: primero un modelo interpretable para filtrar casos ‚Äúclaros‚Äù y luego un modelo complejo para los ‚Äúdudosos‚Äù.\n",
        "\n",
        "---\n"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}