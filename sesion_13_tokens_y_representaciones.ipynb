{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rubuntu/Taller_Introduccion_a_Ciencia_de_Datos_IA_e_Ingenieria_de_Datos/blob/main/sesion_13_tokens_y_representaciones.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1259c12",
      "metadata": {
        "id": "d1259c12"
      },
      "source": [
        "# Sesi√≥n 13 ‚Äì Tokens y Representaciones\n",
        "\n",
        "## Objetivos\n",
        "- Comprender c√≥mo representar texto en formato num√©rico.\n",
        "- Comparar Bag-of-Words, TF-IDF y Embeddings.\n",
        "- Implementar embeddings preentrenados.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "### ¬øQu√© es NLP?\n",
        "\n",
        "El **Procesamiento de Lenguaje Natural (NLP, por sus siglas en ingl√©s)** es una rama de la inteligencia artificial que estudia c√≥mo las computadoras pueden comprender, interpretar y generar lenguaje humano.\n",
        "Sus aplicaciones incluyen traducci√≥n autom√°tica, chatbots, an√°lisis de sentimientos, clasificaci√≥n de texto, extracci√≥n de informaci√≥n, entre muchas otras.\n",
        "\n",
        "---\n",
        "\n",
        "### Tokens en NLP\n",
        "\n",
        "En NLP, los *tokens* son las unidades m√≠nimas en que se divide el texto para su an√°lisis.\n",
        "\n",
        "* **Tokenizaci√≥n tradicional**: cada palabra se considera un token (ej. ‚Äúaprendiendo NLP‚Äù ‚Üí \\[‚Äúaprendiendo‚Äù, ‚ÄúNLP‚Äù]).\n",
        "* **Tokenizaci√≥n subword**: divide palabras en fragmentos m√°s peque√±os (ej. ‚Äúaprendiendo‚Äù ‚Üí \\[‚Äúaprend‚Äù, ‚Äúiendo‚Äù]), √∫til para manejar vocabularios extensos y palabras desconocidas.\n",
        "* **Caracteres**: en algunos sistemas, cada letra o car√°cter es un token.\n",
        "\n",
        "---\n",
        "\n",
        "### Bag-of-Words (BoW), TF-IDF y Embeddings\n",
        "\n",
        "#### Bag-of-Words (BoW)\n",
        "\n",
        "* Representa el texto como un vector que cuenta la frecuencia de cada palabra en un vocabulario.\n",
        "* Ventajas: sencillo, interpretable.\n",
        "* Limitaciones:\n",
        "\n",
        "  * Pierde el orden de las palabras.\n",
        "  * Alt√≠sima dimensionalidad (un vector por cada palabra del vocabulario).\n",
        "  * No captura relaciones sem√°nticas (ej. ‚Äúgato‚Äù y ‚Äúfelino‚Äù aparecen como vectores totalmente distintos).\n",
        "\n",
        "#### TF-IDF (Term Frequency ‚Äì Inverse Document Frequency)\n",
        "\n",
        "* Mejora sobre BoW ponderando cada palabra seg√∫n:\n",
        "\n",
        "  * **TF (Frecuencia de t√©rmino)**: cu√°ntas veces aparece en un documento.\n",
        "  * **IDF (Frecuencia inversa de documento)**: qu√© tan rara es esa palabra en la colecci√≥n de documentos.\n",
        "* Intuici√≥n: las palabras comunes como ‚Äúel‚Äù o ‚Äúla‚Äù reciben menor peso, mientras que t√©rminos distintivos como ‚Äúneuronas‚Äù o ‚Äútransformer‚Äù reciben mayor relevancia.\n",
        "* Sigue siendo esparso y no captura relaciones sem√°nticas profundas, pero mejora la representaci√≥n frente a BoW.\n",
        "\n",
        "#### Embeddings\n",
        "\n",
        "* Son representaciones densas y de baja dimensi√≥n, donde cada palabra (o token) se proyecta en un espacio vectorial.\n",
        "* Capturan similitudes sem√°nticas: palabras con significados cercanos quedan m√°s pr√≥ximas en el espacio.\n",
        "* Ejemplos cl√°sicos: **Word2Vec**, **GloVe**, **FastText**.\n",
        "* Ejemplo: ‚Äúrey - hombre + mujer ‚âà reina‚Äù.\n",
        "\n",
        "**Importancia:** Los embeddings permiten que los modelos comprendan mejor las relaciones entre palabras, mejorando traducci√≥n autom√°tica, clasificaci√≥n de texto, an√°lisis de sentimientos, etc.\n",
        "\n",
        "---\n",
        "\n",
        "### Arquitectura Transformer\n",
        "\n",
        "Los **Transformers** revolucionaron el NLP al introducir mecanismos de *atenci√≥n* en lugar de depender de redes recurrentes (RNN/LSTM).\n",
        "\n",
        "* **Self-Attention**: cada token puede ‚Äúprestar atenci√≥n‚Äù a todos los dem√°s en la secuencia, capturando dependencias largas.\n",
        "* **Paralelizaci√≥n**: al no ser recurrente, se entrena de manera m√°s eficiente.\n",
        "* **Escalabilidad**: es la base de los grandes modelos de lenguaje modernos.\n",
        "\n",
        "---\n",
        "\n",
        "### Modelos basados en Transformers: BERT y RoBERTa\n",
        "\n",
        "#### BERT (*Bidirectional Encoder Representations from Transformers*)\n",
        "\n",
        "* Propuesto por Google (2018).\n",
        "* Pre-entrenado en grandes corpus con dos tareas principales:\n",
        "\n",
        "  * *Masked Language Modeling (MLM)*: predecir palabras ocultas en una oraci√≥n.\n",
        "  * *Next Sentence Prediction (NSP)*: predecir si una oraci√≥n sigue a otra.\n",
        "* Es bidireccional: tiene en cuenta el contexto a izquierda y derecha de cada token.\n",
        "* Se adapta bien a m√∫ltiples tareas mediante *fine-tuning* (clasificaci√≥n, preguntas y respuestas, etc.).\n",
        "\n",
        "#### RoBERTa (*Robustly Optimized BERT Approach*)\n",
        "\n",
        "* Propuesto por Facebook AI.\n",
        "* Mejora sobre BERT al:\n",
        "\n",
        "  * Entrenar con m√°s datos y durante m√°s tiempo.\n",
        "  * Usar secuencias m√°s largas.\n",
        "  * Eliminar la tarea NSP, que resultaba poco √∫til.\n",
        "* Generalmente supera a BERT en benchmarks de NLP.\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ **Resumen actualizado:**\n",
        "El **NLP** busca ense√±ar a las m√°quinas a comprender el lenguaje humano. Para representarlo, se empez√≥ con enfoques simples como **Bag-of-Words** y **TF-IDF**, que cuentan frecuencias pero no capturan significado profundo. Los **embeddings** aportaron representaciones densas y sem√°nticas, abriendo paso a modelos m√°s poderosos. Finalmente, los **Transformers**, con arquitecturas como **BERT** y **RoBERTa**, lograron un entendimiento contextual bidireccional, revolucionando el estado del arte en m√∫ltiples tareas de NLP.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "QXhWhPT6doVK"
      },
      "id": "QXhWhPT6doVK"
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo de Clasificador (LogReg/Naive Bayes) con BoW/TF-IDF.\n",
        "\n",
        "from datasets import load_dataset\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Cargar dataset AG News\n",
        "dataset = load_dataset(\"ag_news\")\n",
        "X_train = dataset[\"train\"][\"text\"]\n",
        "y_train = dataset[\"train\"][\"label\"]\n",
        "X_test = dataset[\"test\"][\"text\"]\n",
        "y_test = dataset[\"test\"][\"label\"]\n",
        "\n",
        "# 2. Vectorizadores (BoW y TF-IDF)\n",
        "bow_vectorizer = CountVectorizer(max_features=5000, stop_words=\"english\")\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words=\"english\")\n",
        "\n",
        "X_train_bow = bow_vectorizer.fit_transform(X_train)\n",
        "X_test_bow = bow_vectorizer.transform(X_test)\n",
        "\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# 3. Modelos a comparar\n",
        "models = {\n",
        "    \"LogReg_BoW\": (LogisticRegression(max_iter=1000), X_train_bow, X_test_bow),\n",
        "    \"NaiveBayes_BoW\": (MultinomialNB(), X_train_bow, X_test_bow),\n",
        "    \"LogReg_TFIDF\": (LogisticRegression(max_iter=1000), X_train_tfidf, X_test_tfidf),\n",
        "    \"NaiveBayes_TFIDF\": (MultinomialNB(), X_train_tfidf, X_test_tfidf),\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "# 4. Entrenar y evaluar\n",
        "for name, (model, Xtr, Xte) in models.items():\n",
        "    model.fit(Xtr, y_train)\n",
        "    y_pred = model.predict(Xte)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    results.append({\"Modelo\": name, \"Accuracy\": acc})\n",
        "\n",
        "    # --- Matriz de confusi√≥n ---\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(6,5))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                xticklabels=dataset[\"train\"].features[\"label\"].names,\n",
        "                yticklabels=dataset[\"train\"].features[\"label\"].names)\n",
        "    plt.title(f\"Matriz de Confusi√≥n - {name}\")\n",
        "    plt.xlabel(\"Predicci√≥n\")\n",
        "    plt.ylabel(\"Real\")\n",
        "    plt.show()\n",
        "\n",
        "# 5. Tabla comparativa de accuracies\n",
        "df_results = pd.DataFrame(results).sort_values(by=\"Accuracy\", ascending=False).reset_index(drop=True)\n",
        "print(\"\\nResultados comparativos:\\n\", df_results)\n",
        "\n"
      ],
      "metadata": {
        "id": "-IYVLm0PkTBe"
      },
      "id": "-IYVLm0PkTBe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo de Bag-of-Words vs embeddings\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Ejemplo de oraciones\n",
        "sentences = [\n",
        "    \"El gato duerme en la silla\",\n",
        "    \"Un felino descansa en la silla\",\n",
        "]\n",
        "\n",
        "# --- Bag of Words ---\n",
        "vectorizer = CountVectorizer()\n",
        "bow_matrix = vectorizer.fit_transform(sentences).toarray()\n",
        "\n",
        "# --- Embeddings con modelo preentrenado ---\n",
        "model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
        "embeddings = model.encode(sentences)\n",
        "\n",
        "# --- Similaridades ---\n",
        "bow_sim = cosine_similarity([bow_matrix[0]], [bow_matrix[1]])[0][0]\n",
        "embed_sim = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame({\n",
        "    \"M√©todo\": [\"Bag-of-Words\", \"Embeddings\"],\n",
        "    \"Representaci√≥n (dim)\": [str(bow_matrix.shape[1]), str(embeddings.shape[1])],\n",
        "    \"Similitud coseno\": [bow_sim, embed_sim]\n",
        "})\n",
        "\n",
        "df\n"
      ],
      "metadata": {
        "id": "-8jinFOrdgGv"
      },
      "id": "-8jinFOrdgGv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checklist:**\n",
        "\n",
        "1. Cargar dataset (IMDB o Yelp).\n",
        "2. Mostrar 5 rese√±as aleatorias y discutir su tono.\n",
        "3. Escribir 2 rese√±as propias (positiva y negativa).\n",
        "4. Tokenizar esas rese√±as con Hugging Face `AutoTokenizer`.\n",
        "5. Medir longitud de tokens y compararla con el texto original.\n",
        "6. Visualizar tokens generados (IDs y palabras).\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Cargar dataset y explorar"
      ],
      "metadata": {
        "id": "O0_bDA7qdhZb"
      },
      "id": "O0_bDA7qdhZb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12bc114a",
      "metadata": {
        "id": "12bc114a"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# IMDB reviews (50k rese√±as)\n",
        "dataset = load_dataset(\"imdb\")\n",
        "print(dataset)\n",
        "\n",
        "# Ejemplo\n",
        "print(dataset[\"train\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a20d1ade",
      "metadata": {
        "id": "a20d1ade"
      },
      "source": [
        "---\n",
        "\n",
        "## 2. Explorar rese√±as y escribir ejemplos propios"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from transformers import pipeline\n",
        "\n",
        "# Convierte el split 'train' a un DataFrame de pandas\n",
        "df = pd.DataFrame(dataset[\"train\"])\n",
        "\n",
        "# Configura pandas para mostrar el contenido completo de las columnas\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Filtra las rese√±as que tienen menos de 140 caracteres\n",
        "# (El dataset IMDB tiene textos largos, ajustamos el filtro si es necesario)\n",
        "short_reviews_df = df[df['text'].str.len() < 140]\n",
        "\n",
        "# Toma una muestra aleatoria reproducible de 5 rese√±as cortas\n",
        "sample_of_short_reviews = short_reviews_df.sample(n=5, random_state=42)\n",
        "print(\"Muestra de 6 rese√±as cortas seleccionada.\")\n",
        "\n",
        "# Traducci√≥n del texto\n",
        "\n",
        "try:\n",
        "    # 6. Carga el pipeline de traducci√≥n (descargar√° el modelo la primera vez)\n",
        "    print(\"\\nCargando el modelo de traducci√≥n (Helsinki-NLP/opus-mt-en-es)...\")\n",
        "    translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-es\")\n",
        "    print(\"Modelo de traducci√≥n cargado. ‚úÖ\")\n",
        "\n",
        "    # 7. Extrae la lista de textos en ingl√©s del DataFrame\n",
        "    texts_to_translate = sample_of_short_reviews['text'].tolist()\n",
        "\n",
        "    # 8. Traduce la lista de textos\n",
        "    print(\"\\nTraduciendo textos...\")\n",
        "    translated_texts = translator(texts_to_translate)\n",
        "    print(\"Traducci√≥n completada.\")\n",
        "\n",
        "    # 9. A√±ade la traducci√≥n como una nueva columna al DataFrame\n",
        "    sample_of_short_reviews['texto_traducido'] = [t['translation_text'] for t in translated_texts]\n",
        "\n",
        "    # 10. Muestra el DataFrame final con ambas columnas\n",
        "    print(\"\\n--- Resultado Final: Rese√±as Originales y Traducidas ---\")\n",
        "    print(sample_of_short_reviews[['text', 'texto_traducido', 'label']])\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nHa ocurrido un error: {e}\")\n",
        "    print(\"Aseg√∫rate de tener todas las librer√≠as instaladas.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "W8KN9j_FHcq3"
      },
      "id": "W8KN9j_FHcq3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d7bf04a3",
      "metadata": {
        "id": "d7bf04a3"
      },
      "source": [
        "---\n",
        "\n",
        "## 3. Tokenizar rese√±as"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "my_reviews = [\n",
        "    \"Fui a ver 'Ecos del Silencio' este fin de semana sin muchas expectativas, y sal√≠ de la sala completamente maravillado.\",\n",
        "    \"Ten√≠a muchas ganas de ver 'Misi√≥n Cifrada', pero lamentablemente ha sido una gran decepci√≥n.\"\n",
        "]\n",
        "\n",
        "# RoBERTa base en espa√±ol, con tokenizador BPE est√°ndar\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bertin-project/bertin-roberta-base-spanish\")\n",
        "\n",
        "print(\"‚úÖ Tokenizador RoBERTa en espa√±ol cargado correctamente.\")\n",
        "\n",
        "tokens = tokenizer(my_reviews, padding=True, truncation=True)\n",
        "print(\"\\n--- Resultado de la Tokenizaci√≥n ---\")\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "CBCWBWzKWkbq"
      },
      "id": "CBCWBWzKWkbq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c053db78",
      "metadata": {
        "id": "c053db78"
      },
      "source": [
        "---\n",
        "\n",
        "## 4. An√°lisis de tokens\n",
        "\n",
        "* Comparar n√∫mero de palabras vs tokens.\n",
        "* Visualizar palabras ‚Üí IDs."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- C√≥digo para comparar palabras vs. tokens ---\n",
        "\n",
        "for i, review in enumerate(my_reviews):\n",
        "    print(f\"--- RESE√ëA #{i+1} ---\")\n",
        "    print(f\"Texto original: \\\"{review}\\\"\")\n",
        "\n",
        "    # 1. Conteo de palabras (m√©todo simple: dividir por espacios)\n",
        "    word_count = len(review.split())\n",
        "    print(f\"üîπ N√∫mero de Palabras: {word_count}\")\n",
        "\n",
        "    # 2. Conteo de tokens (usando el tokenizador)\n",
        "    # Tokenizamos el texto y obtenemos los IDs de los tokens\n",
        "    token_ids = tokenizer.encode(review)\n",
        "\n",
        "    # Contamos el n√∫mero total de tokens\n",
        "    token_count = len(token_ids)\n",
        "    print(f\"üî∏ N√∫mero de Tokens: {token_count}\")\n",
        "\n",
        "    # Para visualizar, convertimos los IDs de vuelta a tokens\n",
        "    tokens_list = tokenizer.convert_ids_to_tokens(token_ids)\n",
        "    print(f\"   Tokens generados: {tokens_list}\\n\")"
      ],
      "metadata": {
        "id": "R1vSozkUZs4y"
      },
      "id": "R1vSozkUZs4y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Preguntas de discusi√≥n\n",
        "\n",
        "### 1. ¬øQu√© ventajas tienen los embeddings frente a bag-of-words?\n",
        "\n",
        "Los **embeddings** (incrustaciones de palabras) representan un avance fundamental sobre el modelo **bag-of-words** (BoW) porque capturan el **significado sem√°ntico** y el **contexto** de las palabras, algo que BoW es incapaz de hacer.\n",
        "\n",
        "El modelo **bag-of-words** √∫nicamente registra la frecuencia de las palabras en un texto, pero ignora por completo su orden y la relaci√≥n que tienen entre s√≠. Para BoW, las frases \"el perro persigue al gato\" y \"el gato persigue al perro\" son muy similares, aunque su significado es opuesto. Adem√°s, trata palabras como \"rey\" y \"reina\" como dos conceptos totalmente independientes y sin ninguna relaci√≥n.\n",
        "\n",
        "Los **embeddings**, en cambio, mapean palabras a vectores de n√∫meros de tal manera que las palabras con significados similares tienen vectores cercanos en un espacio multidimensional.\n",
        "\n",
        "Las principales ventajas son:\n",
        "\n",
        "* **Captura de Relaciones Sem√°nticas:** Los embeddings sit√∫an palabras como \"rey\" y \"reina\" cerca una de la otra. Incluso pueden capturar relaciones anal√≥gicas, como la famosa `vector('rey') - vector('hombre') + vector('mujer') ‚âà vector('reina')`.\n",
        "* **Eficiencia Dimensional:** Mientras que BoW crea vectores muy largos (uno por cada palabra del vocabulario) y dispersos (llenos de ceros), los embeddings son vectores densos y de menor dimensi√≥n (ej. 300 dimensiones vs. 50,000 de BoW), lo que los hace computacionalmente m√°s eficientes.\n",
        "* **Generalizaci√≥n:** Un modelo pre-entrenado con embeddings (como Word2Vec o GloVe) ya \"sabe\" que \"excelente\" y \"fant√°stico\" son similares, incluso si en tu set de datos de entrenamiento nunca aparecen en el mismo contexto. BoW no puede hacer esta generalizaci√≥n.\n",
        "\n",
        "En resumen, pasar de bag-of-words a embeddings es como pasar de un simple conteo de palabras a una verdadera comprensi√≥n de su significado y de c√≥mo se relacionan entre s√≠.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. ¬øQu√© tipo de rese√±as ser√≠an m√°s dif√≠ciles de clasificar?\n",
        "\n",
        "Los modelos de clasificaci√≥n de texto, incluso los m√°s avanzados, tienen dificultades con rese√±as que requieren una comprensi√≥n profunda del lenguaje humano, el contexto y el conocimiento del mundo. Las m√°s dif√≠ciles son:\n",
        "\n",
        "* **Sarcasmo e Iron√≠a:** Son el mayor desaf√≠o. Una rese√±a como *\"Claro, me encant√≥ esperar 40 minutos por un caf√© fr√≠o. Una experiencia fant√°stica.\"* utiliza palabras positivas (\"encant√≥\", \"fant√°stica\") para expresar un sentimiento fuertemente negativo. El modelo, al leer literalmente las palabras, puede clasificarla err√≥neamente.\n",
        "\n",
        "* **Rese√±as Mixtas o Ambiguas:** Aquellas que contienen tanto elementos positivos como negativos son dif√≠ciles de encasillar en una sola categor√≠a. Por ejemplo: *\"La comida era deliciosa y el ambiente muy bueno, pero el servicio fue p√©simo y arruin√≥ la noche.\"* ¬øEs una rese√±a positiva o negativa? Depende del peso que se le d√© a cada aspecto.\n",
        "\n",
        "* **Lenguaje Comparativo:** Rese√±as que eval√∫an un producto en relaci√≥n con otro. *\"Es mucho mejor que el modelo anterior, aunque sigue sin estar a la altura de la competencia.\"* La clasificaci√≥n depende de un punto de referencia que el modelo puede no conocer.\n",
        "\n",
        "* **Falta de Contexto o Conocimiento del Mundo:** Frases que requieren conocimiento externo. Por ejemplo, *\"Este producto es el 'New Coke' de los videojuegos.\"* Para entender que esto es una cr√≠tica muy negativa, el modelo necesitar√≠a saber sobre el famoso fracaso comercial de Coca-Cola en los a√±os 80.\n",
        "\n",
        "* **Textos muy cortos o con jerga:** Rese√±as como *\"meh\"* o *\"equis\"* son dif√≠ciles porque contienen muy poca informaci√≥n contextual. De igual manera, el uso de jerga muy espec√≠fica de un nicho puede confundir a un modelo que no fue entrenado con ella."
      ],
      "metadata": {
        "id": "AFWl4S_LgEOB"
      },
      "id": "AFWl4S_LgEOB"
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}