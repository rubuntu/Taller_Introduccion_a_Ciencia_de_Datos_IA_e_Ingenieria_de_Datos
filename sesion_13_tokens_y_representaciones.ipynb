{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rubuntu/Taller_Introduccion_a_Ciencia_de_Datos_IA_e_Ingenieria_de_Datos/blob/main/sesion_13_tokens_y_representaciones.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1259c12",
      "metadata": {
        "id": "d1259c12"
      },
      "source": [
        "# Sesión 13 – Tokens y Representaciones\n",
        "\n",
        "## Objetivos\n",
        "- Comprender cómo representar texto en formato numérico.\n",
        "- Comparar Bag-of-Words, TF-IDF y Embeddings.\n",
        "- Implementar embeddings preentrenados.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "### ¿Qué es NLP?\n",
        "\n",
        "El **Procesamiento de Lenguaje Natural (NLP, por sus siglas en inglés)** es una rama de la inteligencia artificial que estudia cómo las computadoras pueden comprender, interpretar y generar lenguaje humano.\n",
        "Sus aplicaciones incluyen traducción automática, chatbots, análisis de sentimientos, clasificación de texto, extracción de información, entre muchas otras.\n",
        "\n",
        "---\n",
        "\n",
        "### Tokens en NLP\n",
        "\n",
        "En NLP, los *tokens* son las unidades mínimas en que se divide el texto para su análisis.\n",
        "\n",
        "* **Tokenización tradicional**: cada palabra se considera un token (ej. “aprendiendo NLP” → \\[“aprendiendo”, “NLP”]).\n",
        "* **Tokenización subword**: divide palabras en fragmentos más pequeños (ej. “aprendiendo” → \\[“aprend”, “iendo”]), útil para manejar vocabularios extensos y palabras desconocidas.\n",
        "* **Caracteres**: en algunos sistemas, cada letra o carácter es un token.\n",
        "\n",
        "---\n",
        "\n",
        "### Bag-of-Words (BoW), TF-IDF y Embeddings\n",
        "\n",
        "#### Bag-of-Words (BoW)\n",
        "\n",
        "* Representa el texto como un vector que cuenta la frecuencia de cada palabra en un vocabulario.\n",
        "* Ventajas: sencillo, interpretable.\n",
        "* Limitaciones:\n",
        "\n",
        "  * Pierde el orden de las palabras.\n",
        "  * Altísima dimensionalidad (un vector por cada palabra del vocabulario).\n",
        "  * No captura relaciones semánticas (ej. “gato” y “felino” aparecen como vectores totalmente distintos).\n",
        "\n",
        "#### TF-IDF (Term Frequency – Inverse Document Frequency)\n",
        "\n",
        "* Mejora sobre BoW ponderando cada palabra según:\n",
        "\n",
        "  * **TF (Frecuencia de término)**: cuántas veces aparece en un documento.\n",
        "  * **IDF (Frecuencia inversa de documento)**: qué tan rara es esa palabra en la colección de documentos.\n",
        "* Intuición: las palabras comunes como “el” o “la” reciben menor peso, mientras que términos distintivos como “neuronas” o “transformer” reciben mayor relevancia.\n",
        "* Sigue siendo esparso y no captura relaciones semánticas profundas, pero mejora la representación frente a BoW.\n",
        "\n",
        "#### Embeddings\n",
        "\n",
        "* Son representaciones densas y de baja dimensión, donde cada palabra (o token) se proyecta en un espacio vectorial.\n",
        "* Capturan similitudes semánticas: palabras con significados cercanos quedan más próximas en el espacio.\n",
        "* Ejemplos clásicos: **Word2Vec**, **GloVe**, **FastText**.\n",
        "* Ejemplo: “rey - hombre + mujer ≈ reina”.\n",
        "\n",
        "**Importancia:** Los embeddings permiten que los modelos comprendan mejor las relaciones entre palabras, mejorando traducción automática, clasificación de texto, análisis de sentimientos, etc.\n",
        "\n",
        "---\n",
        "\n",
        "### Arquitectura Transformer\n",
        "\n",
        "Los **Transformers** revolucionaron el NLP al introducir mecanismos de *atención* en lugar de depender de redes recurrentes (RNN/LSTM).\n",
        "\n",
        "* **Self-Attention**: cada token puede “prestar atención” a todos los demás en la secuencia, capturando dependencias largas.\n",
        "* **Paralelización**: al no ser recurrente, se entrena de manera más eficiente.\n",
        "* **Escalabilidad**: es la base de los grandes modelos de lenguaje modernos.\n",
        "\n",
        "---\n",
        "\n",
        "### Modelos basados en Transformers: BERT y RoBERTa\n",
        "\n",
        "#### BERT (*Bidirectional Encoder Representations from Transformers*)\n",
        "\n",
        "* Propuesto por Google (2018).\n",
        "* Pre-entrenado en grandes corpus con dos tareas principales:\n",
        "\n",
        "  * *Masked Language Modeling (MLM)*: predecir palabras ocultas en una oración.\n",
        "  * *Next Sentence Prediction (NSP)*: predecir si una oración sigue a otra.\n",
        "* Es bidireccional: tiene en cuenta el contexto a izquierda y derecha de cada token.\n",
        "* Se adapta bien a múltiples tareas mediante *fine-tuning* (clasificación, preguntas y respuestas, etc.).\n",
        "\n",
        "#### RoBERTa (*Robustly Optimized BERT Approach*)\n",
        "\n",
        "* Propuesto por Facebook AI.\n",
        "* Mejora sobre BERT al:\n",
        "\n",
        "  * Entrenar con más datos y durante más tiempo.\n",
        "  * Usar secuencias más largas.\n",
        "  * Eliminar la tarea NSP, que resultaba poco útil.\n",
        "* Generalmente supera a BERT en benchmarks de NLP.\n",
        "\n",
        "---\n",
        "\n",
        "✅ **Resumen actualizado:**\n",
        "El **NLP** busca enseñar a las máquinas a comprender el lenguaje humano. Para representarlo, se empezó con enfoques simples como **Bag-of-Words** y **TF-IDF**, que cuentan frecuencias pero no capturan significado profundo. Los **embeddings** aportaron representaciones densas y semánticas, abriendo paso a modelos más poderosos. Finalmente, los **Transformers**, con arquitecturas como **BERT** y **RoBERTa**, lograron un entendimiento contextual bidireccional, revolucionando el estado del arte en múltiples tareas de NLP.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "QXhWhPT6doVK"
      },
      "id": "QXhWhPT6doVK"
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo de Clasificador (LogReg/Naive Bayes) con BoW/TF-IDF.\n",
        "\n",
        "from datasets import load_dataset\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Cargar dataset AG News\n",
        "dataset = load_dataset(\"ag_news\")\n",
        "X_train = dataset[\"train\"][\"text\"]\n",
        "y_train = dataset[\"train\"][\"label\"]\n",
        "X_test = dataset[\"test\"][\"text\"]\n",
        "y_test = dataset[\"test\"][\"label\"]\n",
        "\n",
        "# 2. Vectorizadores (BoW y TF-IDF)\n",
        "bow_vectorizer = CountVectorizer(max_features=5000, stop_words=\"english\")\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words=\"english\")\n",
        "\n",
        "X_train_bow = bow_vectorizer.fit_transform(X_train)\n",
        "X_test_bow = bow_vectorizer.transform(X_test)\n",
        "\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# 3. Modelos a comparar\n",
        "models = {\n",
        "    \"LogReg_BoW\": (LogisticRegression(max_iter=1000), X_train_bow, X_test_bow),\n",
        "    \"NaiveBayes_BoW\": (MultinomialNB(), X_train_bow, X_test_bow),\n",
        "    \"LogReg_TFIDF\": (LogisticRegression(max_iter=1000), X_train_tfidf, X_test_tfidf),\n",
        "    \"NaiveBayes_TFIDF\": (MultinomialNB(), X_train_tfidf, X_test_tfidf),\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "# 4. Entrenar y evaluar\n",
        "for name, (model, Xtr, Xte) in models.items():\n",
        "    model.fit(Xtr, y_train)\n",
        "    y_pred = model.predict(Xte)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    results.append({\"Modelo\": name, \"Accuracy\": acc})\n",
        "\n",
        "    # --- Matriz de confusión ---\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(6,5))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                xticklabels=dataset[\"train\"].features[\"label\"].names,\n",
        "                yticklabels=dataset[\"train\"].features[\"label\"].names)\n",
        "    plt.title(f\"Matriz de Confusión - {name}\")\n",
        "    plt.xlabel(\"Predicción\")\n",
        "    plt.ylabel(\"Real\")\n",
        "    plt.show()\n",
        "\n",
        "# 5. Tabla comparativa de accuracies\n",
        "df_results = pd.DataFrame(results).sort_values(by=\"Accuracy\", ascending=False).reset_index(drop=True)\n",
        "print(\"\\nResultados comparativos:\\n\", df_results)\n",
        "\n"
      ],
      "metadata": {
        "id": "-IYVLm0PkTBe"
      },
      "id": "-IYVLm0PkTBe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo de Bag-of-Words vs embeddings\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Ejemplo de oraciones\n",
        "sentences = [\n",
        "    \"El gato duerme en la silla\",\n",
        "    \"Un felino descansa en la silla\",\n",
        "]\n",
        "\n",
        "# --- Bag of Words ---\n",
        "vectorizer = CountVectorizer()\n",
        "bow_matrix = vectorizer.fit_transform(sentences).toarray()\n",
        "\n",
        "# --- Embeddings con modelo preentrenado ---\n",
        "model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
        "embeddings = model.encode(sentences)\n",
        "\n",
        "# --- Similaridades ---\n",
        "bow_sim = cosine_similarity([bow_matrix[0]], [bow_matrix[1]])[0][0]\n",
        "embed_sim = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame({\n",
        "    \"Método\": [\"Bag-of-Words\", \"Embeddings\"],\n",
        "    \"Representación (dim)\": [str(bow_matrix.shape[1]), str(embeddings.shape[1])],\n",
        "    \"Similitud coseno\": [bow_sim, embed_sim]\n",
        "})\n",
        "\n",
        "df\n"
      ],
      "metadata": {
        "id": "-8jinFOrdgGv"
      },
      "id": "-8jinFOrdgGv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checklist:**\n",
        "\n",
        "1. Cargar dataset (IMDB o Yelp).\n",
        "2. Mostrar 5 reseñas aleatorias y discutir su tono.\n",
        "3. Escribir 2 reseñas propias (positiva y negativa).\n",
        "4. Tokenizar esas reseñas con Hugging Face `AutoTokenizer`.\n",
        "5. Medir longitud de tokens y compararla con el texto original.\n",
        "6. Visualizar tokens generados (IDs y palabras).\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Cargar dataset y explorar"
      ],
      "metadata": {
        "id": "O0_bDA7qdhZb"
      },
      "id": "O0_bDA7qdhZb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12bc114a",
      "metadata": {
        "id": "12bc114a"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# IMDB reviews (50k reseñas)\n",
        "dataset = load_dataset(\"imdb\")\n",
        "print(dataset)\n",
        "\n",
        "# Ejemplo\n",
        "print(dataset[\"train\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a20d1ade",
      "metadata": {
        "id": "a20d1ade"
      },
      "source": [
        "---\n",
        "\n",
        "## 2. Explorar reseñas y escribir ejemplos propios"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from transformers import pipeline\n",
        "\n",
        "# Convierte el split 'train' a un DataFrame de pandas\n",
        "df = pd.DataFrame(dataset[\"train\"])\n",
        "\n",
        "# Configura pandas para mostrar el contenido completo de las columnas\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Filtra las reseñas que tienen menos de 140 caracteres\n",
        "# (El dataset IMDB tiene textos largos, ajustamos el filtro si es necesario)\n",
        "short_reviews_df = df[df['text'].str.len() < 140]\n",
        "\n",
        "# Toma una muestra aleatoria reproducible de 5 reseñas cortas\n",
        "sample_of_short_reviews = short_reviews_df.sample(n=5, random_state=42)\n",
        "print(\"Muestra de 6 reseñas cortas seleccionada.\")\n",
        "\n",
        "# Traducción del texto\n",
        "\n",
        "try:\n",
        "    # 6. Carga el pipeline de traducción (descargará el modelo la primera vez)\n",
        "    print(\"\\nCargando el modelo de traducción (Helsinki-NLP/opus-mt-en-es)...\")\n",
        "    translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-es\")\n",
        "    print(\"Modelo de traducción cargado. ✅\")\n",
        "\n",
        "    # 7. Extrae la lista de textos en inglés del DataFrame\n",
        "    texts_to_translate = sample_of_short_reviews['text'].tolist()\n",
        "\n",
        "    # 8. Traduce la lista de textos\n",
        "    print(\"\\nTraduciendo textos...\")\n",
        "    translated_texts = translator(texts_to_translate)\n",
        "    print(\"Traducción completada.\")\n",
        "\n",
        "    # 9. Añade la traducción como una nueva columna al DataFrame\n",
        "    sample_of_short_reviews['texto_traducido'] = [t['translation_text'] for t in translated_texts]\n",
        "\n",
        "    # 10. Muestra el DataFrame final con ambas columnas\n",
        "    print(\"\\n--- Resultado Final: Reseñas Originales y Traducidas ---\")\n",
        "    print(sample_of_short_reviews[['text', 'texto_traducido', 'label']])\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nHa ocurrido un error: {e}\")\n",
        "    print(\"Asegúrate de tener todas las librerías instaladas.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "W8KN9j_FHcq3"
      },
      "id": "W8KN9j_FHcq3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d7bf04a3",
      "metadata": {
        "id": "d7bf04a3"
      },
      "source": [
        "---\n",
        "\n",
        "## 3. Tokenizar reseñas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "my_reviews = [\n",
        "    \"Fui a ver 'Ecos del Silencio' este fin de semana sin muchas expectativas, y salí de la sala completamente maravillado.\",\n",
        "    \"Tenía muchas ganas de ver 'Misión Cifrada', pero lamentablemente ha sido una gran decepción.\"\n",
        "]\n",
        "\n",
        "# RoBERTa base en español, con tokenizador BPE estándar\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bertin-project/bertin-roberta-base-spanish\")\n",
        "\n",
        "print(\"✅ Tokenizador RoBERTa en español cargado correctamente.\")\n",
        "\n",
        "tokens = tokenizer(my_reviews, padding=True, truncation=True)\n",
        "print(\"\\n--- Resultado de la Tokenización ---\")\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "CBCWBWzKWkbq"
      },
      "id": "CBCWBWzKWkbq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c053db78",
      "metadata": {
        "id": "c053db78"
      },
      "source": [
        "---\n",
        "\n",
        "## 4. Análisis de tokens\n",
        "\n",
        "* Comparar número de palabras vs tokens.\n",
        "* Visualizar palabras → IDs."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Código para comparar palabras vs. tokens ---\n",
        "\n",
        "for i, review in enumerate(my_reviews):\n",
        "    print(f\"--- RESEÑA #{i+1} ---\")\n",
        "    print(f\"Texto original: \\\"{review}\\\"\")\n",
        "\n",
        "    # 1. Conteo de palabras (método simple: dividir por espacios)\n",
        "    word_count = len(review.split())\n",
        "    print(f\"🔹 Número de Palabras: {word_count}\")\n",
        "\n",
        "    # 2. Conteo de tokens (usando el tokenizador)\n",
        "    # Tokenizamos el texto y obtenemos los IDs de los tokens\n",
        "    token_ids = tokenizer.encode(review)\n",
        "\n",
        "    # Contamos el número total de tokens\n",
        "    token_count = len(token_ids)\n",
        "    print(f\"🔸 Número de Tokens: {token_count}\")\n",
        "\n",
        "    # Para visualizar, convertimos los IDs de vuelta a tokens\n",
        "    tokens_list = tokenizer.convert_ids_to_tokens(token_ids)\n",
        "    print(f\"   Tokens generados: {tokens_list}\\n\")"
      ],
      "metadata": {
        "id": "R1vSozkUZs4y"
      },
      "id": "R1vSozkUZs4y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Preguntas de discusión\n",
        "\n",
        "### 1. ¿Qué ventajas tienen los embeddings frente a bag-of-words?\n",
        "\n",
        "Los **embeddings** (incrustaciones de palabras) representan un avance fundamental sobre el modelo **bag-of-words** (BoW) porque capturan el **significado semántico** y el **contexto** de las palabras, algo que BoW es incapaz de hacer.\n",
        "\n",
        "El modelo **bag-of-words** únicamente registra la frecuencia de las palabras en un texto, pero ignora por completo su orden y la relación que tienen entre sí. Para BoW, las frases \"el perro persigue al gato\" y \"el gato persigue al perro\" son muy similares, aunque su significado es opuesto. Además, trata palabras como \"rey\" y \"reina\" como dos conceptos totalmente independientes y sin ninguna relación.\n",
        "\n",
        "Los **embeddings**, en cambio, mapean palabras a vectores de números de tal manera que las palabras con significados similares tienen vectores cercanos en un espacio multidimensional.\n",
        "\n",
        "Las principales ventajas son:\n",
        "\n",
        "* **Captura de Relaciones Semánticas:** Los embeddings sitúan palabras como \"rey\" y \"reina\" cerca una de la otra. Incluso pueden capturar relaciones analógicas, como la famosa `vector('rey') - vector('hombre') + vector('mujer') ≈ vector('reina')`.\n",
        "* **Eficiencia Dimensional:** Mientras que BoW crea vectores muy largos (uno por cada palabra del vocabulario) y dispersos (llenos de ceros), los embeddings son vectores densos y de menor dimensión (ej. 300 dimensiones vs. 50,000 de BoW), lo que los hace computacionalmente más eficientes.\n",
        "* **Generalización:** Un modelo pre-entrenado con embeddings (como Word2Vec o GloVe) ya \"sabe\" que \"excelente\" y \"fantástico\" son similares, incluso si en tu set de datos de entrenamiento nunca aparecen en el mismo contexto. BoW no puede hacer esta generalización.\n",
        "\n",
        "En resumen, pasar de bag-of-words a embeddings es como pasar de un simple conteo de palabras a una verdadera comprensión de su significado y de cómo se relacionan entre sí.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. ¿Qué tipo de reseñas serían más difíciles de clasificar?\n",
        "\n",
        "Los modelos de clasificación de texto, incluso los más avanzados, tienen dificultades con reseñas que requieren una comprensión profunda del lenguaje humano, el contexto y el conocimiento del mundo. Las más difíciles son:\n",
        "\n",
        "* **Sarcasmo e Ironía:** Son el mayor desafío. Una reseña como *\"Claro, me encantó esperar 40 minutos por un café frío. Una experiencia fantástica.\"* utiliza palabras positivas (\"encantó\", \"fantástica\") para expresar un sentimiento fuertemente negativo. El modelo, al leer literalmente las palabras, puede clasificarla erróneamente.\n",
        "\n",
        "* **Reseñas Mixtas o Ambiguas:** Aquellas que contienen tanto elementos positivos como negativos son difíciles de encasillar en una sola categoría. Por ejemplo: *\"La comida era deliciosa y el ambiente muy bueno, pero el servicio fue pésimo y arruinó la noche.\"* ¿Es una reseña positiva o negativa? Depende del peso que se le dé a cada aspecto.\n",
        "\n",
        "* **Lenguaje Comparativo:** Reseñas que evalúan un producto en relación con otro. *\"Es mucho mejor que el modelo anterior, aunque sigue sin estar a la altura de la competencia.\"* La clasificación depende de un punto de referencia que el modelo puede no conocer.\n",
        "\n",
        "* **Falta de Contexto o Conocimiento del Mundo:** Frases que requieren conocimiento externo. Por ejemplo, *\"Este producto es el 'New Coke' de los videojuegos.\"* Para entender que esto es una crítica muy negativa, el modelo necesitaría saber sobre el famoso fracaso comercial de Coca-Cola en los años 80.\n",
        "\n",
        "* **Textos muy cortos o con jerga:** Reseñas como *\"meh\"* o *\"equis\"* son difíciles porque contienen muy poca información contextual. De igual manera, el uso de jerga muy específica de un nicho puede confundir a un modelo que no fue entrenado con ella."
      ],
      "metadata": {
        "id": "AFWl4S_LgEOB"
      },
      "id": "AFWl4S_LgEOB"
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}