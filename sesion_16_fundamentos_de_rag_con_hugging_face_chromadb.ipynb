{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rubuntu/Taller_Introduccion_a_Ciencia_de_Datos_IA_e_Ingenieria_de_Datos/blob/main/sesion_16_fundamentos_de_rag_con_hugging_face_chromadb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2de29db",
      "metadata": {
        "id": "f2de29db"
      },
      "source": [
        "# üìò Sesi√≥n 16 ‚Äì Fundamentos de RAG con Hugging Face + ChromaDB\n",
        "\n",
        "## Objetivos\n",
        "\n",
        "* Comprender los conceptos b√°sicos de RAG y su arquitectura.\n",
        "* Aprender a transformar texto ‚Üí embeddings ‚Üí almacenar en base vectorial.\n",
        "* Construir un mini-RAG con ChromaDB y Hugging Face embeddings.\n",
        "\n",
        "---\n",
        "\n",
        "## Contenido\n",
        "\n",
        "1. **Concepto de RAG**:\n",
        "\n",
        "   * Separar *memoria a largo plazo* (vector store) del LLM.\n",
        "   * Flujo: **Pregunta ‚Üí Embedding ‚Üí Recuperaci√≥n ‚Üí Contexto ‚Üí LLM ‚Üí Respuesta**.\n",
        "\n",
        "2. **Primer prototipo**:\n",
        "\n",
        "   * Usar `sentence-transformers` de Hugging Face para embeddings.\n",
        "   * Guardar y consultar embeddings en **ChromaDB**.\n",
        "   * Pasar contexto recuperado a un LLM (ej. `transformers` o `openai`).\n",
        "\n",
        "---\n",
        "\n",
        "## Demo C√≥digo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8a98950-5254-4e72-bf6b-1b5878ce346a",
      "metadata": {
        "id": "e8a98950-5254-4e72-bf6b-1b5878ce346a"
      },
      "outputs": [],
      "source": [
        "# ========================\n",
        "# Instalaci√≥n de librer√≠as\n",
        "# ========================\n",
        "%%capture\n",
        "!pip install -q -U wikipedia-api chromadb bitsandbytes gradio\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# 1. Cargar datos de Wikipedia (Paraguay)\n",
        "# ========================\n",
        "import wikipediaapi\n",
        "\n",
        "wiki_wiki = wikipediaapi.Wikipedia(\n",
        "    language='es',\n",
        "    user_agent='MiProyectoRAG/1.0 (https://github.com/rubuntu)'\n",
        ")\n",
        "\n",
        "paginas = [\n",
        "    \"Paraguay\",\n",
        "    \"Geograf√≠a de Paraguay\",\n",
        "    \"Econom√≠a de Paraguay\",\n",
        "    \"Cultura de Paraguay\",\n",
        "    \"Historia de Paraguay\",\n",
        "    \"Asunci√≥n\",\n",
        "     #\"Guerra de la Triple Alianza\", \"Guerra del Chaco\"\n",
        "]\n",
        "\n",
        "docs = []\n",
        "for titulo in paginas:\n",
        "    page = wiki_wiki.page(titulo)\n",
        "    if page.exists():\n",
        "        docs.append(page.text)\n",
        "        print(f\"Cargado: {titulo}\")\n",
        "print(f\"Total documentos cargados: {len(docs)}\")\n"
      ],
      "metadata": {
        "id": "KdmoLm0czcjU"
      },
      "id": "KdmoLm0czcjU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# 2. Chunking para RAG\n",
        "# ========================\n",
        "def chunk_text(text, max_chars=1200, overlap=200):\n",
        "    chunks, start = [], 0\n",
        "    while start < len(text):\n",
        "        end = start + max_chars\n",
        "        chunk = text[start:end]\n",
        "        chunks.append(chunk)\n",
        "        start += max_chars - overlap\n",
        "    return chunks\n",
        "\n",
        "all_chunks = []\n",
        "for doc in docs:\n",
        "    all_chunks.extend(chunk_text(doc))\n",
        "\n",
        "print(f\"Total chunks creados: {len(all_chunks)}\")\n"
      ],
      "metadata": {
        "id": "ob5aJB-GztVG"
      },
      "id": "ob5aJB-GztVG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# 3. Embeddings y ChromaDB\n",
        "# ========================\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "\n",
        "emb_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "embeddings = emb_model.encode(all_chunks, batch_size=16, show_progress_bar=True)\n",
        "\n",
        "client = chromadb.Client()\n",
        "collection = client.create_collection(\"paraguay_wiki\")\n",
        "\n",
        "for i, chunk in enumerate(all_chunks):\n",
        "    collection.add(documents=[chunk], embeddings=[embeddings[i].tolist()], ids=[str(i)])\n",
        "\n",
        "print(\"‚úÖ Base vectorial creada con ChromaDB.\")\n"
      ],
      "metadata": {
        "id": "UA8Y6sqQzxXs"
      },
      "id": "UA8Y6sqQzxXs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# 4. Cargar Modelo\n",
        "# ========================\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_id = \"unsloth/gemma-2b-it-bnb-4bit\"  # versi√≥n ligera y estable\n",
        "model_id = \"unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True, use_fast=False)\n",
        "llm = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Modelo cargado:\", model_id)\n"
      ],
      "metadata": {
        "id": "Hwpv3Ml6z_Gy"
      },
      "id": "Hwpv3Ml6z_Gy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# 5. Funci√≥n RAG\n",
        "# ========================\n",
        "def to_gemma_chat(user_text):\n",
        "    return f\"<bos><start_of_turn>user\\n{user_text}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "\n",
        "def rag_answer(question, top_k=3, max_new_tokens=200, show_context=False):\n",
        "    # Recuperar contexto\n",
        "    query_emb = emb_model.encode([question])\n",
        "    results = collection.query(query_embeddings=query_emb.tolist(), n_results=top_k)\n",
        "    context = \"\\n\".join(results[\"documents\"][0])\n",
        "\n",
        "    if show_context:\n",
        "        print(\"\\n--- CONTEXTO ---\")\n",
        "        print(context[:1000] + (\"...\" if len(context) > 1000 else \"\"))\n",
        "        print(\"--- FIN CONTEXTO ---\\n\")\n",
        "\n",
        "    # Prompt\n",
        "    prompt = f\"\"\"\n",
        "Responde a la pregunta bas√°ndote √∫nicamente en el siguiente contexto.\n",
        "Si el contexto no contiene la respuesta, responde \"No lo s√©\".\n",
        "\n",
        "Contexto:\n",
        "{context}\n",
        "\n",
        "Pregunta: {question}\n",
        "\n",
        "Respuesta:\n",
        "\"\"\"\n",
        "    chat_prompt = to_gemma_chat(prompt)\n",
        "\n",
        "    inputs = tokenizer(chat_prompt, return_tensors=\"pt\", truncation=True, max_length=4096).to(\"cuda\")\n",
        "    outputs = llm.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True,\n",
        "        temperature=0.3,\n",
        "        top_p=0.9,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # --- limpieza de la salida ---\n",
        "    # cortar donde empieza <start_of_turn>model\n",
        "    if \"<start_of_turn>model\" in decoded:\n",
        "        answer = decoded.split(\"<start_of_turn>model\")[-1].strip()\n",
        "    else:\n",
        "        # fallback: quitar el prompt entero si aparece\n",
        "        answer = decoded.replace(chat_prompt, \"\").strip()\n",
        "\n",
        "    return answer\n"
      ],
      "metadata": {
        "id": "MlJ9HBQ68oSO"
      },
      "id": "MlJ9HBQ68oSO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# 6. Preguntas de ejemplo\n",
        "# ========================\n",
        "questions = [\n",
        "    #\"¬øCu√°l es la capital de Paraguay?\",\n",
        "    #\"¬øQu√© importancia tiene el r√≠o Paraguay?\",\n",
        "    \"Comenta sobre el r√≠o Paraguay\",\n",
        "    #\"¬øQui√©n fue Francisco Solano L√≥pez?\",\n",
        "    #\"¬øQu√© papel tuvo Paraguay en la Guerra de la Triple Alianza?\",\n",
        "    #\"¬øCu√°les son los principales productos de exportaci√≥n de Paraguay?\"\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    print(\"\\n‚ùì\", q)\n",
        "    print(\"üí°\", rag_answer(q, ))"
      ],
      "metadata": {
        "id": "NOJVOm7d0Gwe"
      },
      "id": "NOJVOm7d0Gwe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9d654db-eed8-476d-af1a-0550b0552989",
      "metadata": {
        "id": "e9d654db-eed8-476d-af1a-0550b0552989"
      },
      "outputs": [],
      "source": [
        "## ========================\n",
        "## 7. Interfaz con Gradio\n",
        "## ========================\n",
        "#import gradio as gr\n",
        "\n",
        "#def ask_question(query):\n",
        "#    return rag_answer(query)\n",
        "\n",
        "#demo = gr.Interface(\n",
        "#    fn=ask_question,\n",
        "#    inputs=gr.Textbox(lines=2, placeholder=\"Escribe tu pregunta sobre Paraguay...\"),\n",
        "#    outputs=\"text\",\n",
        "#    title=\"RAG sobre Paraguay (Wikipedia + LLM)\",\n",
        "#    description=\"Haz preguntas sobre Paraguay usando RAG con ChromaDB y un modelo abierto (Gemma, Mistral, Llama, Qwen).\"\n",
        "#)\n",
        "\n",
        "#demo.launch(share=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddf7ccc4",
      "metadata": {
        "id": "ddf7ccc4"
      },
      "source": [
        "---\n",
        "\n",
        "## Preguntas de discusi√≥n\n",
        "\n",
        "1. ¬øPor qu√© un LLM necesita un vector store externo para RAG?\n",
        "2. ¬øQu√© limitaciones tendr√≠a un mini-RAG con pocas docenas de documentos?\n",
        "3. ¬øC√≥mo escalar√≠as esto a millones de documentos?\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}