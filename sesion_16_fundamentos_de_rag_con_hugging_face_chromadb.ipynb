{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rubuntu/Taller_Introduccion_a_Ciencia_de_Datos_IA_e_Ingenieria_de_Datos/blob/main/sesion_16_fundamentos_de_rag_con_hugging_face_chromadb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2de29db",
      "metadata": {
        "id": "f2de29db"
      },
      "source": [
        "# üìò Sesi√≥n 16 ‚Äì Fundamentos de RAG con Hugging Face + ChromaDB\n",
        "\n",
        "## Objetivos\n",
        "\n",
        "* Comprender los conceptos b√°sicos de RAG y su arquitectura.\n",
        "* Aprender a transformar texto ‚Üí embeddings ‚Üí almacenar en base vectorial.\n",
        "* Construir un mini-RAG con ChromaDB y Hugging Face embeddings.\n",
        "\n",
        "---\n",
        "\n",
        "## Contenido\n",
        "\n",
        "1. **Concepto de RAG**:\n",
        "\n",
        "   * Separar *memoria a largo plazo* (vector store) del LLM.\n",
        "   * Flujo: **Pregunta ‚Üí Embedding ‚Üí Recuperaci√≥n ‚Üí Contexto ‚Üí LLM ‚Üí Respuesta**.\n",
        "\n",
        "2. **Primer prototipo**:\n",
        "\n",
        "   * Usar `sentence-transformers` de Hugging Face para embeddings.\n",
        "   * Guardar y consultar embeddings en **ChromaDB**.\n",
        "   * Pasar contexto recuperado a un LLM (ej. `transformers` o `openai`).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## 1. ¬øQu√© es RAG?\n",
        "\n",
        "* **RAG** = *Retrieval-Augmented Generation*.\n",
        "* Es una t√©cnica que combina dos mundos:\n",
        "\n",
        "  1. **Recuperaci√≥n de informaci√≥n (IR / search engine)** ‚Üí buscar documentos relevantes en una base de datos.\n",
        "  2. **Generaci√≥n con modelos de lenguaje (LLM)** ‚Üí producir una respuesta en lenguaje natural.\n",
        "\n",
        "üëâ La idea principal: **el modelo no depende solo de lo que tiene en sus pesos, sino que se ‚Äúalimenta‚Äù de fuentes externas actualizadas**.\n",
        "\n",
        "Ejemplo:\n",
        "\n",
        "> Preguntas: *‚Äú¬øQui√©n es el actual presidente de Paraguay?‚Äù*\n",
        ">\n",
        "> * Un LLM puro (sin RAG) puede fallar si no est√° actualizado.\n",
        "> * Un RAG consulta primero en su base/vector store con datos recientes ‚Üí encuentra el documento correcto ‚Üí luego el LLM genera la respuesta bas√°ndose en ese contexto.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Arquitectura b√°sica de RAG\n",
        "\n",
        "Se suele dividir en **4 pasos principales**:\n",
        "\n",
        "### üîπ Paso 1. **Indexaci√≥n (offline)**\n",
        "\n",
        "* Se prepara una colecci√≥n de documentos (ej: art√≠culos de Wikipedia, PDFs, noticias, etc.).\n",
        "* Se dividen en **chunks** (trozos) para que no sean demasiado largos.\n",
        "* Cada chunk se convierte en un **embedding vectorial** (ej: usando *SentenceTransformers*).\n",
        "* Los embeddings se guardan en un **vector store** (ChromaDB, Pinecone, Weaviate, FAISS, etc.) junto con el texto original.\n",
        "\n",
        "### üîπ Paso 2. **Consulta del usuario**\n",
        "\n",
        "* El usuario hace una pregunta en lenguaje natural.\n",
        "* Esa pregunta tambi√©n se convierte en un **embedding**.\n",
        "\n",
        "### üîπ Paso 3. **Recuperaci√≥n**\n",
        "\n",
        "* Se busca en el vector store los **chunks m√°s similares** al embedding de la pregunta.\n",
        "* Estos fragmentos relevantes forman el **contexto** que se pasar√° al modelo.\n",
        "\n",
        "### üîπ Paso 4. **Generaci√≥n**\n",
        "\n",
        "* Se construye un **prompt** que combina:\n",
        "\n",
        "  * La pregunta del usuario.\n",
        "  * Los documentos recuperados como *contexto*.\n",
        "* Se env√≠a este prompt al **LLM** (Gemma, Mistral, Llama, Qwen, etc.).\n",
        "* El modelo genera la respuesta final, apoy√°ndose en el contexto.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Diagrama de flujo simplificado\n",
        "\n",
        "```\n",
        "Usuario ‚Üí Pregunta\n",
        "   ‚Üì\n",
        "Embeddings de la pregunta\n",
        "   ‚Üì\n",
        "Vector Store (ChromaDB) ‚Üí Recupera chunks relevantes\n",
        "   ‚Üì\n",
        "Construcci√≥n del Prompt (Contexto + Pregunta)\n",
        "   ‚Üì\n",
        "LLM (ej: Gemma-3n) ‚Üí Genera respuesta\n",
        "   ‚Üì\n",
        "Respuesta final al usuario\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Beneficios de RAG\n",
        "\n",
        "* **Actualizaci√≥n din√°mica:** basta con cambiar la base de datos, no hay que reentrenar el LLM.\n",
        "* **Precisi√≥n y grounding:** el modelo se apoya en fuentes externas verificables.\n",
        "* **Eficiencia:** usar un LLM relativamente peque√±o + base vectorial puede superar a un LLM gigante sin RAG.\n",
        "* **Menor alucinaci√≥n:** como el modelo tiene el contexto correcto, es menos probable que invente informaci√≥n.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Limitaciones y desaf√≠os\n",
        "\n",
        "* **Calidad de la base:** si la base de documentos es pobre, el LLM no podr√° responder bien.\n",
        "* **Chunking:** si se cortan mal los textos, puede faltar contexto.\n",
        "* **Ventana de contexto:** los LLM tienen l√≠mite en la cantidad de tokens que pueden recibir.\n",
        "* **Coste de recuperaci√≥n:** b√∫squedas vectoriales en bases muy grandes requieren optimizaci√≥n.\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ **En resumen:**\n",
        "RAG es una arquitectura que **ampl√≠a la memoria de los LLM** conect√°ndolos a una base de conocimiento externa. Esto permite respuestas m√°s precisas, actualizadas y con respaldo en datos, en lugar de depender solo de lo aprendido durante el entrenamiento del modelo.\n",
        "\n",
        "---\n",
        "\n",
        "## Demo C√≥digo"
      ],
      "metadata": {
        "id": "cCAlvfluL8RB"
      },
      "id": "cCAlvfluL8RB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8a98950-5254-4e72-bf6b-1b5878ce346a",
      "metadata": {
        "id": "e8a98950-5254-4e72-bf6b-1b5878ce346a"
      },
      "outputs": [],
      "source": [
        "# ========================\n",
        "# Instalaci√≥n de librer√≠as\n",
        "# ========================\n",
        "%%capture\n",
        "!pip install -q -U wikipedia-api chromadb bitsandbytes gradio\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# 1. Cargar datos de Wikipedia (Paraguay)\n",
        "# ========================\n",
        "import wikipediaapi\n",
        "\n",
        "wiki_wiki = wikipediaapi.Wikipedia(\n",
        "    language='es',\n",
        "    user_agent='MiProyectoRAG/1.0 (https://github.com/rubuntu)'\n",
        ")\n",
        "\n",
        "paginas = [\n",
        "    \"Paraguay\",\n",
        "    \"Geograf√≠a de Paraguay\",\n",
        "    \"Econom√≠a de Paraguay\",\n",
        "    \"Cultura de Paraguay\",\n",
        "    \"Historia de Paraguay\",\n",
        "    \"Asunci√≥n\",\n",
        "    \"Guerra de la Triple Alianza\",\n",
        "    \"Guerra del Chaco\",\n",
        "]\n",
        "\n",
        "docs = []\n",
        "for titulo in paginas:\n",
        "    page = wiki_wiki.page(titulo)\n",
        "    if page.exists():\n",
        "        docs.append(page.text)\n",
        "        print(f\"Cargado: {titulo}\")\n",
        "print(f\"Total documentos cargados: {len(docs)}\")\n"
      ],
      "metadata": {
        "id": "KdmoLm0czcjU"
      },
      "id": "KdmoLm0czcjU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# 2. Chunking para RAG\n",
        "# ========================\n",
        "def chunk_text(text, max_chars=1200, overlap=200):\n",
        "    chunks, start = [], 0\n",
        "    while start < len(text):\n",
        "        end = start + max_chars\n",
        "        chunk = text[start:end]\n",
        "        chunks.append(chunk)\n",
        "        start += max_chars - overlap\n",
        "    return chunks\n",
        "\n",
        "all_chunks = []\n",
        "for doc in docs:\n",
        "    all_chunks.extend(chunk_text(doc))\n",
        "\n",
        "print(f\"Total chunks creados: {len(all_chunks)}\")\n"
      ],
      "metadata": {
        "id": "ob5aJB-GztVG"
      },
      "id": "ob5aJB-GztVG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicaci√≥n:\n",
        "\n",
        "* **Objetivo:** dividir documentos largos en fragmentos (*chunks*) m√°s peque√±os para que los embeddings y el LLM los procesen mejor en RAG.\n",
        "* **Par√°metros:**\n",
        "\n",
        "  * `max_chars=1200` ‚Üí tama√±o m√°ximo de cada chunk en caracteres.\n",
        "  * `overlap=200` ‚Üí solapamiento entre chunks para no perder continuidad.\n",
        "* **Funcionamiento:** avanza por el texto tomando bloques de 1200 caracteres, retrocede 200 y sigue ‚Üí as√≠ asegura que frases no queden cortadas.\n",
        "* **Resultado:** `all_chunks` contiene todos los fragmentos de todos los documentos, listos para generar embeddings e indexar en ChromaDB.\n",
        "\n",
        "üëâ En pocas palabras: **corta los documentos grandes en trozos solapados, m√°s f√°ciles de buscar y recuperar en el RAG.**\n"
      ],
      "metadata": {
        "id": "nV5Lh3qRJ7ow"
      },
      "id": "nV5Lh3qRJ7ow"
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# 3. Embeddings y ChromaDB\n",
        "# ========================\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "\n",
        "emb_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "embeddings = emb_model.encode(all_chunks, batch_size=16, show_progress_bar=True)\n",
        "\n",
        "client = chromadb.Client()\n",
        "collection = client.create_collection(\"paraguay_wiki\")\n",
        "\n",
        "for i, chunk in enumerate(all_chunks):\n",
        "    collection.add(documents=[chunk], embeddings=[embeddings[i].tolist()], ids=[str(i)])\n",
        "\n",
        "print(\"‚úÖ Base vectorial creada con ChromaDB.\")\n"
      ],
      "metadata": {
        "id": "UA8Y6sqQzxXs"
      },
      "id": "UA8Y6sqQzxXs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicaci√≥n:\n",
        "\n",
        "* Se carga un **modelo de embeddings** (`all-MiniLM-L6-v2`) para convertir texto en vectores num√©ricos que capturan su significado.\n",
        "* Se generan embeddings para todos los *chunks* de texto (`emb_model.encode`).\n",
        "* Se crea una **colecci√≥n en ChromaDB** llamada `\"paraguay_wiki\"`.\n",
        "* Cada chunk se guarda en la colecci√≥n junto con su embedding y un ID √∫nico.\n",
        "* Resultado: ya tienes una **base vectorial** lista para buscar y recuperar fragmentos relevantes en un flujo RAG.\n",
        "\n",
        "üëâ En pocas palabras: **convierte los chunks en vectores y los indexa en ChromaDB para poder hacer b√∫squedas sem√°nticas.**\n"
      ],
      "metadata": {
        "id": "30UzHi2UKMnk"
      },
      "id": "30UzHi2UKMnk"
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# 4. Cargar Modelo\n",
        "# ========================\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_id = \"unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True, use_fast=False)\n",
        "llm = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Modelo cargado:\", model_id)\n"
      ],
      "metadata": {
        "id": "Hwpv3Ml6z_Gy"
      },
      "id": "Hwpv3Ml6z_Gy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicaci√≥n:\n",
        "\n",
        "* Se usa **Hugging Face Transformers** para cargar un **tokenizer** y un **modelo de lenguaje causal (LLM)**.\n",
        "* `model_id` apunta a **`unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit`**, una versi√≥n de **Gemma-3n** (variante **E2B**) ya **quantizada en 4 bits con Unsloth**, lo que la hace m√°s ligera.\n",
        "* El **tokenizer** convierte texto en tokens (IDs num√©ricos).\n",
        "* El **modelo LLM** (cargado con `AutoModelForCausalLM`) genera texto a partir de esos tokens.\n",
        "* `device_map=\"auto\"` env√≠a autom√°ticamente el modelo a GPU si est√° disponible.\n",
        "\n",
        "üëâ En pocas palabras: **se carga Gemma-3n (E2B, quantizado 4-bit) con Unsloth, optimizado para bajo consumo de memoria y buen rendimiento en tareas de chat/instrucci√≥n.**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "w6YlG-8QLGpX"
      },
      "id": "w6YlG-8QLGpX"
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# 5. Funci√≥n RAG\n",
        "# ========================\n",
        "def to_gemma_chat(user_text):\n",
        "    return f\"<bos><start_of_turn>user\\n{user_text}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "\n",
        "def rag_answer(question, top_k=3, max_new_tokens=200, show_context=False):\n",
        "    # Recuperar contexto\n",
        "    query_emb = emb_model.encode([question])\n",
        "    results = collection.query(query_embeddings=query_emb.tolist(), n_results=top_k)\n",
        "    context = \"\\n\".join(results[\"documents\"][0])\n",
        "\n",
        "    if show_context:\n",
        "        print(\"\\n--- CONTEXTO ---\")\n",
        "        print(context[:1000] + (\"...\" if len(context) > 1000 else \"\"))\n",
        "        print(\"--- FIN CONTEXTO ---\\n\")\n",
        "\n",
        "    # Prompt\n",
        "    prompt = f\"\"\"\n",
        "Responde a la pregunta bas√°ndote √∫nicamente en el siguiente contexto.\n",
        "No incluir la palabra \"Contexto\" en la respuesta.\n",
        "Si el contexto no contiene la respuesta, responde \"No lo s√©\".\n",
        "\n",
        "Contexto:\n",
        "{context}\n",
        "\n",
        "Pregunta: {question}\n",
        "\n",
        "Respuesta:\n",
        "\"\"\"\n",
        "    chat_prompt = to_gemma_chat(prompt)\n",
        "\n",
        "    inputs = tokenizer(chat_prompt, return_tensors=\"pt\", truncation=True, max_length=4096).to(\"cuda\")\n",
        "    outputs = llm.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True,\n",
        "        temperature=0.3,\n",
        "        top_p=0.9,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # --- limpieza de la salida ---\n",
        "    # cortar donde empieza <start_of_turn>model\n",
        "    if \"<start_of_turn>model\" in decoded:\n",
        "        answer = decoded.split(\"<start_of_turn>model\")[-1].strip()\n",
        "    else:\n",
        "        # fallback: quitar el prompt entero si aparece\n",
        "        answer = decoded.replace(chat_prompt, \"\").strip()\n",
        "\n",
        "    return answer\n"
      ],
      "metadata": {
        "id": "MlJ9HBQ68oSO"
      },
      "id": "MlJ9HBQ68oSO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "Explicaci√≥n\n",
        "\n",
        "1. **to\\_gemma\\_chat**\n",
        "\n",
        "   * Adapta el texto del *prompt* al formato especial que entiende **Gemma** (`<bos><start_of_turn>user ... <start_of_turn>model`).\n",
        "\n",
        "2. **rag\\_answer**\n",
        "\n",
        "   * **Recupera contexto:** convierte la pregunta a embedding y busca en ChromaDB los chunks m√°s similares (`top_k`).\n",
        "   * **Construye el prompt:** inserta el contexto + la pregunta en una instrucci√≥n clara (‚Äúresponde solo con el contexto o di ‚ÄòNo lo s√©‚Äô‚Äù).\n",
        "   * **Prepara el chat\\_prompt:** lo transforma al formato de Gemma usando `to_gemma_chat`.\n",
        "   * **Genera respuesta:** pasa el prompt al modelo (`llm.generate`), limitando longitud y usando sampling controlado (`temperature`, `top_p`).\n",
        "   * **Limpieza:** elimina el eco del prompt y se queda solo con la respuesta del modelo.\n",
        "   * **Devuelve la respuesta final.**\n",
        "\n",
        "---\n",
        "\n",
        "üëâ Resumen: (Pregunta ‚Üí Embedding ‚Üí B√∫squeda en Chroma ‚Üí Prompt ‚Üí LLM ‚Üí Respuesta)    \n",
        "La funci√≥n **RAG** toma una pregunta ‚Üí busca los fragmentos de texto relevantes ‚Üí arma un prompt con ese contexto ‚Üí lo da al modelo (Gemma) ‚Üí limpia la salida ‚Üí devuelve la respuesta clara.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "kUxUSe6fJRtb"
      },
      "id": "kUxUSe6fJRtb"
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# 6. Preguntas de ejemplo\n",
        "# ========================\n",
        "questions = [\n",
        "    \"¬øCu√°l es la capital de Paraguay?\",\n",
        "    \"¬øQui√©n fue Francisco Solano L√≥pez?\",\n",
        "    \"¬øQu√© papel tuvo Paraguay en la Guerra de la Triple Alianza?\",\n",
        "    \"¬øCu√°les son los principales productos de exportaci√≥n de Paraguay?\"\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    print(\"\\n‚ùì\", q)\n",
        "    print(\"üí°\", rag_answer(q, ))"
      ],
      "metadata": {
        "id": "NOJVOm7d0Gwe"
      },
      "id": "NOJVOm7d0Gwe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9d654db-eed8-476d-af1a-0550b0552989",
      "metadata": {
        "id": "e9d654db-eed8-476d-af1a-0550b0552989"
      },
      "outputs": [],
      "source": [
        "# ========================\n",
        "# 7. Interfaz con Gradio\n",
        "# ========================\n",
        "import gradio as gr\n",
        "\n",
        "def ask_question(query):\n",
        "    return rag_answer(query)\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=ask_question,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Escribe tu pregunta sobre Paraguay...\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"RAG sobre Paraguay (Wikipedia + LLM)\",\n",
        "    description=\"Haz preguntas sobre Paraguay usando RAG con ChromaDB y un modelo abierto (Gemma).\"\n",
        ")\n",
        "\n",
        "demo.launch(share=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddf7ccc4",
      "metadata": {
        "id": "ddf7ccc4"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "# üìò Preguntas de discusi√≥n sobre RAG\n",
        "\n",
        "---\n",
        "\n",
        "## 1. ¬øPor qu√© un LLM necesita un vector store externo para RAG?\n",
        "\n",
        "* **Limitaci√≥n de memoria de los LLM:**\n",
        "  Un LLM no puede \"recordar\" informaci√≥n fuera de lo que se entren√≥. Si se quiere consultar datos **actualizados o muy espec√≠ficos**, hay que tener una fuente externa.\n",
        "\n",
        "* **B√∫squeda eficiente:**\n",
        "  El vector store convierte textos en **embeddings** (vectores num√©ricos). Eso permite hacer b√∫squedas sem√°nticas r√°pidas en miles o millones de documentos.\n",
        "  Ejemplo: la pregunta *‚Äú¬øCu√°l es la capital de Paraguay?‚Äù* se convierte en un vector ‚Üí se compara contra todos los chunks ‚Üí se recuperan los m√°s relevantes.\n",
        "\n",
        "* **Ventaja sobre embeddings directos:**\n",
        "  Sin un vector store, tendr√≠as que comparar la pregunta con todos los documentos cada vez (coste enorme). Con un motor especializado (ChromaDB, FAISS, Pinecone) esto se resuelve en milisegundos usando t√©cnicas como *Approximate Nearest Neighbors (ANN)*.\n",
        "\n",
        "üëâ En resumen: el vector store es el **‚Äúcerebro externo‚Äù** que permite al LLM acceder a datos organizados y recuperarlos de forma r√°pida y precisa.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. ¬øQu√© limitaciones tendr√≠a un mini-RAG con pocas docenas de documentos?\n",
        "\n",
        "* **Cobertura limitada:**\n",
        "  Con pocos documentos, la base de conocimiento es reducida ‚Üí el modelo solo puede responder a preguntas relacionadas con ese corpus.\n",
        "\n",
        "* **Alto riesgo de ‚ÄúNo s√©‚Äù o alucinaciones:**\n",
        "  Si la informaci√≥n buscada no est√° en esos documentos, el modelo puede inventar respuestas o decir que no sabe.\n",
        "\n",
        "* **Menor diversidad sem√°ntica:**\n",
        "  Un vector store peque√±o no ofrece gran variedad de embeddings ‚Üí recupera siempre los mismos textos aunque no sean perfectos.\n",
        "\n",
        "* **Buen caso de uso:**\n",
        "  Es √∫til como **demo educativo o POC (proof of concept)**, pero insuficiente para producci√≥n donde se requieren millones de art√≠culos, actualizaciones peri√≥dicas y calidad robusta.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. ¬øC√≥mo escalar√≠as esto a millones de documentos?\n",
        "\n",
        "Para llevar un RAG de un prototipo a producci√≥n a gran escala, habr√≠a que:\n",
        "\n",
        "* **Indexaci√≥n distribuida:**\n",
        "\n",
        "  * Usar bases vectoriales dise√±adas para grandes vol√∫menes (Pinecone, Weaviate, Milvus, Vespa, FAISS con sharding).\n",
        "  * Permitir b√∫squeda paralela en varios nodos.\n",
        "\n",
        "* **Preprocesamiento avanzado:**\n",
        "\n",
        "  * Buen chunking din√°mico (seg√∫n p√°rrafos, secciones, etc.).\n",
        "  * Enriquecer con metadatos (fecha, fuente, tipo de documento) para filtrar m√°s r√°pido.\n",
        "\n",
        "* **T√©cnicas de b√∫squeda h√≠brida:**\n",
        "\n",
        "  * Combinar **b√∫squeda sem√°ntica** (embeddings) con **b√∫squeda lexical** (BM25, keyword search).\n",
        "  * Esto reduce falsos positivos y mejora la relevancia.\n",
        "\n",
        "* **Optimizaci√≥n de embeddings:**\n",
        "\n",
        "  * Usar modelos m√°s robustos y multiling√ºes.\n",
        "  * Batch processing para indexar millones de textos sin colapsar memoria.\n",
        "\n",
        "* **Pipeline escalable:**\n",
        "\n",
        "  * Arquitectura de microservicios: un servicio para embeddings, otro para indexaci√≥n, otro para retrieval.\n",
        "  * Cachear consultas frecuentes.\n",
        "  * Actualizar la base vectorial de manera incremental (streaming ingestion).\n",
        "\n",
        "üëâ En pocas palabras: pasar de un mini-RAG a millones de documentos requiere **infraestructura especializada, almacenamiento distribuido y t√©cnicas de b√∫squeda h√≠brida para mantener velocidad y precisi√≥n.**\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}